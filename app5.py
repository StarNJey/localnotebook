# ì›¹í¬ë¡¤ë§ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ Deep Research Chatbot

# ê¸°ì¡´ PDF ì²˜ë¦¬ ê¸°ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œ ì›¹ ê²€ìƒ‰ ë° í¬ë¡¤ë§ ê¸°ëŠ¥ì„ ì¶”ê°€

from __future__ import annotations
import os
import re
import json
import hashlib
import time
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
from urllib.parse import urlparse, quote_plus

import streamlit as st
import numpy as np
import torch, gc
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
from sentence_transformers.cross_encoder import CrossEncoder
import warnings

# ì›¹í¬ë¡¤ë§ì„ ìœ„í•œ ìƒˆë¡œìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import requests
    from bs4 import BeautifulSoup
    WEB_CRAWLING_AVAILABLE = True
except ImportError:
    WEB_CRAWLING_AVAILABLE = False

# âš ï¸ ìˆ˜ì •ì‚¬í•­ 1: st.set_page_configë¥¼ ë§¨ ì•ìœ¼ë¡œ ì´ë™
st.set_page_config(page_title="Deep Research Chatbot with Web Crawling", layout="wide")

# PDF ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False

# í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    from kiwipiepy import Kiwi
    KIWI_AVAILABLE = True
except ImportError:
    KIWI_AVAILABLE = False

warnings.filterwarnings("ignore")
os.environ["TRANSFORMERS_VERBOSITY"] = "error"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ì›¹í¬ë¡¤ë§ ê´€ë ¨ í´ë˜ìŠ¤ (ìƒˆë¡œ ì¶”ê°€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class WebDocument:
    """í¬ë¡¤ë§ëœ ì›¹ ë¬¸ì„œ ë°ì´í„° êµ¬ì¡°"""
    url: str
    title: str
    text: str
    domain: str
    crawl_time: str
    source: str
    page: int = 0
    paragraph: int = 0
    chunk_id: str = ""

class WebCrawler:
    """ì›¹ ê²€ìƒ‰ ë° í¬ë¡¤ë§ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, timeout: int = 10, max_content_length: int = 50000):
        self.timeout = timeout
        self.max_content_length = max_content_length
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0'
        })

    def search_web(self, query: str, max_results: int = 15) -> List[str]:
        """DuckDuckGoë¥¼ í†µí•œ ì›¹ ê²€ìƒ‰"""
        try:
            search_url = f"https://html.duckduckgo.com/html/?q={quote_plus(query)}"
            response = self.session.get(search_url, timeout=self.timeout)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            urls: List[str] = []
            for link in soup.find_all('a', class_='result__a')[:max_results]:
                href = link.get('href')
                if href and href.startswith('http'):
                    urls.append(href)
            return urls
        except Exception as e:
            st.warning(f"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def crawl_page(self, url: str) -> Optional[WebDocument]:
        """ë‹¨ì¼ ì›¹í˜ì´ì§€ í¬ë¡¤ë§"""
        try:
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            content_type = response.headers.get('content-type', '')
            if 'text/html' not in content_type.lower():
                return None

            soup = BeautifulSoup(response.text, 'html.parser')
            title_tag = soup.find('title')
            title = title_tag.get_text().strip() if title_tag else "ì œëª© ì—†ìŒ"

            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            text = soup.get_text()
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            if len(text) > self.max_content_length:
                text = text[:self.max_content_length] + "..."
            if len(text) < 100:
                return None

            domain = urlparse(url).netloc
            crawl_time = datetime.now().isoformat()
            source = f"web:{url}"
            chunk_id = hashlib.md5(f"{url}_{text[:100]}".encode()).hexdigest()[:8]

            return WebDocument(
                url=url, title=title[:200], text=text,
                domain=domain, crawl_time=crawl_time,
                source=source, chunk_id=chunk_id
            )
        except Exception as e:
            st.warning(f"í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨ ({url}): {e}")
            return None

    def crawl_multiple_pages(self, urls: List[str]) -> List[WebDocument]:
        """ì—¬ëŸ¬ í˜ì´ì§€ í¬ë¡¤ë§"""
        web_docs: List[WebDocument] = []
        progress_bar = st.progress(0)
        for i, url in enumerate(urls):
            doc = self.crawl_page(url)
            if doc:
                web_docs.append(doc)
            progress_bar.progress((i + 1) / len(urls))
            time.sleep(0.5)
        return web_docs

    def search_and_crawl(self, query: str, max_results: int = 15) -> List[WebDocument]:
        """ê²€ìƒ‰ + í¬ë¡¤ë§ í†µí•© ë©”ì„œë“œ"""
        urls = self.search_web(query, max_results)
        if not urls:
            return []
        return self.crawl_multiple_pages(urls)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ê¸°ì¡´ ë°ì´í„° êµ¬ì¡° ì •ì˜ (ìœ ì§€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ResearchPhase(Enum):
    PLANNING = "planning"
    INITIAL_RETRIEVAL = "initial_retrieval"
    WEB_CRAWLING = "web_crawling"
    DEEP_ANALYSIS = "deep_analysis"
    CROSS_VALIDATION = "cross_validation"
    SYNTHESIS = "synthesis"
    FINAL_VALIDATION = "final_validation"

@dataclass
class ResearchState:
    """ì—°êµ¬ ì§„í–‰ ìƒíƒœë¥¼ ì¶”ì í•˜ëŠ” í´ë˜ìŠ¤"""
    phase: ResearchPhase
    query: str
    sub_queries: List[str]
    retrieved_docs: List[Dict[str, Any]]
    web_docs: List[WebDocument]
    confidence_history: List[float]
    insights: List[str]
    gaps: List[str]
    cycle_count: int = 0
    max_cycles: int = 3

@dataclass
class SearchQuery:
    """ê²€ìƒ‰ ì¿¼ë¦¬ ì •ë³´"""
    text: str
    priority: float
    category: str
    reason: str
    search_web: bool = True

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ê¸°ì¡´ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ìœ ì§€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output.last_hidden_state
    mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * mask_expanded, 1) / torch.clamp(mask_expanded.sum(1), min=1e-9)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ê¸°ì¡´ ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ë“¤ (ìœ ì§€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ImprovedKoreanSentenceChunker:
    def __init__(self, min_chunk_length=50, max_chunk_length=300, sentences_per_chunk=2):
        self.min_chunk_length = min_chunk_length
        self.max_chunk_length = max_chunk_length
        self.sentences_per_chunk = sentences_per_chunk
        self.kiwi = Kiwi() if KIWI_AVAILABLE else None

    def chunk_text(self, text: str) -> List[str]:
        if not text.strip():
            return []
        sentences = self._split_sentences(text)
        if not sentences:
            return []
        sentences = self._postprocess_sentences(sentences)
        return self._create_chunks(sentences)

    def _split_sentences(self, text: str) -> List[str]:
        if KIWI_AVAILABLE and self.kiwi:
            try:
                kiwi_result = self.kiwi.split_into_sents(text.strip())
                sents = [sent.text.strip() for sent in kiwi_result if sent.text.strip()]
                if len(sents) > 1:
                    return sents
            except:
                pass
        return self._regex_sentence_split(text)

    def _regex_sentence_split(self, text: str) -> List[str]:
        patterns = [
            r'[.!?]+\s+', r'[ë‹¤ê°€ë‚˜ë‹ˆê¹Œìš”ë˜ìŠµë‹ˆë‹¤]\s*[.!?]*\s+',
            r'[ë‹ˆë‹¤í–ˆë‹¤ìŠµë‹ˆë‹¤ì˜€ë‹¤ì•˜ë‹¤]\s*[.!?]*\s+', r'\n\s*\n', r'\.\s*\n',
        ]
        combined = '|'.join(f'({p})' for p in patterns)
        parts = re.split(combined, text)
        return [p.strip() for p in parts if p and not re.match(r'^[\s.!?\n]+$', p)]

    def _postprocess_sentences(self, sents: List[str]) -> List[str]:
        processed, current = [], ""
        for sent in sents:
            sent = sent.strip()
            if not sent: continue
            if len(current) < self.min_chunk_length:
                current = f"{current} {sent}".strip() if current else sent
            else:
                processed.append(current)
                current = sent
        if current:
            if processed and len(current) < self.min_chunk_length:
                processed[-1] += f" {current}"
            else:
                processed.append(current)
        return processed

    def _create_chunks(self, sents: List[str]) -> List[str]:
        chunks, curr, cnt = [], "", 0
        for sent in sents:
            test = f"{curr} {sent}".strip()
            if len(test) <= self.max_chunk_length and cnt < self.sentences_per_chunk:
                curr, cnt = test, cnt + 1
            else:
                if curr: chunks.append(curr.strip())
                curr, cnt = sent, 1
        if curr: chunks.append(curr.strip())
        return chunks

class ImprovedPDFExtractor:
    def __init__(self):
        self.available_methods = ["pdfplumber"] if PDFPLUMBER_AVAILABLE else []

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        if PDFPLUMBER_AVAILABLE:
            try:
                text = self._extract_with_pdfplumber(pdf_path)
                if text.strip():
                    return text
            except Exception as e:
                st.warning(f"pdfplumber ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return ""

    def _extract_with_pdfplumber(self, pdf_path: str) -> str:
        text = ""
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                if page.extract_text():
                    text += page.extract_text() + "\n"
        return text

class DeviceConfig:
    def __init__(self):
        self.device, self.info = self._detect()
        self.config = self._get_config()

    def _detect(self):
        if torch.cuda.is_available():
            return "cuda", {
                "type": "GPU",
                "name": torch.cuda.get_device_name(0),
                "memory": torch.cuda.get_device_properties(0).total_memory // (1024**3),
                "count": torch.cuda.device_count()
            }
        import psutil
        return "cpu", {
            "type": "CPU",
            "name": "CPU",
            "cores": psutil.cpu_count(),
            "memory": psutil.virtual_memory().total // (1024**3)
        }

    def _get_config(self):
        if self.device == "cuda":
            mem = self.info["memory"]
            if mem >= 24:
                return {"torch_dtype": torch.bfloat16, "max_new_tokens": 8000, "top_k": 12,
                        "embedding_batch_size": 64, "do_sample": True, "temperature": 0.1, "top_p": 0.9, "sim_threshold": 0.3}
            if mem >= 12:
                return {"torch_dtype": torch.bfloat16, "max_new_tokens": 6000, "top_k": 12,
                        "embedding_batch_size": 32, "do_sample": True, "temperature": 0.1, "top_p": 0.9, "sim_threshold": 0.3}
            if mem >= 8:
                return {"torch_dtype": torch.bfloat16, "max_new_tokens": 4000, "top_k": 10,
                        "embedding_batch_size": 16, "do_sample": True, "temperature": 0.1, "top_p": 0.9, "sim_threshold": 0.3}
            return {"torch_dtype": torch.float16, "max_new_tokens": 3000, "top_k": 10,
                    "embedding_batch_size": 8, "do_sample": False, "temperature": 0.1, "sim_threshold": 0.3}
        return {"torch_dtype": torch.float32, "max_new_tokens": 2000, "top_k": 10,
                "embedding_batch_size": 4, "do_sample": False, "temperature": 0.1, "sim_threshold": 0.3}

    def get_adaptive_config(self, complexity_level: float) -> Dict[str, Any]:
        base = self.config.copy()
        if complexity_level > 0.8:
            base["max_new_tokens"] = int(base["max_new_tokens"] * 1.5)
            base["top_k"] = min(base["top_k"] + 5, 20)
            base["temperature"] = min(base.get("temperature", 0.1) + 0.1, 0.3)
        elif complexity_level < 0.3:
            base["max_new_tokens"] = int(base["max_new_tokens"] * 0.7)
            base["top_k"] = max(base["top_k"] - 2, 5)
            base["do_sample"] = False
        return base

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. Agent í´ë˜ìŠ¤ë“¤ (ì›¹í¬ë¡¤ë§ ê¸°ëŠ¥ ì¶”ê°€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ResearchPlannerAgent:
    """ì—°êµ¬ ê³„íš ë° ì „ëµ ìˆ˜ë¦½ ì—ì´ì „íŠ¸"""
    def __init__(self, llm_tokenizer, llm_model, device_config):
        self.tokenizer = llm_tokenizer
        self.model = llm_model
        self.device_config = device_config

    def analyze_query_complexity(self, query: str) -> float:
        indicators = ["ë¹„êµ","ë¶„ì„","í‰ê°€","ê²€í† ","ì—°ê´€","ê´€ê³„","ì˜í–¥",
                      "ì›ì¸","ê²°ê³¼","ì–´ë–»ê²Œ","ì™œ","ì–¸ì œ","ì–´ë””ì„œ","ëˆ„ê°€",
                      "ë¬´ì—‡ì„","ìƒì„¸","êµ¬ì²´ì ","ìµœì‹ ","í˜„ì¬","ë™í–¥","íŠ¸ë Œë“œ"]
        score = 0.3
        if len(query) > 50: score += 0.2
        cnt = sum(1 for i in indicators if i in query)
        score += min(cnt * 0.1, 0.3)
        if "?" in query: score += 0.1
        if any(w in query for w in ["ê·¸ë¦¬ê³ ","ë˜í•œ","í•˜ì§€ë§Œ","ê·¸ëŸ¬ë‚˜"]): score += 0.1
        return min(score,1.0)

    def generate_research_plan(self, query: str, state: ResearchState) -> List[SearchQuery]:
        prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì²´ê³„ì ì¸ ì—°êµ¬ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”:
ì§ˆë¬¸: {query}
ë‹¤ìŒ ë‹¨ê³„ë¡œ êµ¬ì„±ëœ ê²€ìƒ‰ ê³„íšì„ ìƒì„±í•˜ì„¸ìš”:
1. í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
2. í•˜ìœ„ ì§ˆë¬¸ ë¶„í•´
3. ìš°ì„ ìˆœìœ„ ì„¤ì •
4. ì›¹ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨
ê° ê²€ìƒ‰ ì¿¼ë¦¬ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìƒì„±:
- ê²€ìƒ‰ì–´: [êµ¬ì²´ì  ê²€ìƒ‰ì–´]
- ìš°ì„ ìˆœìœ„: [1-10ì ]
- ì¹´í…Œê³ ë¦¬: [ì£¼ìš”ê°œë…/ì„¸ë¶€ì‚¬í•­/ë°°ê²½ì§€ì‹/ë¹„êµë¶„ì„/ìµœì‹ ì •ë³´]
- ì´ìœ : [ì™œ ì´ ê²€ìƒ‰ì´ í•„ìš”í•œì§€]
- ì›¹ê²€ìƒ‰: [YES/NO]
ìµœëŒ€ 5ê°œì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”."""
        messages = [
            {"role":"system","content":"ë‹¹ì‹ ì€ ì—°êµ¬ ê³„íšì„ ìˆ˜ë¦½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. PDF ë¬¸ì„œë¿ë§Œ ì•„ë‹ˆë¼ ì›¹ ê²€ìƒ‰ë„ ê³ ë ¤í•˜ì„¸ìš”."},
            {"role":"user","content":prompt}
        ]
        response = self._generate_llm_response(messages, max_tokens=1000)
        return self._parse_search_queries(response, state)

    def _parse_search_queries(self, response: str, state: ResearchState) -> List[SearchQuery]:
        queries, curr = [], {}
        for line in response.split("\n"):
            line = line.strip()
            if line.startswith("- ê²€ìƒ‰ì–´:"):
                curr["text"] = line.replace("- ê²€ìƒ‰ì–´:","").strip()
            elif line.startswith("- ìš°ì„ ìˆœìœ„:"):
                try:
                    curr["priority"] = float(re.findall(r"\d+", line)[0]) / 10.0
                except:
                    curr["priority"] = 0.5
            elif line.startswith("- ì¹´í…Œê³ ë¦¬:"):
                curr["category"] = line.replace("- ì¹´í…Œê³ ë¦¬:","").strip()
            elif line.startswith("- ì´ìœ :"):
                curr["reason"] = line.replace("- ì´ìœ :","").strip()
            elif line.startswith("- ì›¹ê²€ìƒ‰:"):
                val = line.replace("- ì›¹ê²€ìƒ‰:","").strip().upper()
                curr["search_web"] = "YES" in val
            if all(k in curr for k in ["text","priority","category","reason"]):
                if "search_web" not in curr:
                    curr["search_web"] = True
                queries.append(SearchQuery(**curr))
                curr = {}
        if not queries:
            queries.append(SearchQuery(text=state.query, priority=1.0, category="ì£¼ìš”ê°œë…", reason="ê¸°ë³¸ ê²€ìƒ‰", search_web=True))
        return queries[:10]

    def identify_knowledge_gaps(self, state: ResearchState) -> List[str]:
        if not state.retrieved_docs and not state.web_docs:
            return ["ê¸°ì´ˆ ì •ë³´ ë¶€ì¡±"]
        texts = ""
        if state.retrieved_docs:
            texts += "\n".join(d.get("text","")[:200] for d in state.retrieved_docs[:3])
        if state.web_docs:
            texts += "\n".join(d.text[:200] for d in state.web_docs[:3])
        prompt = f"""ë‹¤ìŒ ì—°êµ¬ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ë¶€ì¡±í•œ ì •ë³´ë¥¼ ì‹ë³„í•˜ì„¸ìš”:
ì›ë³¸ ì§ˆë¬¸: {state.query}
í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ì •ë³´ (PDF + ì›¹):
{texts[:1000]}...
ë¶€ì¡±í•œ ì •ë³´ë‚˜ ì¶”ê°€ ì¡°ì‚¬ê°€ í•„ìš”í•œ ì˜ì—­ì„ ìµœëŒ€ 3ê°œê¹Œì§€ ë‚˜ì—´í•˜ì„¸ìš”."""
        messages = [
            {"role":"system","content":"ë‹¹ì‹ ì€ ì—°êµ¬ ê²©ì°¨ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role":"user","content":prompt}
        ]
        resp = self._generate_llm_response(messages, max_tokens=500)
        gaps = [g.strip() for g in resp.split("\n") if g.strip()]
        return gaps[:3]

    def _generate_llm_response(self, messages: List[Dict], max_tokens: int) -> str:
        try:
            input_ids = self.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")
            if self.device_config.device == "cuda":
                input_ids = input_ids.to("cuda")
            gen_kwargs = {
                "max_new_tokens": max_tokens,
                "do_sample": False,
                "repetition_penalty":1.1,
                "pad_token_id":self.tokenizer.eos_token_id,
                "eos_token_id":self.tokenizer.eos_token_id
            }
            with torch.no_grad():
                output = self.model.generate(input_ids, **gen_kwargs)
            return self.tokenizer.decode(output[0][input_ids.shape[1]:], skip_special_tokens=True).strip()
        except Exception as e:
            st.warning(f"LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""

class RetrieverAgent:
    """ë¬¸ì„œ ê²€ìƒ‰ ì „ë¬¸ ì—ì´ì „íŠ¸ (PDF + ì›¹ í†µí•©)"""
    def __init__(self, embed_tokenizer, embed_model, reranker, device_config, web_crawler):
        self.embed_tokenizer = embed_tokenizer
        self.embed_model = embed_model
        self.reranker = reranker
        self.device_config = device_config
        self.web_crawler = web_crawler

    def multi_query_retrieval(self, search_queries: List[SearchQuery],
                              documents: List[Dict], embeddings: np.ndarray,
                              orchestrator=None) -> List[Dict]:
        all_results, web_docs_collected = [], []

        # PDF ê²€ìƒ‰
        for sq in search_queries:
            pdf_res = self._single_query_retrieval(
                sq.text, documents, embeddings,
                top_k=max(5, int(10 * sq.priority))
            )
            for r in pdf_res:
                r.update({
                    "search_priority": sq.priority,
                    "search_category": sq.category,
                    "search_reason": sq.reason,
                    "source_type": "PDF"
                })
            all_results.extend(pdf_res)

        # ì›¹ í¬ë¡¤ë§
        if WEB_CRAWLING_AVAILABLE:
            web_qs = [sq for sq in search_queries if sq.search_web]
            if web_qs:
                st.info(f"ğŸŒ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘... ({len(web_qs)}ê°œ)")
                for wq in web_qs:
                    try:
                        crawled = self.web_crawler.search_and_crawl(wq.text, max_results=8)
                        web_docs_collected.extend(crawled)
                    except Exception as e:
                        st.warning(f"ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            if web_docs_collected:
                # ğŸ’¡ ë³€ê²½ì  1: _process_web_docs í˜¸ì¶œ ì‹œ orchestrator ì¸ì ì œê±°
                web_results = self._process_web_docs(web_docs_collected)
                all_results.extend(web_results)

        unique = self._deduplicate_results(all_results)
        # ğŸ’¡ ë³€ê²½ì  3: _rerank_resultsê°€ ì›¹/PDF ì ìˆ˜ë¥¼ ë‹¤ë¥´ê²Œ ê³„ì‚°
        return self._rerank_results(search_queries[0].text if search_queries else "", unique)

    # ğŸ’¡ ë³€ê²½ì  2: _process_web_docsì—ì„œ í•˜ë“œì½”ë”©ëœ ì ìˆ˜ ì œê±°
    def _process_web_docs(self, web_docs: List[WebDocument]) -> List[Dict]:
        """í¬ë¡¤ë§ëœ ì›¹ ë¬¸ì„œë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì ìˆ˜ ê³„ì‚° ì—†ìŒ)"""
        web_results = []
        for wd in web_docs:
            web_results.append({
                "text": wd.text,
                "page": 0, "paragraph": 0,
                "source": wd.url,
                "chunk_id": wd.chunk_id,
                "search_category": "ì›¹ì •ë³´",
                "source_type": "WEB",
                "web_title": wd.title,
                "web_domain": wd.domain,
                "web_crawl_time": wd.crawl_time
                # "similarity" ì™€ "final_score" ëŠ” ì—¬ê¸°ì„œ ì„¤ì •í•˜ì§€ ì•ŠìŒ
            })
        return web_results

    def _single_query_retrieval(self, query: str, documents: List[Dict],
                                embeddings: np.ndarray, top_k: int) -> List[Dict]:
        if embeddings is None or not documents:
            return []
        enc = self.embed_tokenizer([query], padding=True, truncation=True, return_tensors="pt")
        enc = enc.to(self.device_config.device)
        with torch.no_grad():
            out = self.embed_model(**enc)
        q_emb = mean_pooling(out, enc["attention_mask"]).cpu().numpy()
        sims = cosine_similarity(q_emb, embeddings)[0]
        idxs = np.argsort(sims)[::-1][:top_k * 3]
        threshold = self.device_config.config["sim_threshold"]
        res = []
        for idx in idxs:
            if idx < len(documents) and sims[idx] >= threshold:
                doc = documents[idx].copy()
                doc["similarity"] = float(sims[idx])
                res.append(doc)
        return res[:top_k]

    def _deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        seen, unique = set(), []
        for r in results:
            # chunk_idê°€ ìˆëŠ” ê²½ìš° ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ í•´ì‹œ ì‚¬ìš©
            identifier = r.get("chunk_id")
            if not identifier:
                 identifier = hashlib.md5(r["text"].encode()).hexdigest()

            if identifier not in seen:
                seen.add(identifier)
                unique.append(r)
        return unique

    # ğŸ’¡ ë³€ê²½ì  4: _rerank_resultsì—ì„œ ì›¹/PDF ì ìˆ˜ ê³„ì‚° ë¡œì§ ë¶„ë¦¬
    def _rerank_results(self, main_query: str, results: List[Dict], top_k: int = 15) -> List[Dict]:
        """Cross-Encoderë¡œ ê²°ê³¼ ì¬í‰ê°€ (ì›¹/PDF ì ìˆ˜ ê³„ì‚° ë¶„ë¦¬)"""
        if not results:
            return []
        
        texts = [r["text"] for r in results]
        # CrossEncoderëŠ” ì •ê·œí™”ë˜ì§€ ì•Šì€ ì ìˆ˜ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¡œ 0-1 ì‚¬ì´ ê°’ìœ¼ë¡œ ë³€í™˜
        scores = torch.sigmoid(torch.tensor(self.reranker.predict([(main_query, t) for t in texts]))).tolist()

        for r, sc in zip(results, scores):
            r["rerank_score"] = float(sc)
            
            # PDFëŠ” ì„ë² ë”© ìœ ì‚¬ë„ì™€ ì¬í‰ê°€ ì ìˆ˜ë¥¼ ê²°í•©
            if r.get("source_type") == "PDF":
                sim = r.get("similarity", 0.0)
                r["final_score"] = sim * 0.4 + r["rerank_score"] * 0.6
            # WEBì€ ì¬í‰ê°€ ì ìˆ˜ë¥¼ ìµœì¢… ì ìˆ˜ë¡œ ì‚¬ìš©
            else: 
                r["similarity"] = 0.0 # ì´ˆê¸° ìœ ì‚¬ë„ ì—†ìŒ
                r["final_score"] = r["rerank_score"]

        results.sort(key=lambda x: x["final_score"], reverse=True)
        return results[:top_k]


class SynthesizerAgent:
    """ì •ë³´ í†µí•© ë° ìµœì¢… ë‹µë³€ ìƒì„± ì—ì´ì „íŠ¸ (ì›¹ ì¶œì²˜ í‘œì‹œ ê¸°ëŠ¥ ê°•í™”)"""
    def __init__(self, llm_tokenizer, llm_model, device_config):
        self.tokenizer = llm_tokenizer
        self.model = llm_model
        self.device_config = device_config

    def synthesize_comprehensive_answer(self, query: str, state: ResearchState,
                                       analysis_results: Dict) -> str:
        total_docs = len(state.retrieved_docs) + len(state.web_docs)
        complexity = total_docs * 0.1 + len(state.insights) * 0.2
        cfg = self.device_config.get_adaptive_config(complexity)
        prompt = self._build_synthesis_prompt(query, state, analysis_results)
        messages = [
            {"role":"system","content":self._get_synthesis_system_prompt()},
            {"role":"user","content":prompt}
        ]
        return self._generate_final_answer(messages, cfg)
    def _build_synthesis_prompt(self, query: str, state: ResearchState, analysis_results: Dict) -> str:
        # PDF ë¬¸ì„œ ìš”ì•½ì— ì¸ìš© íƒœê·¸ ìë™ í• ë‹¹
        pdf_docs = [d for d in state.retrieved_docs if d.get("source_type") == "PDF"][:5]
        pdf_summaries = []
        for idx, d in enumerate(pdf_docs, 1):
            tag = f"[{idx}]"
            pdf_summaries.append(
                f"{tag} íŒŒì¼: {d['source']}, í˜ì´ì§€:{d['page']}, ë‹¨ë½:{d['paragraph']}, ê´€ë ¨ë„:{d['final_score']:.2f}\n"
                f"ë‚´ìš©: {d['text'][:200]}...\n"
            )
    
        # ì›¹ ë¬¸ì„œ ìš”ì•½ì— ì¸ìš© íƒœê·¸ ìë™ í• ë‹¹
        web_docs = [d for d in state.retrieved_docs if d.get("source_type") == "WEB"][:5]
        web_summaries = []
        offset = len(pdf_summaries)
        for j, d in enumerate(web_docs, 1):
            tag = f"[{offset + j}]"
            web_summaries.append(
                f"{tag} ë„ë©”ì¸: {d['web_domain']}, URL: {d['source']}, í¬ë¡¤ë§ì‹œê°„:{d['web_crawl_time']}, ê´€ë ¨ë„:{d['final_score']:.2f}\n"
                f"ë‚´ìš©: {d['text'][:200]}...\n"
            )
    
        # êµì°¨ê²€ì¦ ê²°ê³¼ ìš”ì•½
        analysis_section = (
            f"êµì°¨ ê²€ì¦ ê²°ê³¼:\n"
            f"- ì •ë³´ ì¼ê´€ì„±: {analysis_results.get('consistency', 0.5):.2f}\n"
            f"- ìƒì¶© ì •ë³´: {len(analysis_results.get('conflicts', []))}ê±´\n"
            f"- ê³µí†µ ì •ë³´: {len(analysis_results.get('consensus', []))}ê±´\n"
        )
    
        # ì—°êµ¬ ì§„í–‰ í˜„í™©
        status_section = (
            f"ì—°êµ¬ ì§„í–‰ í˜„í™©:\n"
            f"- íƒìƒ‰ ì‚¬ì´í´: {state.cycle_count + 1}/{state.max_cycles}\n"
            f"- ë°œê²¬ëœ ì¸ì‚¬ì´íŠ¸: {len(state.insights)}ê°œ\n"
            f"- ì‹ë³„ëœ ì§€ì‹ ê²©ì°¨: {len(state.gaps)}ê°œ\n"
            f"- PDF ë¬¸ì„œ: {len(pdf_docs)}ê°œ\n"
            f"- ì›¹ ë¬¸ì„œ: {len(web_docs)}ê°œ\n"
        )
    
        pdf_block = "".join(pdf_summaries) if pdf_summaries else "PDF ë¬¸ì„œ ì—†ìŒ\n"
        web_block = "".join(web_summaries) if web_summaries else "ì›¹ ë¬¸ì„œ ì—†ìŒ\n"
    
        return f"""ë‹¤ìŒ ì—°êµ¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ì¢…í•©ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”:
    
    ì›ë³¸ ì§ˆë¬¸: {query}
    
    === PDF ë¬¸ì„œ ìš”ì•½ ===
    {pdf_block}
    
    === ì›¹ ë¬¸ì„œ ìš”ì•½ ===
    {web_block}
    
    === ë¶„ì„ ê²°ê³¼ ===
    {analysis_section}
    {status_section}
    
    === ì¶œì²˜ ë° ì¸ìš© íƒœê·¸ ì•ˆë‚´ ===
    - ìœ„ ìš”ì•½ ë¸”ë¡ì—ì„œ ì •ì˜ëœ ì¸ìš© íƒœê·¸([1], [2], â€¦)ë¥¼ ë¬¸ì¥ ëì— ë°˜ë“œì‹œ ì¬ì‚¬ìš©í•˜ì„¸ìš”.
    - PDF ì¸ìš© í˜•ì‹: [íŒŒì¼ëª…, í˜ì´ì§€:ë‹¨ë½, ê´€ë ¨ë„: X.XX]
    - ì›¹ ì¸ìš© í˜•ì‹: [ë„ë©”ì¸, URL, í¬ë¡¤ë§ì‹œê°„, ê´€ë ¨ë„: X.XX]
    - ì˜ˆì‹œ:
      > â€œAI ì‹œì¥ì€ ì—°í‰ê·  20% ì„±ì¥í•  ì „ë§ì…ë‹ˆë‹¤.â€ [example.pdf, p.3:2, 0.87]
    
    === ì‘ì„± ì§€ì¹¨ ===
    1. ê° ë¬¸ì¥ë§ˆë‹¤ í•´ë‹¹ ì¸ìš© íƒœê·¸ë¥¼ ë¶™ì—¬ ê·¼ê±°ë¥¼ ëª…í™•íˆ í‘œì‹œí•˜ì„¸ìš”.
    2. ìƒì¶©ë˜ëŠ” ì •ë³´ê°€ ìˆìœ¼ë©´ íƒœê·¸ì™€ í•¨ê»˜ ê°ê´€ì ìœ¼ë¡œ ì œì‹œí•˜ì„¸ìš”.
    3. ë¶€ì¡±í•œ ì •ë³´ê°€ ìˆìœ¼ë©´ ì†”ì§íˆ ì–¸ê¸‰í•˜ì„¸ìš”.
    4. ë‹µë³€ ë§ë¯¸ì— ì¢…í•© ì‹ ë¢°ë„ë¥¼ ì œì‹œí•˜ì„¸ìš”.
    
    ë‹µë³€:
    """

    def _get_synthesis_system_prompt(self) -> str:
        return """ë‹¹ì‹ ì€ ë‹¤ì¤‘ ì†ŒìŠ¤ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì •í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì—°êµ¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
í•µì‹¬ ì›ì¹™:
1. ì œê³µëœ PDF ë° ì›¹ ë¬¸ì„œ ì •ë³´ë§Œ ì‚¬ìš©
2. ëª¨ë“  ì£¼ì¥ì— ëŒ€í•œ ì •í™•í•œ ì¶œì²˜ ëª…ì‹œ í•„ìˆ˜
3. PDFì™€ ì›¹ ì¶œì²˜ë¥¼ êµ¬ë¶„í•˜ì—¬ ë°˜ë“œì‹œ í‘œì‹œ
4. ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì‹ ë¢°ë„ì™€ í•¨ê»˜ ì œì‹œ
5. ìƒì¶©ë˜ëŠ” ì •ë³´ëŠ” ê°ê´€ì ìœ¼ë¡œ ì œì‹œ
6. ì§€ì‹ ê²©ì°¨ëŠ” ì†”ì§í•˜ê²Œ ì¸ì •
7. ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ êµ¬ì¡°ë¡œ ë‹µë³€
ì¶œì²˜ í‘œì‹œ í˜•ì‹:

- PDF: [íŒŒì¼ëª…, í˜ì´ì§€:ë‹¨ë½, ê´€ë ¨ë„: 0.XX]
- ì›¹: [ë„ë©”ì¸, URL, í¬ë¡¤ë§ì‹œê°„, ê´€ë ¨ë„: 0.XX]

ë³¸ë¬¸ ì‘ì„± ì§€ì‹œ:
- ì•ì„œ ìš”ì•½ ë¸”ë¡ì—ì„œ ì •ì˜ëœ ì¸ìš© íƒœê·¸([1], , â€¦)ë¥¼ **ë¬¸ì¥ ëì— ë°˜ë“œì‹œ ì¬ì‚¬ìš©**.
- ì˜ˆì‹œ:
  > â€œAI ì‹œì¥ì€ ì—°í‰ê·  20% ì„±ì¥í•  ì „ë§ì…ë‹ˆë‹¤.â€ [example.pdf, p.3:2, 0.87]
ë‹µë³€ êµ¬ì¡°(ì¶œì²˜ ë°˜ë“œì‹œ í‘œì‹œ):
- í•µì‹¬ ë‹µë³€ (ìš”ì•½)
- ìƒì„¸ ì„¤ëª… (ê·¼ê±°ì™€ ì¶œì²˜ í•¨ê»˜)
- ì¶”ê°€ ê³ ë ¤ì‚¬í•­ (í•œê³„ì  í¬í•¨)
- ì‹ ë¢°ë„ í‰ê°€
ì›¹ ì¶œì²˜ì˜ ê²½ìš° ë°˜ë“œì‹œ URLê³¼ í¬ë¡¤ë§ ì‹œê°„ì„ í¬í•¨í•˜ì—¬ ë…ìê°€ ì •ë³´ì˜ ì¶œì²˜ì™€ ì‹œì ì„ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ í•˜ì„¸ìš”."""
    def _generate_final_answer(self, messages: List[Dict], config: Dict) -> str:
        try:
            input_ids = self.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")
            if self.device_config.device == "cuda":
                input_ids = input_ids.to("cuda")
            gen_kwargs = {
                "max_new_tokens": config["max_new_tokens"],
                "do_sample": config.get("do_sample", False),
                "repetition_penalty":1.1,
                "pad_token_id":self.tokenizer.eos_token_id,
                "eos_token_id":self.tokenizer.eos_token_id
            }
            if config.get("do_sample"):
                gen_kwargs.update({"temperature":config.get("temperature",0.1),"top_p":config.get("top_p",0.9)})
            with torch.no_grad():
                output = self.model.generate(input_ids, **gen_kwargs)
            return self.tokenizer.decode(output[0][input_ids.shape[1]:], skip_special_tokens=True).strip()
        except Exception as e:
            st.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

class DeepResearchOrchestrator:
    """Deep Research ìŠ¤íƒ€ì¼ì˜ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í˜‘ì—… ì‹œìŠ¤í…œ (ì›¹í¬ë¡¤ë§ í†µí•©)"""
    def __init__(
        self,
        model_name: str = "LGAI-EXAONE/EXAONE-4.0-1.2B",
        embed_model: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        reranker_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2",
        min_chunk_length: int = 50,
        max_chunk_length: int = 300,
        sentences_per_chunk: int = 2,
    ):
        st.info("ğŸš€ Deep Research ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘... (ì›¹í¬ë¡¤ë§ ê¸°ëŠ¥ í¬í•¨)")
        if not WEB_CRAWLING_AVAILABLE:
            st.warning("âš ï¸ ì›¹í¬ë¡¤ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. pip install requests beautifulsoup4ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device_config = DeviceConfig()
        self.pdf_extractor = ImprovedPDFExtractor()
        self.chunker = ImprovedKoreanSentenceChunker(
            min_chunk_length=min_chunk_length,
            max_chunk_length=max_chunk_length,
            sentences_per_chunk=sentences_per_chunk
        )
        self.web_crawler = WebCrawler() if WEB_CRAWLING_AVAILABLE else None
        self._load_models()
        self._initialize_agents()
        self.documents: List[Dict[str, Any]] = []
        self.embeddings: Optional[np.ndarray] = None
        self.chunk_hashes: Dict[str, Dict] = {}
        self.loaded_pdfs: List[str] = []
        st.success("âœ… Deep Research ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! (ì›¹í¬ë¡¤ë§ ê¸°ëŠ¥ í¬í•¨)")

    def _load_models(self):
        st.info("â–¶ ì„ë² ë”© ëª¨ë¸ ë¡œë”©...")
        self.embed_tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        self.embed_model = AutoModel.from_pretrained("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2").to(self.device_config.device)
        self.reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2", device=self.device_config.device)
        st.info("â–¶ LLM ë¡œë”©...")
        self.tokenizer = AutoTokenizer.from_pretrained("LGAI-EXAONE/EXAONE-4.0-1.2B", trust_remote_code=True)
        map_dev = "auto" if self.device_config.device == "cuda" else "cpu"
        self.model = AutoModelForCausalLM.from_pretrained(
            "LGAI-EXAONE/EXAONE-4.0-1.2B",
            torch_dtype=self.device_config.config["torch_dtype"],
            device_map=map_dev,
            max_memory={0: "14GB"},
            trust_remote_code=True
        ).to(self.device)
        torch.cuda.empty_cache(); gc.collect()

    def _initialize_agents(self):
        st.info("â–¶ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
        self.planner = ResearchPlannerAgent(self.tokenizer, self.model, self.device_config)
        self.retriever = RetrieverAgent(self.embed_tokenizer, self.embed_model, self.reranker, self.device_config, self.web_crawler)
        self.synthesizer = SynthesizerAgent(self.tokenizer, self.model, self.device_config)

    def load_pdf_documents(self, pdf_paths: List[str]) -> None:
        st.info(f"ğŸ“š PDF ë¬¸ì„œ ì²˜ë¦¬ ì¤‘... ({len(pdf_paths)}ê°œ)")
        all_docs: List[Dict[str, Any]] = []
        progress_bar = st.progress(0)
        for i, pdf_path in enumerate(pdf_paths):
            if not os.path.exists(pdf_path):
                st.warning(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {pdf_path}")
                continue
            full_text = self.pdf_extractor.extract_text_from_pdf(pdf_path)
            if not full_text.strip():
                st.warning(f"âš ï¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ: {pdf_path}")
                continue
            chunks = self.chunker.chunk_text(full_text)
            for idx, chunk in enumerate(chunks,1):
                if chunk.strip():
                    all_docs.append({
                        "text": chunk.strip(),
                        "page": 1,
                        "paragraph": idx,
                        "source": os.path.basename(pdf_path),
                        "full_path": pdf_path,
                        "source_type": "PDF"
                    })
            self.loaded_pdfs.append(pdf_path)
            progress_bar.progress((i+1)/len(pdf_paths))
        if not all_docs:
            st.error("âŒ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        self.documents = all_docs
        st.info("ğŸ§® ì„ë² ë”© ìƒì„± ì¤‘...")
        self._generate_embeddings()
        for d in self.documents:
            cid = hashlib.md5(f"{d['source']}_{d['page']}_{d['paragraph']}_{d['text']}".encode()).hexdigest()[:8]
            d["chunk_id"] = cid
            self.chunk_hashes[cid] = d
        st.success(f"ğŸ‰ ì²˜ë¦¬ ì™„ë£Œ! ì´ {len(self.documents)}ê°œ ì²­í¬ ìƒì„±")

    def _generate_embeddings(self):
        texts = [d["text"] for d in self.documents]
        bs = self.device_config.config["embedding_batch_size"]
        embs = []
        progress_bar = st.progress(0)
        for i in range(0, len(texts), bs):
            batch = texts[i:i+bs]
            enc = self.embed_tokenizer(batch, padding=True, truncation=True, return_tensors="pt").to(self.device_config.device)
            with torch.no_grad():
                out = self.embed_model(**enc)
            pooled = mean_pooling(out, enc["attention_mask"])
            embs.append(pooled.cpu().numpy())
            progress_bar.progress(min(1.0, (i+bs)/len(texts)))
        self.embeddings = np.vstack(embs)

    def deep_research(self, query: str) -> Dict[str, Any]:
        st.info("ğŸ” Deep Research í”„ë¡œì„¸ìŠ¤ ì‹œì‘... (PDF + ì›¹)")
        complexity = self.planner.analyze_query_complexity(query)
        max_cycles = 2 if complexity < 0.5 else 3
        state = ResearchState(
            phase=ResearchPhase.PLANNING,
            query=query,
            sub_queries=[],
            retrieved_docs=[],
            web_docs=[],
            confidence_history=[],
            insights=[],
            gaps=[],
            max_cycles=max_cycles
        )
        research_log, search_queries = [], []
        for cycle in range(state.max_cycles):
            state.cycle_count = cycle
            research_log.append(f"=== ì‚¬ì´í´ {cycle+1} ===")
            # Phase1: ê³„íš
            if cycle == 0:
                st.info(f"ğŸ“‹ ì—°êµ¬ ê³„íš ìˆ˜ë¦½ ì¤‘... (ë³µì¡ë„: {complexity:.2f})")
                state.phase = ResearchPhase.PLANNING
                search_queries = self.planner.generate_research_plan(query, state)
                state.sub_queries = [sq.text for sq in search_queries]
                web_cnt = sum(1 for sq in search_queries if sq.search_web)
                research_log.append(f"ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬: {len(search_queries)}ê°œ (ì›¹ê²€ìƒ‰: {web_cnt}ê°œ)")
            # Phase2: ê²€ìƒ‰
            st.info(f"ğŸ” ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘... (ì¿¼ë¦¬ {len(search_queries)}ê°œ)")
            state.phase = ResearchPhase.INITIAL_RETRIEVAL if cycle==0 else ResearchPhase.DEEP_ANALYSIS
            retrieved = self.retriever.multi_query_retrieval(search_queries, self.documents, self.embeddings, self)
            existing = {d.get("chunk_id","") for d in state.retrieved_docs}
            new_docs = [d for d in retrieved if d.get("chunk_id","") not in existing]
            state.retrieved_docs.extend(new_docs)
            web_docs = [d for d in new_docs if d.get("source_type")=="WEB"]
            pdf_docs = [d for d in new_docs if d.get("source_type")=="PDF"]
            research_log.append(f"ê²€ìƒ‰ëœ ë¬¸ì„œ: PDF {len(pdf_docs)}ê°œ, ì›¹ {len(web_docs)}ê°œ")
            # ì¤‘ê°„ ì‹ ë¢°ë„
            conf = self._calculate_interim_confidence(state)
            state.confidence_history.append(conf)
            if conf > 0.85 and len(state.retrieved_docs)>=5:
                research_log.append(f"ë†’ì€ ì‹ ë¢°ë„ ë‹¬ì„± ({conf:.2f}) - ì¡°ê¸° ì¢…ë£Œ")
                break

        # Phase2 ì¢…ë£Œ â†’ ì‹¤ì œ êµì°¨ê²€ì¦ ìˆ˜í–‰
        st.info("âœ… êµì°¨ê²€ì¦ ìˆ˜í–‰ ì¤‘... (PDF + ì›¹)")
        state.phase = ResearchPhase.CROSS_VALIDATION
        analysis_results = self.analyzer.cross_validate_information(state.retrieved_docs[:10])

# Phase3: ì¢…í•©ë‹µë³€
        st.info("ğŸ“ ì¢…í•© ë‹µë³€ ìƒì„± ì¤‘... (PDF + ì›¹)")
        state.phase = ResearchPhase.SYNTHESIS
        answer = self.synthesizer.synthesize_comprehensive_answer(query, state, analysis_results)


        # ê²°ê³¼ í¬ë§·
        sources = []
        for d in state.retrieved_docs[:15]:
            info = {
                "page": d.get("page",0),
                "paragraph": d.get("paragraph",0),
                "chunk_id": d.get("chunk_id",""),
                "source_file": d.get("source","Unknown"),
                "source_type": d.get("source_type","PDF"),
                "preview": d["text"][:400]+"..." if len(d["text"])>400 else d["text"],
                "similarity": float(d.get("final_score",d.get("similarity",0))),
                "chunk_size": len(d["text"]),
                "search_category": d.get("search_category","ì¼ë°˜"),
                "key_insight": d.get("key_insight","")
            }
            if info["source_type"]=="WEB":
                info.update({
                    "web_title": d.get("web_title","ì œëª© ì—†ìŒ"),
                    "web_domain": d.get("web_domain","Unknown"),
                    "web_crawl_time": d.get("web_crawl_time","Unknown")
                })
            sources.append(info)

        return {
            "answer": answer,
            "confidence": state.confidence_history[-1] if state.confidence_history else 0.0,
            "warnings": [],
            "sources": sources,
            "research_metadata": {
                "cycles_completed": state.cycle_count+1,
                "total_documents_analyzed": len(state.retrieved_docs),
                "pdf_documents": sum(1 for s in sources if s["source_type"]=="PDF"),
                "web_documents": sum(1 for s in sources if s["source_type"]=="WEB"),
                "insights_discovered": len(state.insights),
                "knowledge_gaps_identified": len(state.gaps),
                "query_complexity": complexity,
                "confidence_progression": state.confidence_history,
                "research_log": research_log,
                "web_crawling_enabled": WEB_CRAWLING_AVAILABLE
            }
        }

    def _calculate_interim_confidence(self, state: ResearchState) -> float:
        if not state.retrieved_docs:
            return 0.0
        scores = [d.get("final_score", d.get("similarity",0.5)) for d in state.retrieved_docs]
        avg = np.mean(scores) if scores else 0.5
        pdf_src = len(set(d["source"] for d in state.retrieved_docs if d["source_type"]=="PDF"))
        web_src = len(set(d["source"] for d in state.retrieved_docs if d["source_type"]=="WEB"))
        div = min((pdf_src + web_src) / 5.0, 1.0)
        ins = min(len(state.insights)/5.0,1.0)
        return avg*0.5 + div*0.3 + ins*0.2

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. Streamlit UI (ì›¹í¬ë¡¤ë§ ê¸°ëŠ¥ ì¶”ê°€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.write("CUDA available:", torch.cuda.is_available())
st.write("ì›¹í¬ë¡¤ë§ ê¸°ëŠ¥:", "âœ… ì‚¬ìš© ê°€ëŠ¥" if WEB_CRAWLING_AVAILABLE else "âŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”")
st.title("ğŸ§  Deep Research Chatbot with Web Crawling by C.H.PARK")
st.markdown("### AI ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í˜‘ì—… ì‹œìŠ¤í…œ + ì‹¤ì‹œê°„ ì›¹í¬ë¡¤ë§ìœ¼ë¡œ êµ¬í˜„í•œ í•˜ì´ë¸Œë¦¬ë“œ RAG")

st.sidebar.header("âš™ï¸ ì„¤ì •")
st.sidebar.subheader("ì²­í‚¹ ì„¤ì •")
min_chunk_length = st.sidebar.slider("ìµœì†Œ ì²­í¬ ê¸¸ì´", 30, 500, 50)
max_chunk_length = st.sidebar.slider("ìµœëŒ€ ì²­í¬ ê¸¸ì´", 200, 3000, 300)
sentences_per_chunk = st.sidebar.slider("ì²­í¬ë‹¹ ìµœëŒ€ ë¬¸ì¥ ìˆ˜", 1, 10, 2)

st.sidebar.subheader("ğŸŒ ì›¹í¬ë¡¤ë§ ì„¤ì •")
enable_web_crawling = st.sidebar.checkbox("ì›¹í¬ë¡¤ë§ í™œì„±í™”", value=WEB_CRAWLING_AVAILABLE)

st.sidebar.subheader("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ")
uploaded_files = st.sidebar.file_uploader(
    "PDF íŒŒì¼ ì—…ë¡œë“œ",
    type="pdf",
    accept_multiple_files=True,
    help="ì—¬ëŸ¬ PDF íŒŒì¼ì„ ë™ì‹œì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
)

if st.sidebar.button("ğŸ”„ Deep Research ì‹œìŠ¤í…œ ì‹œì‘", type="primary"):
    if not uploaded_files:
        st.sidebar.error("âŒ ìµœì†Œ í•˜ë‚˜ì˜ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    else:
        pdf_paths = []
        temp_dir = "temp_uploads"
        os.makedirs(temp_dir, exist_ok=True)
        for f in uploaded_files:
            path = os.path.join(temp_dir, f.name)
            with open(path, "wb") as wf:
                wf.write(f.getbuffer())
            pdf_paths.append(path)
        with st.spinner("ğŸ”„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘..."):
            try:
                st.session_state.research_bot = DeepResearchOrchestrator(
                    min_chunk_length=min_chunk_length,
                    max_chunk_length=max_chunk_length,
                    sentences_per_chunk=sentences_per_chunk
                )
                st.session_state.research_bot.load_pdf_documents(pdf_paths)
                st.sidebar.success("âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! (PDF + ì›¹)")
                st.sidebar.info(f"ğŸ“Š ì´ {len(st.session_state.research_bot.documents)}ì²­í¬ ìƒì„±ë¨")
                st.sidebar.info(f"ğŸ“ {len(st.session_state.research_bot.loaded_pdfs)}íŒŒì¼ ë¡œë”©ë¨")
                st.sidebar.info(f"ğŸŒ ì›¹í¬ë¡¤ë§: {'ON' if WEB_CRAWLING_AVAILABLE else 'OFF'}")
            except Exception as e:
                st.sidebar.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

if "research_bot" in st.session_state:
    bot = st.session_state.research_bot
    col1, col2, col3, col4, col5 = st.columns(5)
    with col1:
        st.metric("ğŸ“ ë¡œë”©ëœ íŒŒì¼", len(bot.loaded_pdfs))
    with col2:
        st.metric("ğŸ“„ PDF ì²­í¬", len(bot.documents))
    with col3:
        avg_size = np.mean([len(d["text"]) for d in bot.documents]) if bot.documents else 0
        st.metric("ğŸ“ í‰ê·  ì²­í¬ í¬ê¸°", f"{avg_size:.0f}ì")
    with col4:
        st.metric("ğŸ¤– í™œì„± ì—ì´ì „íŠ¸", "5ê°œ")
    with col5:
        st.metric("ì›¹í¬ë¡¤ë§", "ğŸŒ ON" if WEB_CRAWLING_AVAILABLE else "ğŸŒ OFF")

    st.divider()
    st.subheader("ğŸ’¬ í•˜ì´ë¸Œë¦¬ë“œ Deep Research ì§ˆë¬¸")
    st.info("ğŸ’¡ PDF ë¬¸ì„œ ë¶„ì„ê³¼ ìµœì‹  ì›¹ ì •ë³´ë¥¼ í•¨ê»˜ í™œìš©í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤.")
    query = st.text_input("ì‹¬ì¸µ ì—°êµ¬í•  ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”:", key="hybrid_query_input")

    if st.button("ğŸ§  í•˜ì´ë¸Œë¦¬ë“œ Research ì‹œì‘", type="primary") and query:
        with st.spinner("ğŸ§  ì—°êµ¬ ìˆ˜í–‰ ì¤‘..."):
            start = time.time()
            result = bot.deep_research(query)
            elapsed = time.time() - start
            torch.cuda.empty_cache(); gc.collect()
            if "research_history" not in st.session_state:
                st.session_state.research_history = []
            st.session_state.research_history.append({
                "query": query,
                "result": result,
                "timestamp": datetime.now(),
                "elapsed_time": elapsed
            })

        st.subheader("ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ Deep Research ê²°ê³¼")
        st.write(result["answer"])
        md = result["research_metadata"]
        c1,c2,c3,c4,c5 = st.columns(5)
        with c1: st.metric("ì‹ ë¢°ë„", f"{result['confidence']:.3f}")
        with c2: st.metric("ì‚¬ì´í´", f"{md['cycles_completed']}/{md.get('max_cycles',3)}")
        with c3: st.metric("PDF ë¬¸ì„œ", md.get('pdf_documents',0))
        with c4: st.metric("ì›¹ ë¬¸ì„œ", md.get('web_documents',0))
        with c5: st.metric("ì†Œìš” ì‹œê°„", f"{elapsed:.1f}ì´ˆ")

        if result["sources"]:
            pdf_srcs = [s for s in result["sources"] if s["source_type"]=="PDF"]
            web_srcs = [s for s in result["sources"] if s["source_type"]=="WEB"]
            colp, colw = st.columns(2)
            with colp:
                if pdf_srcs:
                    st.subheader("ğŸ“„ PDF ë¬¸ì„œ ì†ŒìŠ¤")
                    for i, s in enumerate(pdf_srcs[:5],1):
                        with st.expander(f"PDF {i}: {s['source_file']} (ì ìˆ˜: {s['similarity']:.3f})"):
                            st.write(f"**íŒŒì¼**: {s['source_file']}")
                            st.write(f"**í˜ì´ì§€**: {s['page']}, **ë‹¨ë½**: {s['paragraph']}")
                            st.write(f"**ë¯¸ë¦¬ë³´ê¸°**: {s['preview'][:200]}...")
                            if s.get("key_insight"):
                                st.write(f"ğŸ’¡ **ì¸ì‚¬ì´íŠ¸**: {s['key_insight']}")
            with colw:
                if web_srcs:
                    st.subheader("ğŸŒ ì›¹ ë¬¸ì„œ ì†ŒìŠ¤")
                    for i, s in enumerate(web_srcs[:5],1):
                        with st.expander(f"ì›¹ {i}: {s['web_title']} (ì ìˆ˜: {s['similarity']:.3f})"):
                            st.write(f"**ì œëª©**: {s['web_title']}")
                            st.write(f"**URL**: {s['source_file']}")
                            st.write(f"**ë„ë©”ì¸**: {s['web_domain']}")
                            st.write(f"**í¬ë¡¤ë§ ì‹œê°„**: {s['web_crawl_time']}")
                            st.write(f"**ë¯¸ë¦¬ë³´ê¸°**: {s['preview'][:200]}...")
        with st.expander("ğŸ” ìƒì„¸ ì—°êµ¬ ë¡œê·¸"):
            for entry in md.get("research_log", []):
                st.write(entry)

        if "research_history" in st.session_state:
            st.divider()
            st.subheader("ğŸ“œ í•˜ì´ë¸Œë¦¬ë“œ ì—°êµ¬ ê¸°ë¡")
            for rec in reversed(st.session_state.research_history[-3:]):
                with st.expander(f"ì—°êµ¬: {rec['query'][:50]}..."):
                    st.write(f"**ì§ˆë¬¸**: {rec['query']}")
                    st.write(f"**ë‹µë³€ ì¼ë¶€**: {rec['result']['answer'][:300]}...")
                    st.write(f"**ì‹œê°„**: {rec['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}")
                    st.write(f"**ì‹ ë¢°ë„**: {rec['result']['confidence']:.3f}")
    else:
        st.info("ğŸ‘† ì™¼ìª½ì—ì„œ PDF ì—…ë¡œë“œ í›„ ì‹œìŠ¤í…œ ì‹œì‘ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

    st.subheader("ğŸ†• í•˜ì´ë¸Œë¦¬ë“œ Deep Researchì˜ ìƒˆë¡œìš´ ê¸°ëŠ¥")
    c1, c2 = st.columns(2)
    with c1:
        st.markdown("""
**ğŸ“„ PDF ë¬¸ì„œ ë¶„ì„**
- ì—…ë¡œë“œëœ ë¬¸ì„œ ì‹¬ì¸µ ë¶„ì„
- ì²­í‚¹ ë° ì„ë² ë”©
- ì¶œì²˜: [íŒŒì¼ëª…, ê´€ë ¨ë„]
""")
    with c2:
        st.markdown("""
**ğŸŒ ì‹¤ì‹œê°„ ì›¹í¬ë¡¤ë§**
- ìµœì‹  ì •ë³´ ìë™ ê²€ìƒ‰
- ì›¹í˜ì´ì§€ í¬ë¡¤ë§
- ì¶œì²˜: [ì œëª©, URL, í¬ë¡¤ë§ì‹œê°„, ê´€ë ¨ë„]
""")
    with st.expander("ğŸ“ ì‚¬ìš© ê°€ì´ë“œ"):
        st.markdown("""
### ğŸ§  í•˜ì´ë¸Œë¦¬ë“œ Deep Research ì‹œìŠ¤í…œ
**ë‹¤ì¤‘ ì—ì´ì „íŠ¸ + ì›¹í¬ë¡¤ë§**  
- Research Planner: PDF/ì›¹ ê³„íš ìˆ˜ë¦½  
- Retriever Agent: PDF+ì›¹ í†µí•© ê²€ìƒ‰  
- Synthesizer Agent: ì¶œì²˜ë³„ ì¢…í•© ë‹µë³€  
- Validator Agent: ìµœì¢… ê²€ì¦  

**ì¶œì²˜ í‘œì‹œ**  
- PDF: [íŒŒì¼ëª…, ê´€ë ¨ë„: 0.XX]  
- ì›¹: [ì œëª©/ë„ë©”ì¸, URL, í¬ë¡¤ë§ì‹œê°„, ê´€ë ¨ë„: 0.XX]  
""")
    st.info("ğŸ’¡ PDFì˜ ì •í™•ì„±ê³¼ ì›¹ì˜ ìµœì‹ ì„±ì„ ê²°í•©í•´ë“œë¦½ë‹ˆë‹¤.")
