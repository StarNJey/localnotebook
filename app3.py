from __future__ import annotations

import os
import re
import json
import hashlib
import time
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

import streamlit as st
import numpy as np
import torch, gc
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
from sentence_transformers.cross_encoder import CrossEncoder
import warnings

# âš ï¸ ìˆ˜ì •ì‚¬í•­ 1: st.set_page_configë¥¼ ë§¨ ì•ìœ¼ë¡œ ì´ë™
st.set_page_config(page_title="Deep Research Chatbot", layout="wide")

# PDF ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False

# í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    from kiwipiepy import Kiwi
    KIWI_AVAILABLE = True
except ImportError:
    KIWI_AVAILABLE = False

warnings.filterwarnings("ignore")
os.environ["TRANSFORMERS_VERBOSITY"] = "error"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ë°ì´í„° êµ¬ì¡° ì •ì˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ResearchPhase(Enum):
    PLANNING = "planning"
    INITIAL_RETRIEVAL = "initial_retrieval"
    DEEP_ANALYSIS = "deep_analysis"
    CROSS_VALIDATION = "cross_validation"
    SYNTHESIS = "synthesis"
    FINAL_VALIDATION = "final_validation"

@dataclass
class ResearchState:
    """ì—°êµ¬ ì§„í–‰ ìƒíƒœë¥¼ ì¶”ì í•˜ëŠ” í´ë˜ìŠ¤"""
    phase: ResearchPhase
    query: str
    sub_queries: List[str]
    retrieved_docs: List[Dict[str, Any]]
    confidence_history: List[float]
    insights: List[str]
    gaps: List[str]
    cycle_count: int = 0
    max_cycles: int = 3
    
@dataclass
class SearchQuery:
    """ê²€ìƒ‰ ì¿¼ë¦¬ ì •ë³´"""
    text: str
    priority: float
    category: str
    reason: str

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output.last_hidden_state
    mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * mask_expanded, 1) / torch.clamp(mask_expanded.sum(1), min=1e-9)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ê¸°ì¡´ ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ë“¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ImprovedKoreanSentenceChunker:
    def __init__(self, min_chunk_length=50, max_chunk_length=300, sentences_per_chunk=2):
        self.min_chunk_length = min_chunk_length
        self.max_chunk_length = max_chunk_length
        self.sentences_per_chunk = sentences_per_chunk
        
        self.kiwi = None
        if KIWI_AVAILABLE:
            try:
                self.kiwi = Kiwi()
            except Exception as e:
                if 'st' in globals():
                    st.warning(f"âš ï¸ Kiwi ë¡œë“œ ì‹¤íŒ¨: {e}")

    def chunk_text(self, text: str) -> List[str]:
        if not text.strip():
            return []
        
        sentences = self._split_sentences(text)
        if not sentences:
            return []
        
        sentences = self._postprocess_sentences(sentences)
        chunks = self._create_chunks(sentences)
        
        return chunks

    def _split_sentences(self, text: str) -> List[str]:
        if KIWI_AVAILABLE and self.kiwi:
            try:
                kiwi_result = self.kiwi.split_into_sents(text.strip())
                sentences = [sent.text.strip() for sent in kiwi_result if sent.text.strip()]
                if sentences and len(sentences) > 1:
                    return sentences
            except Exception as e:
                if 'st' in globals():
                    st.warning(f"Kiwi ë¬¸ì¥ ë¶„ë¦¬ ì‹¤íŒ¨: {e}")
        
        return self._regex_sentence_split(text.strip())

    def _regex_sentence_split(self, text: str) -> List[str]:
        patterns = [
            r'[.!?]+\s+', r'[ë‹¤ê°€ë‚˜ë‹ˆê¹Œìš”ë˜ìŠµë‹ˆë‹¤]\s*[.!?]*\s+',
            r'[ë‹¤ê°€ë‚˜ë‹ˆê¹Œìš”ë˜ìŠµë‹ˆë‹¤]\s+', r'[ë‹ˆë‹¤í–ˆë‹¤ìŠµë‹ˆë‹¤ì˜€ë‹¤ì•˜ë‹¤]\s*[.!?]*\s+',
            r'\n\s*\n', r'\.\s*\n',
        ]
        
        combined_pattern = '|'.join(f'({p})' for p in patterns)
        sentences = re.split(combined_pattern, text)
        
        result = []
        for s in sentences:
            if s and not re.match(r'^\s*[.!?\n\s]*$', s):
                result.append(s.strip())
        
        return result if result else [text.strip()]

    def _postprocess_sentences(self, sentences: List[str]) -> List[str]:
        if not sentences:
            return []
        
        processed = []
        current_sentence = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            if len(current_sentence) < self.min_chunk_length:
                if current_sentence:
                    current_sentence += " " + sentence
                else:
                    current_sentence = sentence
            else:
                if current_sentence:
                    processed.append(current_sentence)
                current_sentence = sentence
        
        if current_sentence:
            if processed and len(current_sentence) < self.min_chunk_length:
                processed[-1] += " " + current_sentence
            else:
                processed.append(current_sentence)
        
        return processed

    def _create_chunks(self, sentences: List[str]) -> List[str]:
        if not sentences:
            return []
        
        chunks = []
        current_chunk = ""
        sentence_count = 0
        
        for sentence in sentences:
            test_chunk = current_chunk + (" " + sentence if current_chunk else sentence)
            
            if (len(test_chunk) <= self.max_chunk_length and 
                sentence_count < self.sentences_per_chunk):
                current_chunk = test_chunk
                sentence_count += 1
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence
                sentence_count = 1
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks

class ImprovedPDFExtractor:
    def __init__(self):
        self.available_methods = []
        if PDFPLUMBER_AVAILABLE:
            self.available_methods.append("pdfplumber")

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        if PDFPLUMBER_AVAILABLE:
            try:
                text = self._extract_with_pdfplumber(pdf_path)
                if text.strip():
                    return text
            except Exception as e:
                if 'st' in globals():
                    st.warning(f"pdfplumber ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return ""

    def _extract_with_pdfplumber(self, pdf_path: str) -> str:
        text = ""
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        return text

class DeviceConfig:
    def __init__(self):
        self.device, self.info = self._detect()
        self.config = self._get_config()

    def _detect(self):
        if torch.cuda.is_available():
            return "cuda", {
                "type": "GPU",
                "name": torch.cuda.get_device_name(0),
                "memory": torch.cuda.get_device_properties(0).total_memory // (1024**3),
                "count": torch.cuda.device_count()
            }
        try:
            import psutil
            return "cpu", {
                "type": "CPU",
                "name": "CPU",
                "cores": psutil.cpu_count(),
                "memory": psutil.virtual_memory().total // (1024**3)
            }
        except ImportError:
            return "cpu", {"type": "CPU", "name": "CPU", "cores": "Unknown", "memory": "Unknown"}

    def _get_config(self):
        if self.device == "cuda":
            mem = self.info["memory"]
            if mem >= 24:
                return {"torch_dtype": torch.bfloat16, "max_new_tokens": 8000, "top_k": 12,
                        "embedding_batch_size": 64, "do_sample": True,
                        "temperature": 0.1, "top_p": 0.9, "expected_time": "5-15ì´ˆ",
                        "sim_threshold": 0.3}
            if mem >= 12:
                return {"torch_dtype": torch.bfloat16, "max_new_tokens": 6000, "top_k": 12,
                        "embedding_batch_size": 32, "do_sample": True,
                        "temperature": 0.1, "top_p": 0.9, "expected_time": "5-15ì´ˆ",
                        "sim_threshold": 0.3}
            if mem >= 8:
                return {"torch_dtype": torch.bfloat16, "max_new_tokens": 4000, "top_k": 10,
                        "embedding_batch_size": 16, "do_sample": True,
                        "temperature": 0.1, "top_p": 0.9, "expected_time": "8-20ì´ˆ",
                        "sim_threshold": 0.3}
            return {"torch_dtype": torch.float16, "max_new_tokens": 3000, "temperature": 0.1, "top_k": 10,
                    "embedding_batch_size": 8, "do_sample": False, "expected_time": "10-25ì´ˆ",
                    "sim_threshold": 0.3}
        return {"torch_dtype": torch.float32, "max_new_tokens": 2000, "temperature": 0.1, "top_k": 10,
                "embedding_batch_size": 4, "do_sample": False, "expected_time": "30-90ì´ˆ",
                "sim_threshold": 0.3}

    def get_adaptive_config(self, complexity_level: float) -> Dict[str, Any]:
        """Test-Time Compute: ë³µì¡ë„ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì„¤ì • ì¡°ì •"""
        base_config = self.config.copy()
        
        if complexity_level > 0.8:  # ë§¤ìš° ë³µì¡í•œ ì§ˆë¬¸
            base_config["max_new_tokens"] = int(base_config["max_new_tokens"] * 1.5)
            base_config["top_k"] = min(base_config["top_k"] + 5, 20)
            base_config["temperature"] = min(base_config.get("temperature", 0.1) + 0.1, 0.3)
        elif complexity_level < 0.3:  # ê°„ë‹¨í•œ ì§ˆë¬¸
            base_config["max_new_tokens"] = int(base_config["max_new_tokens"] * 0.7)
            base_config["top_k"] = max(base_config["top_k"] - 2, 5)
            base_config["do_sample"] = False
            
        return base_config

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Agent í´ë˜ìŠ¤ë“¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ResearchPlannerAgent:
    """ì—°êµ¬ ê³„íš ë° ì „ëµ ìˆ˜ë¦½ ì—ì´ì „íŠ¸"""
    
    def __init__(self, llm_tokenizer, llm_model, device_config):
        self.tokenizer = llm_tokenizer
        self.model = llm_model
        self.device_config = device_config
        
    def analyze_query_complexity(self, query: str) -> float:
        """ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ ë¶„ì„í•˜ì—¬ 0-1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë°˜í™˜"""
        complexity_indicators = [
            "ë¹„êµ", "ë¶„ì„", "í‰ê°€", "ê²€í† ", "ì—°ê´€", "ê´€ê³„", "ì˜í–¥", "ì›ì¸", "ê²°ê³¼",
            "ì–´ë–»ê²Œ", "ì™œ", "ì–¸ì œ", "ì–´ë””ì„œ", "ëˆ„ê°€", "ë¬´ì—‡ì„", "ìƒì„¸", "êµ¬ì²´ì "
        ]
        
        score = 0.3  # ê¸°ë³¸ ë³µì¡ë„
        
        # ì§ˆë¬¸ ê¸¸ì´
        if len(query) > 50:
            score += 0.2
        
        # ë³µì¡ë„ ì§€ì‹œì–´ ê°œìˆ˜
        indicator_count = sum(1 for indicator in complexity_indicators if indicator in query)
        score += min(indicator_count * 0.1, 0.3)
        
        # ì§ˆë¬¸ êµ¬ì¡° ë³µì¡ë„
        if "?" in query:
            score += 0.1
        if any(word in query for word in ["ê·¸ë¦¬ê³ ", "ë˜í•œ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜"]):
            score += 0.1
            
        return min(score, 1.0)
    
    def generate_research_plan(self, query: str, state: ResearchState) -> List[SearchQuery]:
        """ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ë‹¨ê³„ ê²€ìƒ‰ ê³„íš ìƒì„±"""
        
        planning_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì²´ê³„ì ì¸ ì—°êµ¬ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”:

ì§ˆë¬¸: {query}

ë‹¤ìŒ ë‹¨ê³„ë¡œ êµ¬ì„±ëœ ê²€ìƒ‰ ê³„íšì„ ìƒì„±í•˜ì„¸ìš”:
1. í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
2. í•˜ìœ„ ì§ˆë¬¸ ë¶„í•´
3. ìš°ì„ ìˆœìœ„ ì„¤ì •

ê° ê²€ìƒ‰ ì¿¼ë¦¬ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìƒì„±:
- ê²€ìƒ‰ì–´: [êµ¬ì²´ì  ê²€ìƒ‰ì–´]
- ìš°ì„ ìˆœìœ„: [1-10ì ]
- ì¹´í…Œê³ ë¦¬: [ì£¼ìš”ê°œë…/ì„¸ë¶€ì‚¬í•­/ë°°ê²½ì§€ì‹/ë¹„êµë¶„ì„]
- ì´ìœ : [ì™œ ì´ ê²€ìƒ‰ì´ í•„ìš”í•œì§€]

ìµœëŒ€ 5ê°œì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”."""
        
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì—°êµ¬ ê³„íšì„ ìˆ˜ë¦½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": planning_prompt}
        ]
        
        response = self._generate_llm_response(messages, max_tokens=1000)
        return self._parse_search_queries(response, state)  # âœ… ìˆ˜ì •: state ë§¤ê°œë³€ìˆ˜ ì¶”ê°€
    
    def _parse_search_queries(self, response: str, state: ResearchState) -> List[SearchQuery]:  # âœ… ìˆ˜ì •: state ë§¤ê°œë³€ìˆ˜ ì¶”ê°€
        """LLM ì‘ë‹µì—ì„œ ê²€ìƒ‰ ì¿¼ë¦¬ íŒŒì‹±"""
        queries = []
        lines = response.split('\n')
        
        current_query = {}
        for line in lines:
            line = line.strip()
            if line.startswith("- ê²€ìƒ‰ì–´:"):
                current_query["text"] = line.replace("- ê²€ìƒ‰ì–´:", "").strip()
            elif line.startswith("- ìš°ì„ ìˆœìœ„:"):
                try:
                    current_query["priority"] = float(re.findall(r'\d+', line)[0]) / 10.0
                except:
                    current_query["priority"] = 0.5
            elif line.startswith("- ì¹´í…Œê³ ë¦¬:"):
                current_query["category"] = line.replace("- ì¹´í…Œê³ ë¦¬:", "").strip()
            elif line.startswith("- ì´ìœ :"):
                current_query["reason"] = line.replace("- ì´ìœ :", "").strip()
                
                if all(key in current_query for key in ["text", "priority", "category", "reason"]):
                    queries.append(SearchQuery(**current_query))
                    current_query = {}
        
        # ê¸°ë³¸ ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì§ˆë¬¸ ì‚¬ìš©
        if not queries:
            queries.append(SearchQuery(
                text=state.query,  # âœ… ìˆ˜ì •: ì´ì œ stateë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
                priority=1.0,
                category="ì£¼ìš”ê°œë…",
                reason="ê¸°ë³¸ ê²€ìƒ‰"
            ))
            
        return queries[:5]  # ìµœëŒ€ 5ê°œë¡œ ì œí•œ
    
    def identify_knowledge_gaps(self, state: ResearchState) -> List[str]:
        """í˜„ì¬ê¹Œì§€ì˜ ì—°êµ¬ ê²°ê³¼ì—ì„œ ì§€ì‹ ê²©ì°¨ ì‹ë³„"""
        if not state.retrieved_docs:
            return ["ê¸°ì´ˆ ì •ë³´ ë¶€ì¡±"]
            
        gap_analysis_prompt = f"""ë‹¤ìŒ ì—°êµ¬ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ë¶€ì¡±í•œ ì •ë³´ë¥¼ ì‹ë³„í•˜ì„¸ìš”:

ì›ë³¸ ì§ˆë¬¸: {state.query}
í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ì •ë³´:
{chr(10).join([doc.get('text', '')[:200] + '...' for doc in state.retrieved_docs[:3]])}

ë¶€ì¡±í•œ ì •ë³´ë‚˜ ì¶”ê°€ ì¡°ì‚¬ê°€ í•„ìš”í•œ ì˜ì—­ì„ ìµœëŒ€ 3ê°œê¹Œì§€ ë‚˜ì—´í•˜ì„¸ìš”."""
        
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì—°êµ¬ ê²©ì°¨ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": gap_analysis_prompt}
        ]
        
        response = self._generate_llm_response(messages, max_tokens=500)
        gaps = [gap.strip() for gap in response.split('\n') if gap.strip()]
        return gaps[:3]
    
    def _generate_llm_response(self, messages: List[Dict], max_tokens: int = 500) -> str:
        """LLMì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            input_ids = self.tokenizer.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_tensors="pt"
            )
            
            if self.device_config.device == "cuda":
                input_ids = input_ids.to("cuda")
            
            gen_kwargs = {
                "max_new_tokens": max_tokens,
                "do_sample": False,
                "repetition_penalty": 1.1,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id
            }
            
            with torch.no_grad():
                output = self.model.generate(input_ids, **gen_kwargs)
            
            response = self.tokenizer.decode(
                output[0][input_ids.shape[1]:], 
                skip_special_tokens=True
            ).strip()
            
            return response
            
        except Exception as e:
            if 'st' in globals():
                st.warning(f"LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""

class RetrieverAgent:
    """ë¬¸ì„œ ê²€ìƒ‰ ì „ë¬¸ ì—ì´ì „íŠ¸"""
    
    def __init__(self, embed_tokenizer, embed_model, reranker, device_config):
        self.embed_tokenizer = embed_tokenizer
        self.embed_model = embed_model
        self.reranker = reranker
        self.device_config = device_config
        
    def multi_query_retrieval(self, search_queries: List[SearchQuery], 
                            documents: List[Dict], embeddings: np.ndarray) -> List[Dict]:
        """ë‹¤ì¤‘ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•œ í¬ê´„ì  ê²€ìƒ‰"""
        all_results = []
        
        for search_query in search_queries:
            # ê° ê²€ìƒ‰ ì¿¼ë¦¬ì— ëŒ€í•´ ê²€ìƒ‰ ìˆ˜í–‰
            results = self._single_query_retrieval(
                search_query.text, documents, embeddings, 
                top_k=max(5, int(10 * search_query.priority))
            )
            
            # ìš°ì„ ìˆœìœ„ì™€ ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ê°€
            for result in results:
                result['search_priority'] = search_query.priority
                result['search_category'] = search_query.category
                result['search_reason'] = search_query.reason
                
            all_results.extend(results)
        
        # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ ì •ê·œí™”
        unique_results = self._deduplicate_results(all_results)
        return self._rerank_results(search_queries[0].text if search_queries else "", unique_results)
    
    def _single_query_retrieval(self, query: str, documents: List[Dict], 
                              embeddings: np.ndarray, top_k: int = 10) -> List[Dict]:
        """ë‹¨ì¼ ì¿¼ë¦¬ ê²€ìƒ‰"""
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        enc = self.embed_tokenizer([query], padding=True, truncation=True, return_tensors="pt")
        enc = enc.to(self.device_config.device)
        
        with torch.no_grad():
            out = self.embed_model(**enc)
        
        q_emb = mean_pooling(out, enc["attention_mask"]).cpu().numpy()
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        sims = cosine_similarity(q_emb, embeddings)[0]
        idxs = np.argsort(sims)[::-1][:top_k * 3]  # ë” ë§ì€ í›„ë³´ ì„ íƒ
        threshold = self.device_config.config["sim_threshold"]
        
        results = []
        for idx in idxs:
            if sims[idx] >= threshold:
                doc = documents[idx].copy()
                doc["similarity"] = float(sims[idx])
                results.append(doc)
        
        return results[:top_k]
    
    def _deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """ê²°ê³¼ ì¤‘ë³µ ì œê±°"""
        seen_texts = set()
        unique_results = []
        
        for result in results:
            text_hash = hashlib.md5(result['text'].encode()).hexdigest()
            if text_hash not in seen_texts:
                seen_texts.add(text_hash)
                unique_results.append(result)
        
        return unique_results
    
    def _rerank_results(self, main_query: str, results: List[Dict], top_k: int = 15) -> List[Dict]:
        """Cross-encoderë¥¼ ì‚¬ìš©í•œ ì¬ìˆœìœ„ ë§¤ê¹€"""
        if not results:
            return []
        
        texts = [r["text"] for r in results]
        scores = self.reranker.predict([(main_query, text) for text in texts])
        
        for result, score in zip(results, scores):
            result["rerank_score"] = float(score)
            # ìµœì¢… ì ìˆ˜ëŠ” ì„ë² ë”© ìœ ì‚¬ë„ì™€ ì¬ìˆœìœ„ ì ìˆ˜ì˜ ê°€ì¤‘ í‰ê· 
            result["final_score"] = (result["similarity"] * 0.4 + score * 0.6)
        
        results.sort(key=lambda x: x["final_score"], reverse=True)
        return results[:top_k]

class AnalyzerAgent:
    """ë¬¸ì„œ ë¶„ì„ ë° ê²€ì¦ ì—ì´ì „íŠ¸"""
    
    def __init__(self, llm_tokenizer, llm_model, device_config):
        self.tokenizer = llm_tokenizer
        self.model = llm_model
        self.device_config = device_config
    
    def analyze_document_relevance(self, query: str, documents: List[Dict]) -> List[Dict]:
        """ë¬¸ì„œ ê´€ë ¨ì„± ì‹¬í™” ë¶„ì„"""
        analyzed_docs = []
        
        for doc in documents:
            analysis = self._deep_analyze_single_doc(query, doc)
            doc.update(analysis)
            analyzed_docs.append(doc)
        
        return analyzed_docs
    
    def _deep_analyze_single_doc(self, query: str, doc: Dict) -> Dict[str, Any]:
        """ê°œë³„ ë¬¸ì„œ ì‹¬í™” ë¶„ì„"""
        analysis_prompt = f"""ë‹¤ìŒ ë¬¸ì„œê°€ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ê´€ë ¨ìˆëŠ”ì§€ ë¶„ì„í•˜ì„¸ìš”:

ì§ˆë¬¸: {query}

ë¬¸ì„œ ë‚´ìš©:
{doc['text'][:500]}...

ë‹¤ìŒ í•­ëª©ì„ í‰ê°€í•˜ì„¸ìš”:
1. ì§ì ‘ì  ê´€ë ¨ì„± (1-10ì )
2. í•µì‹¬ ì •ë³´ í¬í•¨ ì—¬ë¶€
3. ì‹ ë¢°ì„± ìˆ˜ì¤€
4. ì£¼ìš” ì¸ì‚¬ì´íŠ¸ (í•œ ì¤„ë¡œ)

ê°„ë‹¨íˆ ë‹µë³€í•˜ì„¸ìš”."""
        
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë¬¸ì„œ ê´€ë ¨ì„±ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": analysis_prompt}
        ]
        
        response = self._generate_llm_response(messages, max_tokens=200)
        
        # ì‘ë‹µ íŒŒì‹±
        relevance_score = self._extract_score(response)
        key_insight = self._extract_insight(response)
        
        return {
            "analysis_score": relevance_score,
            "key_insight": key_insight,
            "analyzed": True
        }
    
    def cross_validate_information(self, documents: List[Dict]) -> Dict[str, Any]:
        """ì—¬ëŸ¬ ë¬¸ì„œ ê°„ ì •ë³´ êµì°¨ ê²€ì¦"""
        if len(documents) < 2:
            return {"consistency": 1.0, "conflicts": [], "consensus": []}
        
        validation_prompt = f"""ë‹¤ìŒ ë¬¸ì„œë“¤ì˜ ì •ë³´ë¥¼ êµì°¨ ê²€ì¦í•˜ì„¸ìš”:

{chr(10).join([f"ë¬¸ì„œ {i+1}: {doc['text'][:200]}..." for i, doc in enumerate(documents[:3])])}

1. ì¼ê´€ëœ ì •ë³´
2. ìƒì¶©ë˜ëŠ” ì •ë³´
3. ì‹ ë¢°ë„ í‰ê°€

ê°„ë‹¨íˆ ì •ë¦¬í•˜ì„¸ìš”."""
        
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì •ë³´ ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": validation_prompt}
        ]
        
        response = self._generate_llm_response(messages, max_tokens=300)
        
        return {
            "consistency": self._calculate_consistency(documents),
            "conflicts": self._identify_conflicts(response),
            "consensus": self._identify_consensus(response)
        }
    
    def _generate_llm_response(self, messages: List[Dict], max_tokens: int = 300) -> str:
        """LLM ì‘ë‹µ ìƒì„±"""
        try:
            input_ids = self.tokenizer.apply_chat_template(
                messages, tokenize=True, add_generation_prompt=True, return_tensors="pt"
            )
            
            if self.device_config.device == "cuda":
                input_ids = input_ids.to("cuda")
            
            gen_kwargs = {
                "max_new_tokens": max_tokens,
                "do_sample": False,
                "repetition_penalty": 1.1,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id
            }
            
            with torch.no_grad():
                output = self.model.generate(input_ids, **gen_kwargs)
            
            return self.tokenizer.decode(output[0][input_ids.shape[1]:], skip_special_tokens=True).strip()
        except:
            return ""
    
    def _extract_score(self, text: str) -> float:
        """í…ìŠ¤íŠ¸ì—ì„œ ì ìˆ˜ ì¶”ì¶œ"""
        scores = re.findall(r'(\d+)ì ', text)
        if scores:
            return float(scores[0]) / 10.0
        return 0.5
    
    def _extract_insight(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        lines = text.split('\n')
        for line in lines:
            if 'ì¸ì‚¬ì´íŠ¸' in line or 'í•µì‹¬' in line:
                return line.strip()
        return "ì¶”ê°€ ë¶„ì„ í•„ìš”"
    
    def _calculate_consistency(self, documents: List[Dict]) -> float:
        """ë¬¸ì„œ ê°„ ì¼ê´€ì„± ê³„ì‚°"""
        if len(documents) < 2:
            return 1.0
        
        # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì¼ê´€ì„± ì¸¡ì •
        texts = [doc['text'] for doc in documents]
        total_similarity = 0
        pairs = 0
        
        for i in range(len(texts)):
            for j in range(i+1, len(texts)):
                words1 = set(texts[i].lower().split())
                words2 = set(texts[j].lower().split())
                similarity = len(words1 & words2) / len(words1 | words2) if words1 | words2 else 0
                total_similarity += similarity
                pairs += 1
        
        return total_similarity / pairs if pairs > 0 else 0.5
    
    def _identify_conflicts(self, text: str) -> List[str]:
        """ìƒì¶© ì •ë³´ ì‹ë³„"""
        conflicts = []
        if 'ìƒì¶©' in text or 'ëª¨ìˆœ' in text or 'ë‹¤ë¥´' in text:
            lines = text.split('\n')
            for line in lines:
                if any(word in line for word in ['ìƒì¶©', 'ëª¨ìˆœ', 'ë‹¤ë¥´']):
                    conflicts.append(line.strip())
        return conflicts
    
    def _identify_consensus(self, text: str) -> List[str]:
        """ì¼ê´€ëœ ì •ë³´ ì‹ë³„"""
        consensus = []
        if 'ì¼ê´€' in text or 'ê³µí†µ' in text or 'ê°™' in text:
            lines = text.split('\n')
            for line in lines:
                if any(word in line for word in ['ì¼ê´€', 'ê³µí†µ', 'ê°™']):
                    consensus.append(line.strip())
        return consensus

class SynthesizerAgent:
    """ì •ë³´ í†µí•© ë° ìµœì¢… ë‹µë³€ ìƒì„± ì—ì´ì „íŠ¸"""
    
    def __init__(self, llm_tokenizer, llm_model, device_config):
        self.tokenizer = llm_tokenizer
        self.model = llm_model
        self.device_config = device_config
    
    def synthesize_comprehensive_answer(self, query: str, state: ResearchState, 
                                      analysis_results: Dict) -> str:
        """í¬ê´„ì  ë‹µë³€ ìƒì„±"""
        
        # Test-Time Compute: ë³µì¡ë„ì— ë”°ë¥¸ ë™ì  ì„¤ì •
        complexity = len(state.retrieved_docs) * 0.1 + len(state.insights) * 0.2
        adaptive_config = self.device_config.get_adaptive_config(complexity)
        
        synthesis_prompt = self._build_synthesis_prompt(query, state, analysis_results)
        
        messages = [
            {"role": "system", "content": self._get_synthesis_system_prompt()},
            {"role": "user", "content": synthesis_prompt}
        ]
        
        return self._generate_final_answer(messages, adaptive_config)
    
    def _build_synthesis_prompt(self, query: str, state: ResearchState, 
                               analysis_results: Dict) -> str:
        """ì¢…í•© ë‹µë³€ìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        # ë¬¸ì„œ ì •ë³´ ì •ë¦¬
        doc_summaries = []
        for i, doc in enumerate(state.retrieved_docs[:8], 1):  # ìµœëŒ€ 8ê°œ ë¬¸ì„œ
            summary = f"[ë¬¸ì„œ {i}] ì¶œì²˜: {doc['source']}, ê´€ë ¨ë„: {doc.get('final_score', 0):.2f}\n"
            summary += f"ë‚´ìš©: {doc['text'][:300]}...\n"
            if 'key_insight' in doc:
                summary += f"í•µì‹¬ ì¸ì‚¬ì´íŠ¸: {doc['key_insight']}\n"
            doc_summaries.append(summary)
        
        # ë¶„ì„ ê²°ê³¼ ì •ë¦¬
        analysis_summary = f"""
êµì°¨ ê²€ì¦ ê²°ê³¼:
- ì •ë³´ ì¼ê´€ì„±: {analysis_results.get('consistency', 0.5):.2f}
- ìƒì¶© ì •ë³´: {len(analysis_results.get('conflicts', []))}ê±´
- ê³µí†µ ì •ë³´: {len(analysis_results.get('consensus', []))}ê±´

ì—°êµ¬ ì§„í–‰ í˜„í™©:
- íƒìƒ‰ ì‚¬ì´í´: {state.cycle_count + 1}/{state.max_cycles}
- ë°œê²¬ëœ ì¸ì‚¬ì´íŠ¸: {len(state.insights)}ê°œ
- ì‹ë³„ëœ ì§€ì‹ ê²©ì°¨: {len(state.gaps)}ê°œ
"""
        
        return f"""ë‹¤ìŒ ì—°êµ¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ì¢…í•©ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”:

ì›ë³¸ ì§ˆë¬¸: {query}

=== ìˆ˜ì§‘ëœ ë¬¸ì„œ ì •ë³´ ===
{chr(10).join(doc_summaries)}

=== ë¶„ì„ ê²°ê³¼ ===
{analysis_summary}

=== ìš”êµ¬ì‚¬í•­ ===
1. ë¬¸ì„œì—ì„œ ì°¾ì€ ì •ë³´ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€
2. ê° ì£¼ì¥ì— ëŒ€í•´ ì¶œì²˜ ëª…ì‹œ (íŒŒì¼ëª…, ê´€ë ¨ë„ ì ìˆ˜)
3. ìƒì¶©ë˜ëŠ” ì •ë³´ê°€ ìˆë‹¤ë©´ ëª…ì‹œ
4. ë¶€ì¡±í•œ ì •ë³´ê°€ ìˆë‹¤ë©´ ì–¸ê¸‰
5. ì‹ ë¢°ë„ ìˆ˜ì¤€ ì œì‹œ
6. êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ ì‘ì„±"""
    
    def _get_synthesis_system_prompt(self) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""
        return """ë‹¹ì‹ ì€ ë‹¤ì¤‘ ì†ŒìŠ¤ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì •í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì—°êµ¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

í•µì‹¬ ì›ì¹™:
1. ì œê³µëœ ë¬¸ì„œ ì •ë³´ë§Œ ì‚¬ìš©
2. ëª¨ë“  ì£¼ì¥ì— ëŒ€í•œ ì¶œì²˜ ëª…ì‹œ
3. ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì‹ ë¢°ë„ì™€ í•¨ê»˜ ì œì‹œ
4. ìƒì¶©ë˜ëŠ” ì •ë³´ëŠ” ê°ê´€ì ìœ¼ë¡œ ì œì‹œ
5. ì§€ì‹ ê²©ì°¨ëŠ” ì†”ì§í•˜ê²Œ ì¸ì •
6. ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ êµ¬ì¡°ë¡œ ë‹µë³€

ë‹µë³€ êµ¬ì¡°:
- í•µì‹¬ ë‹µë³€ (ìš”ì•½)
- ìƒì„¸ ì„¤ëª… (ê·¼ê±°ì™€ í•¨ê»˜)
- ì¶”ê°€ ê³ ë ¤ì‚¬í•­ (í•œê³„ì  í¬í•¨)
- ì‹ ë¢°ë„ í‰ê°€"""
    
    def _generate_final_answer(self, messages: List[Dict], config: Dict) -> str:
        """ìµœì¢… ë‹µë³€ ìƒì„±"""
        try:
            input_ids = self.tokenizer.apply_chat_template(
                messages, tokenize=True, add_generation_prompt=True, return_tensors="pt"
            )
            
            if self.device_config.device == "cuda":
                input_ids = input_ids.to("cuda")
            
            gen_kwargs = {
                "max_new_tokens": config["max_new_tokens"],
                "do_sample": config.get("do_sample", False),
                "repetition_penalty": 1.1,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id
            }
            
            if config.get("do_sample"):
                gen_kwargs.update({
                    "temperature": config.get("temperature", 0.1),
                    "top_p": config.get("top_p", 0.9)
                })
            
            with torch.no_grad():
                output = self.model.generate(input_ids, **gen_kwargs)
            
            return self.tokenizer.decode(
                output[0][input_ids.shape[1]:], skip_special_tokens=True
            ).strip()
        except Exception as e:
            if 'st' in globals():
                st.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

class ValidatorAgent:
    """ìµœì¢… ê²€ì¦ ì—ì´ì „íŠ¸"""
    
    def __init__(self, llm_tokenizer, llm_model, device_config):
        self.tokenizer = llm_tokenizer
        self.model = llm_model
        self.device_config = device_config
        self.forbidden_phrases = {
            "ì•Œë ¤ì§€ì§€ ì•Šì€", "í™•ì‹¤í•˜ì§€ ì•Šì€", "ì•„ë§ˆë„", "ì¶”ì¸¡í•˜ê±´ëŒ€", 
            "ì¼ë°˜ì ìœ¼ë¡œ", "ë³´í†µ", "ëŒ€ë¶€ë¶„ì˜ ê²½ìš°"
        }
    
    def comprehensive_validation(self, query: str, answer: str, 
                               state: ResearchState) -> Dict[str, Any]:
        """í¬ê´„ì  ë‹µë³€ ê²€ì¦"""
        
        validations = {
            "source_grounding": self._validate_source_grounding(answer, state.retrieved_docs),
            "factual_consistency": self._validate_factual_consistency(answer, state.retrieved_docs),
            "completeness": self._validate_completeness(query, answer, state),
            "confidence_assessment": self._assess_confidence(answer, state),
            "forbidden_phrases": self._check_forbidden_phrases(answer)
        }
        
        # ì¢…í•© ì‹ ë¢°ë„ ê³„ì‚°
        overall_confidence = self._calculate_overall_confidence(validations)
        warnings = self._generate_warnings(validations)
        
        return {
            "confidence": overall_confidence,
            "warnings": warnings,
            "detailed_validation": validations
        }
    
    def _validate_source_grounding(self, answer: str, documents: List[Dict]) -> Dict[str, Any]:
        """ì†ŒìŠ¤ ê¸°ë°˜ ê²€ì¦"""
        source_files = set(doc['source'] for doc in documents)
        mentioned_sources = set()
        
        for source in source_files:
            if source in answer:
                mentioned_sources.add(source)
        
        grounding_score = len(mentioned_sources) / len(source_files) if source_files else 0
        
        return {
            "score": grounding_score,
            "total_sources": len(source_files),
            "mentioned_sources": len(mentioned_sources),
            "missing_sources": source_files - mentioned_sources
        }
    
    def _validate_factual_consistency(self, answer: str, documents: List[Dict]) -> Dict[str, Any]:
        """ì‚¬ì‹¤ ì¼ê´€ì„± ê²€ì¦"""
        if not documents:
            return {"score": 0.0, "issues": ["ì°¸ì¡° ë¬¸ì„œ ì—†ìŒ"]}
        
        context_text = " ".join([doc['text'] for doc in documents])
        answer_words = set(answer.lower().split())
        context_words = set(context_text.lower().split())
        
        overlap_ratio = len(answer_words & context_words) / max(len(answer_words), 1)
        
        issues = []
        if overlap_ratio < 0.3:
            issues.append("ë¬¸ì„œ ë‚´ìš©ê³¼ ì—°ê´€ì„± ë‚®ìŒ")
        if overlap_ratio < 0.1:
            issues.append("ë¬¸ì„œ ì™¸ë¶€ ì •ë³´ ì‚¬ìš© ì˜ì‹¬")
            
        return {
            "score": overlap_ratio,
            "overlap_ratio": overlap_ratio,
            "issues": issues
        }
    
    def _validate_completeness(self, query: str, answer: str, state: ResearchState) -> Dict[str, Any]:
        """ë‹µë³€ ì™„ì„±ë„ ê²€ì¦"""
        completeness_prompt = f"""ë‹¤ìŒ ë‹µë³€ì´ ì§ˆë¬¸ì„ ì–¼ë§ˆë‚˜ ì™„ì „íˆ ë‹µí–ˆëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”:

ì§ˆë¬¸: {query}
ë‹µë³€: {answer[:1000]}...

í‰ê°€ í•­ëª©:
1. ì§ˆë¬¸ì˜ ëª¨ë“  ì¸¡ë©´ì„ ë‹¤ë¤˜ëŠ”ê°€?
2. ì¶©ë¶„í•œ ì„¸ë¶€ì‚¬í•­ì„ ì œê³µí–ˆëŠ”ê°€?
3. ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?

ì ìˆ˜ (1-10): """
        
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë‹µë³€ ì™„ì„±ë„ë¥¼ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": completeness_prompt}
        ]
        
        response = self._generate_llm_response(messages, max_tokens=100)
        score = self._extract_score_from_response(response)
        
        return {
            "score": score,
            "evaluation": response
        }
    
    def _assess_confidence(self, answer: str, state: ResearchState) -> Dict[str, float]:  # âœ… ìˆ˜ì •: Dict[str, float] ë°˜í™˜
        """ì‹ ë¢°ë„ í‰ê°€"""
        base_confidence = 0.5
        
        # ë¬¸ì„œ í’ˆì§ˆì— ë”°ë¥¸ ì‹ ë¢°ë„
        if state.retrieved_docs:
            doc_scores = [doc.get('final_score', 0.5) for doc in state.retrieved_docs]
            avg_doc_score = np.mean(doc_scores)
            base_confidence += avg_doc_score * 0.3
        
        # ì—°êµ¬ ê¹Šì´ì— ë”°ë¥¸ ì‹ ë¢°ë„
        research_depth = min(state.cycle_count / state.max_cycles, 1.0)
        base_confidence += research_depth * 0.2
        
        # ì¸ì‚¬ì´íŠ¸ ìˆ˜ì— ë”°ë¥¸ ì‹ ë¢°ë„
        insight_bonus = min(len(state.insights) * 0.05, 0.2)
        base_confidence += insight_bonus
        
        return {"score": min(base_confidence, 1.0)}  # âœ… ìˆ˜ì •: dictë¡œ ë°˜í™˜
    
    def _check_forbidden_phrases(self, answer: str) -> List[str]:
        """ê¸ˆì§€ í‘œí˜„ ê²€ì‚¬"""
        found_phrases = []
        for phrase in self.forbidden_phrases:
            if phrase in answer:
                found_phrases.append(phrase)
        return found_phrases
    
    def _calculate_overall_confidence(self, validations: Dict) -> float:
        """ì¢…í•© ì‹ ë¢°ë„ ê³„ì‚°"""
        weights = {
            "source_grounding": 0.3,
            "factual_consistency": 0.3,
            "completeness": 0.2,
            "confidence_assessment": 0.2
        }
        
        total_score = 0
        for key, weight in weights.items():
            if key in validations:
                value = validations[key]
                # âœ… ìˆ˜ì •: íƒ€ì… ì²´í¬ ì¶”ê°€
                if isinstance(value, dict):
                    score = value.get('score', 0.5)
                elif isinstance(value, (int, float)):
                    score = float(value)
                else:
                    score = 0.5
                total_score += score * weight
        
        # ê¸ˆì§€ í‘œí˜„ í˜ë„í‹°
        if validations.get("forbidden_phrases"):
            total_score *= 0.8
            
        return min(total_score, 1.0)
    
    def _generate_warnings(self, validations: Dict) -> List[str]:
        """ê²½ê³  ë©”ì‹œì§€ ìƒì„±"""
        warnings = []
        
        if validations["source_grounding"]["score"] < 0.5:
            warnings.append("ë¬¸ì„œ ì¶œì²˜ ëª…ì‹œ ë¶€ì¡±")
            
        if validations["factual_consistency"]["score"] < 0.3:
            warnings.append("ë¬¸ì„œ ë‚´ìš©ê³¼ ì—°ê´€ì„± ë‚®ìŒ")
            
        if validations["completeness"]["score"] < 0.6:
            warnings.append("ë‹µë³€ ì™„ì„±ë„ ë¶€ì¡±")
            
        if validations["forbidden_phrases"]:
            warnings.append(f"ë¶ˆí™•ì‹¤ í‘œí˜„ ì‚¬ìš©: {', '.join(validations['forbidden_phrases'])}")
            
        return warnings
    
    def _generate_llm_response(self, messages: List[Dict], max_tokens: int = 200) -> str:
        """LLM ì‘ë‹µ ìƒì„±"""
        try:
            input_ids = self.tokenizer.apply_chat_template(
                messages, tokenize=True, add_generation_prompt=True, return_tensors="pt"
            )
            
            if self.device_config.device == "cuda":
                input_ids = input_ids.to("cuda")
            
            gen_kwargs = {
                "max_new_tokens": max_tokens,
                "do_sample": False,
                "repetition_penalty": 1.1,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id
            }
            
            with torch.no_grad():
                output = self.model.generate(input_ids, **gen_kwargs)
            
            return self.tokenizer.decode(output[0][input_ids.shape[1]:], skip_special_tokens=True).strip()
        except:
            return ""
    
    def _extract_score_from_response(self, response: str) -> float:
        """ì‘ë‹µì—ì„œ ì ìˆ˜ ì¶”ì¶œ"""
        scores = re.findall(r'(\d+)', response)
        if scores:
            return float(scores[0]) / 10.0
        return 0.5

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ë©”ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í´ë˜ìŠ¤ (Deep Research ìŠ¤íƒ€ì¼)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class DeepResearchOrchestrator:
    """Deep Research ìŠ¤íƒ€ì¼ì˜ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í˜‘ì—… ì‹œìŠ¤í…œ"""
    
    def __init__(
        self,
        model_name: str = "LGAI-EXAONE/EXAONE-4.0-1.2B",
        embed_model: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        reranker_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2",
        min_chunk_length: int = 50,
        max_chunk_length: int = 300,
        sentences_per_chunk: int = 2,
    ):
        st.info("ğŸš€ Deep Research ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # ê¸°ë³¸ ì„¤ì •
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device_config = DeviceConfig()
        
        # ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ë“¤
        self.pdf_extractor = ImprovedPDFExtractor()
        self.chunker = ImprovedKoreanSentenceChunker(
            min_chunk_length=min_chunk_length,
            max_chunk_length=max_chunk_length,
            sentences_per_chunk=sentences_per_chunk
        )
        
        # ëª¨ë¸ ë¡œë”©
        self._load_models()
        
        # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        self._initialize_agents()
        
        # ë°ì´í„° ì €ì¥ì†Œ
        self.documents: List[Dict[str, Any]] = []
        self.embeddings: Optional[np.ndarray] = None
        self.chunk_hashes: Dict[str, Dict] = {}
        self.loaded_pdfs: List[str] = []
        
        st.success("âœ… Deep Research ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    
    def _load_models(self):
        """ëª¨ë¸ë“¤ ë¡œë”©"""
        # ì„ë² ë”© ëª¨ë¸
        st.info("â–¶ ì„ë² ë”© ëª¨ë¸ ë¡œë”©...")
        self.embed_tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        self.embed_model = AutoModel.from_pretrained("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2").to(self.device_config.device)
        
        # Cross-Encoder
        self.reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2", device=self.device_config.device)
        
        # LLM
        st.info("â–¶ LLM ë¡œë”©...")
        self.tokenizer = AutoTokenizer.from_pretrained("LGAI-EXAONE/EXAONE-4.0-1.2B", trust_remote_code=True)
        map_dev = "auto" if self.device_config.device == "cuda" else "cpu"
        self.model = AutoModelForCausalLM.from_pretrained(
            "LGAI-EXAONE/EXAONE-4.0-1.2B", torch_dtype=self.device_config.config["torch_dtype"],
            device_map=map_dev, max_memory={0: "14GB"}, trust_remote_code=True
        ).to(self.device)
        
        torch.cuda.empty_cache()
        gc.collect()
    
    def _initialize_agents(self):
        """ì—ì´ì „íŠ¸ë“¤ ì´ˆê¸°í™”"""
        st.info("â–¶ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
        
        self.planner = ResearchPlannerAgent(self.tokenizer, self.model, self.device_config)
        self.retriever = RetrieverAgent(self.embed_tokenizer, self.embed_model, self.reranker, self.device_config)
        self.analyzer = AnalyzerAgent(self.tokenizer, self.model, self.device_config)
        self.synthesizer = SynthesizerAgent(self.tokenizer, self.model, self.device_config)
        self.validator = ValidatorAgent(self.tokenizer, self.model, self.device_config)
    
    def load_pdf_documents(self, pdf_paths: List[str]) -> None:
        """PDF ë¬¸ì„œ ë¡œë”©"""
        st.info(f"ğŸ“š PDF ë¬¸ì„œ ì²˜ë¦¬ ì¤‘... ({len(pdf_paths)}ê°œ)")
        
        self.documents, self.embeddings, self.chunk_hashes, self.loaded_pdfs = [], None, {}, []
        all_docs: List[Dict[str, Any]] = []
        
        progress_bar = st.progress(0)
        
        for i, pdf_path in enumerate(pdf_paths):
            if not os.path.exists(pdf_path):
                st.warning(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {pdf_path}")
                continue
            
            # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
            full_text = self.pdf_extractor.extract_text_from_pdf(pdf_path)
            if not full_text.strip():
                st.warning(f"âš ï¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ: {pdf_path}")
                continue
            
            # í…ìŠ¤íŠ¸ ì²­í‚¹
            chunks = self.chunker.chunk_text(full_text)
            
            # ë¬¸ì„œ ê°ì²´ ìƒì„±
            for chunk_idx, chunk in enumerate(chunks, 1):
                if chunk.strip():
                    all_docs.append({
                        "text": chunk.strip(),
                        "page": 1,
                        "paragraph": chunk_idx,
                        "source": os.path.basename(pdf_path),
                        "full_path": pdf_path
                    })
            
            self.loaded_pdfs.append(pdf_path)
            progress_bar.progress((i + 1) / len(pdf_paths))
        
        if not all_docs:
            st.error("âŒ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        self.documents = all_docs
        
        # ì„ë² ë”© ìƒì„±
        st.info("ğŸ§® ì„ë² ë”© ìƒì„± ì¤‘...")
        self._generate_embeddings()
        
        # ì²­í¬ í•´ì‹œ ìƒì„±
        for d in self.documents:
            cid = self._hash(d["text"], d["page"], d["paragraph"], d["source"])
            d["chunk_id"] = cid
            self.chunk_hashes[cid] = d
        
        st.success(f"ğŸ‰ ì²˜ë¦¬ ì™„ë£Œ! ì´ {len(self.documents)}ê°œ ì²­í¬ ìƒì„±")
    
    def _generate_embeddings(self):
        """ì„ë² ë”© ìƒì„±"""
        texts = [d["text"] for d in self.documents]
        bs = self.device_config.config["embedding_batch_size"]
        embs = []
        
        progress_bar = st.progress(0)
        
        for i in range(0, len(texts), bs):
            batch = texts[i : i + bs]
            enc = self.embed_tokenizer(batch, padding=True, truncation=True, return_tensors="pt").to(self.device_config.device)
            with torch.no_grad():
                out = self.embed_model(**enc)
            pooled = mean_pooling(out, enc["attention_mask"])
            embs.append(pooled.cpu().numpy())
            
            progress_bar.progress(min(1.0, (i + bs) / len(texts)))
        
        self.embeddings = np.vstack(embs)
    
    @staticmethod
    def _hash(text: str, page: int, para: int, source: str) -> str:
        return hashlib.md5(f"{source}_{page}_{para}_{text}".encode()).hexdigest()[:8]
    
    def deep_research(self, query: str) -> Dict[str, Any]:
        """Deep Research ë©”ì¸ í”„ë¡œì„¸ìŠ¤"""
        
        st.info("ğŸ” Deep Research í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
        
        # 1. ì—°êµ¬ ìƒíƒœ ì´ˆê¸°í™”
        complexity = self.planner.analyze_query_complexity(query)
        max_cycles = 2 if complexity < 0.5 else 3  # ë³µì¡ë„ì— ë”°ë¥¸ ì‚¬ì´í´ ìˆ˜ ì¡°ì •
        
        state = ResearchState(
            phase=ResearchPhase.PLANNING,
            query=query,
            sub_queries=[],
            retrieved_docs=[],
            confidence_history=[],
            insights=[],
            gaps=[],
            max_cycles=max_cycles
        )
        
        research_log = []  # ì—°êµ¬ ê³¼ì • ë¡œê·¸
        search_queries = []  # âš ï¸ ìˆ˜ì •ì‚¬í•­ 2: search_queries ë³€ìˆ˜ ë¯¸ë¦¬ ì •ì˜
        
        # ì—°êµ¬ ì‚¬ì´í´ ì‹œì‘
        for cycle in range(state.max_cycles):
            state.cycle_count = cycle
            research_log.append(f"=== ì‚¬ì´í´ {cycle + 1} ===")
            
            # Phase 1: ì—°êµ¬ ê³„íš ìˆ˜ë¦½
            if cycle == 0:
                st.info(f"ğŸ“‹ ì—°êµ¬ ê³„íš ìˆ˜ë¦½ ì¤‘... (ë³µì¡ë„: {complexity:.2f})")
                state.phase = ResearchPhase.PLANNING
                search_queries = self.planner.generate_research_plan(query, state)
                state.sub_queries = [sq.text for sq in search_queries]
                research_log.append(f"ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬: {len(search_queries)}ê°œ")
            else:
                # í›„ì† ì‚¬ì´í´ì—ì„œëŠ” ì§€ì‹ ê²©ì°¨ ê¸°ë°˜ìœ¼ë¡œ ì¶”ê°€ ì¿¼ë¦¬ ìƒì„±
                gaps = self.planner.identify_knowledge_gaps(state)
                state.gaps = gaps
                if gaps:
                    additional_queries = [SearchQuery(
                        text=gap, priority=0.8, category="ê²©ì°¨ë³´ì™„", reason="ì§€ì‹ê²©ì°¨í•´ê²°"
                    ) for gap in gaps[:2]]
                    search_queries.extend(additional_queries)
                    research_log.append(f"ì§€ì‹ ê²©ì°¨ ë³´ì™„ ì¿¼ë¦¬: {len(additional_queries)}ê°œ")
            
            # Phase 2: ë¬¸ì„œ ê²€ìƒ‰
            st.info(f"ğŸ” ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘... (ì¿¼ë¦¬ {len(search_queries)}ê°œ)")
            state.phase = ResearchPhase.INITIAL_RETRIEVAL if cycle == 0 else ResearchPhase.DEEP_ANALYSIS
            
            retrieved_docs = self.retriever.multi_query_retrieval(
                search_queries, self.documents, self.embeddings
            )
            
            # ìƒˆë¡œìš´ ë¬¸ì„œë§Œ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
            existing_ids = {doc.get('chunk_id', '') for doc in state.retrieved_docs}
            new_docs = [doc for doc in retrieved_docs if doc.get('chunk_id', '') not in existing_ids]
            state.retrieved_docs.extend(new_docs)
            
            research_log.append(f"ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(retrieved_docs)}ê°œ (ì‹ ê·œ: {len(new_docs)}ê°œ)")
            
            # Phase 3: ë¬¸ì„œ ë¶„ì„
            if state.retrieved_docs:
                st.info("ğŸ”¬ ë¬¸ì„œ ì‹¬í™” ë¶„ì„ ì¤‘...")
                state.phase = ResearchPhase.CROSS_VALIDATION
                
                analyzed_docs = self.analyzer.analyze_document_relevance(query, state.retrieved_docs[-10:])
                
                # ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
                new_insights = [doc.get('key_insight', '') for doc in analyzed_docs if doc.get('key_insight')]
                state.insights.extend([insight for insight in new_insights if insight not in state.insights])
                
                research_log.append(f"ë¶„ì„ ì™„ë£Œ: {len(analyzed_docs)}ê°œ ë¬¸ì„œ, ì¸ì‚¬ì´íŠ¸: {len(new_insights)}ê°œ")
            
            # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ í™•ì¸
            current_confidence = self._calculate_interim_confidence(state)
            state.confidence_history.append(current_confidence)
            
            if current_confidence > 0.85 and len(state.retrieved_docs) >= 5:
                research_log.append(f"ë†’ì€ ì‹ ë¢°ë„ ë‹¬ì„± ({current_confidence:.2f}) - ì¡°ê¸° ì¢…ë£Œ")
                break
            
            if cycle < state.max_cycles - 1:
                time.sleep(0.5)  # UI ì‘ë‹µì„±ì„ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
        
        # Phase 4: êµì°¨ ê²€ì¦
        st.info("âœ… ì •ë³´ êµì°¨ ê²€ì¦ ì¤‘...")
        state.phase = ResearchPhase.CROSS_VALIDATION
        analysis_results = self.analyzer.cross_validate_information(state.retrieved_docs[:10])
        research_log.append(f"êµì°¨ ê²€ì¦ ì™„ë£Œ - ì¼ê´€ì„±: {analysis_results['consistency']:.2f}")
        
        # Phase 5: ì¢…í•© ë‹µë³€ ìƒì„±
        st.info("ğŸ“ ì¢…í•© ë‹µë³€ ìƒì„± ì¤‘...")
        state.phase = ResearchPhase.SYNTHESIS
        comprehensive_answer = self.synthesizer.synthesize_comprehensive_answer(
            query, state, analysis_results
        )
        research_log.append("ì¢…í•© ë‹µë³€ ìƒì„± ì™„ë£Œ")
        
        # Phase 6: ìµœì¢… ê²€ì¦
        st.info("ğŸ” ìµœì¢… ë‹µë³€ ê²€ì¦ ì¤‘...")
        state.phase = ResearchPhase.FINAL_VALIDATION
        validation_results = self.validator.comprehensive_validation(
            query, comprehensive_answer, state
        )
        research_log.append(f"ìµœì¢… ê²€ì¦ ì™„ë£Œ - ì‹ ë¢°ë„: {validation_results['confidence']:.2f}")
        
        # ê²°ê³¼ ì •ë¦¬
        sources = []
        for doc in state.retrieved_docs[:15]:  # ìƒìœ„ 15ê°œ ë¬¸ì„œë§Œ
            source_info = {
                "page": doc["page"],
                "paragraph": doc["paragraph"],
                "chunk_id": doc["chunk_id"],
                "source_file": doc["source"],
                "preview": doc["text"][:400] + "..." if len(doc["text"]) > 400 else doc["text"],
                "similarity": float(doc.get("final_score", doc.get("similarity", 0))),
                "chunk_size": len(doc["text"]),
                "search_category": doc.get("search_category", "ì¼ë°˜"),
                "key_insight": doc.get("key_insight", "")
            }
            sources.append(source_info)
        
        return {
            "answer": comprehensive_answer,
            "confidence": validation_results["confidence"],
            "warnings": validation_results["warnings"],
            "sources": sources,
            "research_metadata": {
                "cycles_completed": state.cycle_count + 1,
                "total_documents_analyzed": len(state.retrieved_docs),
                "insights_discovered": len(state.insights),
                "knowledge_gaps_identified": len(state.gaps),
                "query_complexity": complexity,
                "confidence_progression": state.confidence_history,
                "research_log": research_log,
                "cross_validation": analysis_results
            }
        }
    
    def _calculate_interim_confidence(self, state: ResearchState) -> float:
        """ì¤‘ê°„ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not state.retrieved_docs:
            return 0.0
        
        # ë¬¸ì„œ ì ìˆ˜ ê¸°ë°˜ ì‹ ë¢°ë„
        doc_scores = [doc.get('final_score', doc.get('similarity', 0.5)) for doc in state.retrieved_docs]
        avg_doc_score = np.mean(doc_scores) if doc_scores else 0.5
        
        # ë¬¸ì„œ ë‹¤ì–‘ì„± ì ìˆ˜
        unique_sources = len(set(doc['source'] for doc in state.retrieved_docs))
        diversity_score = min(unique_sources / 3.0, 1.0)  # 3ê°œ ì´ìƒ ì†ŒìŠ¤ë©´ ë§Œì 
        
        # ì¸ì‚¬ì´íŠ¸ ì ìˆ˜
        insight_score = min(len(state.insights) / 5.0, 1.0)  # 5ê°œ ì´ìƒ ì¸ì‚¬ì´íŠ¸ë©´ ë§Œì 
        
        return (avg_doc_score * 0.5 + diversity_score * 0.3 + insight_score * 0.2)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. Streamlit UI (Deep Research ìŠ¤íƒ€ì¼ë¡œ ì—…ë°ì´íŠ¸)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.write("CUDA available:", torch.cuda.is_available())

st.title("ğŸ§  Deep Research Chatbot by C.H.PARK")
st.markdown("### AI ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í˜‘ì—… ì‹œìŠ¤í…œìœ¼ë¡œ êµ¬í˜„í•œ ì‹¬ì¸µ ì—°êµ¬ ê¸°ëŠ¥")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.header("âš™ï¸ ì„¤ì •")

# ì²­í‚¹ íŒŒë¼ë¯¸í„°
st.sidebar.subheader("ì²­í‚¹ ì„¤ì •")
min_chunk_length = st.sidebar.slider("ìµœì†Œ ì²­í¬ ê¸¸ì´", 30, 500, 50)
max_chunk_length = st.sidebar.slider("ìµœëŒ€ ì²­í¬ ê¸¸ì´", 200, 3000, 300)
sentences_per_chunk = st.sidebar.slider("ì²­í¬ë‹¹ ìµœëŒ€ ë¬¸ì¥ ìˆ˜", 1, 10, 2)

# PDF ì—…ë¡œë“œ
st.sidebar.subheader("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ")
uploaded_files = st.sidebar.file_uploader(
    "PDF íŒŒì¼ ì—…ë¡œë“œ", 
    type="pdf", 
    accept_multiple_files=True,
    help="ì—¬ëŸ¬ PDF íŒŒì¼ì„ ë™ì‹œì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
)

# ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë²„íŠ¼
if st.sidebar.button("ğŸ”„ Deep Research ì‹œìŠ¤í…œ ì‹œì‘", type="primary"):
    if not uploaded_files:
        st.sidebar.error("âŒ ìµœì†Œ í•˜ë‚˜ì˜ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    else:
        # ì—…ë¡œë“œëœ íŒŒì¼ ì €ì¥
        pdf_paths = []
        temp_dir = "temp_uploads"
        os.makedirs(temp_dir, exist_ok=True)
        
        for uploaded_file in uploaded_files:
            file_path = os.path.join(temp_dir, uploaded_file.name)
            with open(file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            pdf_paths.append(file_path)
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        with st.spinner("ğŸ”„ Deep Research ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘..."):
            try:
                st.session_state.research_bot = DeepResearchOrchestrator(
                    min_chunk_length=min_chunk_length,
                    max_chunk_length=max_chunk_length,
                    sentences_per_chunk=sentences_per_chunk
                )
                
                # PDF ë¬¸ì„œ ë¡œë”©
                st.session_state.research_bot.load_pdf_documents(pdf_paths)
                
                st.sidebar.success("âœ… Deep Research ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
                
                # ì‹œìŠ¤í…œ ì •ë³´ í‘œì‹œ
                st.sidebar.info(f"ğŸ“Š ì´ {len(st.session_state.research_bot.documents)}ê°œ ì²­í¬ ìƒì„±ë¨")
                st.sidebar.info(f"ğŸ“ {len(st.session_state.research_bot.loaded_pdfs)}ê°œ íŒŒì¼ ë¡œë”©ë¨")
                
            except Exception as e:
                st.sidebar.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# ë©”ì¸ ì˜ì—­
if "research_bot" in st.session_state:
    # ë¬¸ì„œ ì •ë³´ í‘œì‹œ
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ğŸ“ ë¡œë”©ëœ íŒŒì¼", len(st.session_state.research_bot.loaded_pdfs))
    with col2:
        st.metric("ğŸ“„ ìƒì„±ëœ ì²­í¬", len(st.session_state.research_bot.documents))
    with col3:
        avg_chunk_size = np.mean([len(doc["text"]) for doc in st.session_state.research_bot.documents]) if st.session_state.research_bot.documents else 0
        st.metric("ğŸ“ í‰ê·  ì²­í¬ í¬ê¸°", f"{avg_chunk_size:.0f}ì")
    with col4:
        st.metric("ğŸ¤– í™œì„± ì—ì´ì „íŠ¸", "5ê°œ")
    
    st.divider()
    
    # ì§ˆë¬¸ ì…ë ¥
    st.subheader("ğŸ’¬ Deep Research ì§ˆë¬¸")
    query = st.text_input(
        "ì‹¬ì¸µ ì—°êµ¬í•  ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        placeholder="ì˜ˆ: ì²« ë²ˆì§¸ ë¬¸ì„œì™€ ë‘ ë²ˆì§¸ ë¬¸ì„œì˜ ì£¼ì¥ì„ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”.",
        key="deep_query_input"
    )
    
    col1, col2 = st.columns([1, 4])
    with col1:
        research_button = st.button("ğŸ§  Deep Research ì‹œì‘", type="primary")
    with col2:
        if st.button("ğŸ—‘ï¸ ì—°êµ¬ ê¸°ë¡ ì´ˆê¸°í™”"):
            if "research_history" in st.session_state:
                del st.session_state.research_history
            st.rerun()
    
    # Deep Research ì‹¤í–‰
    if research_button and query:
        with st.spinner("ğŸ§  ë‹¤ì¤‘ ì—ì´ì „íŠ¸ê°€ ì‹¬ì¸µ ì—°êµ¬ë¥¼ ìˆ˜í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            start_time = time.time()
            result = st.session_state.research_bot.deep_research(query)
            elapsed_time = time.time() - start_time
        
        torch.cuda.empty_cache()
        gc.collect()
        
        # ì—°êµ¬ ê¸°ë¡ ì €ì¥
        if "research_history" not in st.session_state:
            st.session_state.research_history = []
        
        st.session_state.research_history.append({
            "query": query,
            "result": result,
            "timestamp": datetime.now(),
            "elapsed_time": elapsed_time
        })
        
        # ê²°ê³¼ í‘œì‹œ
        st.subheader("ğŸ¯ Deep Research ê²°ê³¼")
        st.write(result["answer"])
        
        # ìƒì„¸ ë©”íŠ¸ë¦­ ì •ë³´
        col1, col2, col3, col4 = st.columns(4)
        metadata = result["research_metadata"]
        
        with col1:
            st.metric("ì‹ ë¢°ë„", f"{result['confidence']:.3f}")
        with col2:
            st.metric("ì—°êµ¬ ì‚¬ì´í´", f"{metadata['cycles_completed']}/{metadata.get('max_cycles', 3)}")
        with col3:
            st.metric("ë¶„ì„ ë¬¸ì„œ", metadata['total_documents_analyzed'])
        with col4:
            st.metric("ì†Œìš” ì‹œê°„", f"{elapsed_time:.1f}ì´ˆ")
        
        # ì—°êµ¬ ê³¼ì • ì‹œê°í™”
        if metadata.get('confidence_progression'):
            st.subheader("ğŸ“ˆ ì—°êµ¬ ì§„í–‰ ê³¼ì •")
            col1, col2 = st.columns(2)
            
            with col1:
                st.line_chart({
                    "ì‹ ë¢°ë„ ë³€í™”": metadata['confidence_progression']
                })
            
            with col2:
                research_metrics = {
                    "ë°œê²¬í•œ ì¸ì‚¬ì´íŠ¸": metadata['insights_discovered'],
                    "ì‹ë³„í•œ ì§€ì‹ ê²©ì°¨": metadata['knowledge_gaps_identified'],
                    "ì§ˆë¬¸ ë³µì¡ë„": f"{metadata['query_complexity']:.2f}",
                }
                for metric, value in research_metrics.items():
                    st.write(f"**{metric}**: {value}")
        
        # ê²½ê³  í‘œì‹œ
        if result["warnings"]:
            st.warning("âš ï¸ " + " | ".join(result["warnings"]))
        
        # êµì°¨ ê²€ì¦ ê²°ê³¼
        if "cross_validation" in metadata:
            cv = metadata["cross_validation"]
            st.subheader("ğŸ” êµì°¨ ê²€ì¦ ê²°ê³¼")
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("ì •ë³´ ì¼ê´€ì„±", f"{cv['consistency']:.2f}")
            with col2:
                st.metric("ìƒì¶© ì •ë³´", len(cv.get('conflicts', [])))
            with col3:
                st.metric("ê³µí†µ ì •ë³´", len(cv.get('consensus', [])))
        
        # ì°¸ì¡° ì†ŒìŠ¤ í‘œì‹œ (ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜)
        if result["sources"]:
            st.subheader("ğŸ“š ë¶„ì„ëœ ë¬¸ì„œ ì†ŒìŠ¤")
            
            # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
            categories = {}
            for source in result["sources"]:
                category = source.get('search_category', 'ì¼ë°˜')
                if category not in categories:
                    categories[category] = []
                categories[category].append(source)
            
            for category, sources in categories.items():
                with st.expander(f"ğŸ“‚ {category} ì¹´í…Œê³ ë¦¬ ({len(sources)}ê°œ ë¬¸ì„œ)"):
                    for i, source in enumerate(sources, 1):
                        st.write(f"**ë¬¸ì„œ {i}: {source['source_file']}** (ì ìˆ˜: {source['similarity']:.3f})")
                        if source.get('key_insight'):
                            st.write(f"ğŸ’¡ **í•µì‹¬ ì¸ì‚¬ì´íŠ¸**: {source['key_insight']}")
                        st.write(f"ğŸ“„ **ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°**: {source['preview'][:200]}...")
                        st.write("---")
        
        # ì—°êµ¬ ë¡œê·¸ í‘œì‹œ
        with st.expander("ğŸ” ìƒì„¸ ì—°êµ¬ ë¡œê·¸"):
            for log_entry in metadata.get('research_log', []):
                st.write(log_entry)

    # ì—°êµ¬ ê¸°ë¡ í‘œì‹œ
    if "research_history" in st.session_state and st.session_state.research_history:
        st.divider()
        st.subheader("ğŸ“œ Deep Research ê¸°ë¡")
        
        for i, research in enumerate(reversed(st.session_state.research_history[-3:]), 1):
            with st.expander(f"ì—°êµ¬ {len(st.session_state.research_history) - i + 1}: {research['query'][:50]}..."):
                st.write(f"**ì§ˆë¬¸**: {research['query']}")
                st.write(f"**ì£¼ìš” ê²°ê³¼**: {research['result']['answer'][:300]}...")
                st.write(f"**ì—°êµ¬ ì‹œê°„**: {research['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}")
                st.write(f"**ìµœì¢… ì‹ ë¢°ë„**: {research['result']['confidence']:.3f}")
                
                metadata = research['result']['research_metadata']
                st.write(f"**ì—°êµ¬ ê¹Šì´**: {metadata['cycles_completed']}ì‚¬ì´í´, {metadata['total_documents_analyzed']}ê°œ ë¬¸ì„œ ë¶„ì„")

else:
    # ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°
    st.info("ğŸ‘† ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'Deep Research ì‹œìŠ¤í…œ ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•´ì£¼ì„¸ìš”.")

# ì‚¬ìš© ê°€ì´ë“œ
with st.expander("ğŸ“ Deep Research ì‚¬ìš© ê°€ì´ë“œ"):
    st.markdown("""
    ### ğŸ§  Deep Research ì‹œìŠ¤í…œì˜ íŠ¹ì§•
    
    **ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í˜‘ì—…**:
    - ğŸ“‹ **Research Planner**: ì§ˆë¬¸ ë¶„ì„ ë° ì—°êµ¬ ê³„íš ìˆ˜ë¦½
    - ğŸ” **Retriever Agent**: ë‹¤ì¤‘ ì¿¼ë¦¬ ê¸°ë°˜ í¬ê´„ì  ê²€ìƒ‰
    - ğŸ”¬ **Analyzer Agent**: ë¬¸ì„œ ë¶„ì„ ë° êµì°¨ ê²€ì¦
    - ğŸ“ **Synthesizer Agent**: ì¢…í•©ì  ë‹µë³€ ìƒì„±
    - âœ… **Validator Agent**: ìµœì¢… í’ˆì§ˆ ê²€ì¦
    
    **ì§€ëŠ¥í˜• ì—°êµ¬ ê³¼ì •**:
    - ì§ˆë¬¸ ë³µì¡ë„ì— ë”°ë¥¸ ì ì‘í˜• ì—°êµ¬ ì‚¬ì´í´ (2-3íšŒ)
    - ì§€ì‹ ê²©ì°¨ ìë™ ì‹ë³„ ë° ë³´ì™„ ê²€ìƒ‰
    - ì‹¤ì‹œê°„ ì‹ ë¢°ë„ í‰ê°€ ë° ì¡°ê¸° ì¢…ë£Œ
    - ë‹¤ê°ë„ ì •ë³´ ìˆ˜ì§‘ ë° êµì°¨ ê²€ì¦
    
    **ìµœì í™”ëœ ì§ˆë¬¸ ìœ í˜•**:
    - ğŸ” **ë¹„êµ ë¶„ì„**: "Aì™€ Bì˜ ì°¨ì´ì ì„ ë¶„ì„í•´ì£¼ì„¸ìš”"
    - ğŸ¯ **ì‹¬ì¸µ íƒêµ¬**: "Xì˜ ì›ì¸ê³¼ ê²°ê³¼ë¥¼ ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”"  
    - ğŸ”— **ê´€ê³„ ë¶„ì„**: "Yì™€ Z ì‚¬ì´ì˜ ì—°ê´€ì„±ì„ ì°¾ì•„ì£¼ì„¸ìš”"
    - ğŸ“Š **ì¢…í•© í‰ê°€**: "ì—¬ëŸ¬ ê´€ì ì—ì„œ Wë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”"
    """)
    
    st.info("ğŸ’¡ ì¼ë°˜ RAG ëŒ€ë¹„ ë” ì •í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì§€ë§Œ, ì²˜ë¦¬ ì‹œê°„ì´ ë” ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
