from __future__ import annotations

import os
import re
import json
import hashlib
import time
from datetime import datetime
from typing import List, Dict, Any

import streamlit as st
import numpy as np
import torch, gc
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
from sentence_transformers.cross_encoder import CrossEncoder
import warnings

# PDF ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False


# í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    from kiwipiepy import Kiwi
    KIWI_AVAILABLE = True
except ImportError:
    KIWI_AVAILABLE = False

warnings.filterwarnings("ignore")
os.environ["TRANSFORMERS_VERBOSITY"] = "error"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Mean Pooling í•¨ìˆ˜ ì •ì˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output.last_hidden_state
    mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * mask_expanded, 1) / torch.clamp(mask_expanded.sum(1), min=1e-9)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ê°œì„ ëœ í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬ í´ë˜ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class ImprovedKoreanSentenceChunker:
    def __init__(self, min_chunk_length=50, max_chunk_length=300, sentences_per_chunk=2):
        self.min_chunk_length = min_chunk_length
        self.max_chunk_length = max_chunk_length
        self.sentences_per_chunk = sentences_per_chunk
        
        # Kiwi ì´ˆê¸°í™” (ë¬¸ì¥ ë¶„ë¦¬ìš©)
        self.kiwi = None
        if KIWI_AVAILABLE:
            try:
                self.kiwi = Kiwi()
                # st.info("âœ… Kiwi í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.warning(f"âš ï¸ Kiwi ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        self.available_methods = []
        if KIWI_AVAILABLE and self.kiwi:
            self.available_methods.append("Kiwi")
        self.available_methods.append("Regex")
        
        # st.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ì¥ ë¶„ë¦¬ ë°©ë²•: {', '.join(self.available_methods)}")

    def chunk_text(self, text: str) -> List[str]:
        if not text.strip():
            return []
        
        # 1ë‹¨ê³„: ë¬¸ì¥ ë¶„ë¦¬
        sentences = self._split_sentences(text)
        if not sentences:
            return []
        
        # 2ë‹¨ê³„: ë¬¸ì¥ í›„ì²˜ë¦¬ (ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ë³‘í•©)
        sentences = self._postprocess_sentences(sentences)
        
        # 3ë‹¨ê³„: ì²­í‚¹
        chunks = self._create_chunks(sentences)
        
        return chunks

    def _split_sentences(self, text: str) -> List[str]:
        """ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ë¬¸ì¥ ë¶„ë¦¬ ì‹œë„"""
                
        # ë°©ë²• 2: Kiwi ì‚¬ìš©
        if KIWI_AVAILABLE and self.kiwi:
            try:
                kiwi_result = self.kiwi.split_into_sents(text.strip())
                sentences = [sent.text.strip() for sent in kiwi_result if sent.text.strip()]
                if sentences and len(sentences) > 1:
                    return sentences
            except Exception as e:
                st.warning(f"Kiwi ë¬¸ì¥ ë¶„ë¦¬ ì‹¤íŒ¨: {e}")
        
        # ë°©ë²• 3: ê°œì„ ëœ ì •ê·œì‹ ì‚¬ìš© (fallback)
        return self._regex_sentence_split(text.strip())

    def _regex_sentence_split(self, text: str) -> List[str]:
        """ê°œì„ ëœ ì •ê·œì‹ ê¸°ë°˜ ë¬¸ì¥ ë¶„ë¦¬"""
        # í•œêµ­ì–´ ë¬¸ì¥ ì¢…ê²° íŒ¨í„´ë“¤
        patterns = [
            r'[.!?]+\s+',  # ê¸°ë³¸ ë¬¸ì¥ ë¶€í˜¸ + ê³µë°±
            r'[ë‹¤ê°€ë‚˜ë‹ˆê¹Œìš”ë˜ìŠµë‹ˆë‹¤]\s*[.!?]*\s+',  # í•œêµ­ì–´ ì¢…ê²°ì–´ë¯¸ + ë¬¸ì¥ë¶€í˜¸
            r'[ë‹¤ê°€ë‚˜ë‹ˆê¹Œìš”ë˜ìŠµë‹ˆë‹¤]\s+',  # í•œêµ­ì–´ ì¢…ê²°ì–´ë¯¸ + ê³µë°±
            r'[ë‹ˆë‹¤í–ˆë‹¤ìŠµë‹ˆë‹¤ì˜€ë‹¤ì•˜ë‹¤]\s*[.!?]*\s+',  # ì¶”ê°€ ì¢…ê²°ì–´ë¯¸
            r'\n\s*\n',  # ë¹ˆ ì¤„
            r'\.\s*\n',  # ë§ˆì¹¨í‘œ + ì¤„ë°”ê¿ˆ
        ]
        
        combined_pattern = '|'.join(f'({p})' for p in patterns)
        sentences = re.split(combined_pattern, text)
        
        # ë¹ˆ ë¬¸ìì—´ê³¼ êµ¬ë¶„ì ì œê±°
        result = []
        for s in sentences:
            if s and not re.match(r'^\s*[.!?\n\s]*$', s):
                result.append(s.strip())
        
        return result if result else [text.strip()]

    def _postprocess_sentences(self, sentences: List[str]) -> List[str]:
        """ë¬¸ì¥ í›„ì²˜ë¦¬: ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ë³‘í•©"""
        if not sentences:
            return []
        
        processed = []
        current_sentence = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # í˜„ì¬ ë¬¸ì¥ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ë‹¤ìŒ ë¬¸ì¥ê³¼ ë³‘í•©
            if len(current_sentence) < self.min_chunk_length:
                if current_sentence:
                    current_sentence += " " + sentence
                else:
                    current_sentence = sentence
            else:
                # í˜„ì¬ ë¬¸ì¥ì´ ì¶©ë¶„íˆ ê¸¸ë©´ ì €ì¥í•˜ê³  ìƒˆ ë¬¸ì¥ ì‹œì‘
                if current_sentence:
                    processed.append(current_sentence)
                current_sentence = sentence
        
        # ë§ˆì§€ë§‰ ë¬¸ì¥ ì²˜ë¦¬
        if current_sentence:
            if processed and len(current_sentence) < self.min_chunk_length:
                processed[-1] += " " + current_sentence
            else:
                processed.append(current_sentence)
        
        return processed

    def _create_chunks(self, sentences: List[str]) -> List[str]:
        """ë¬¸ì¥ë“¤ì„ ì²­í¬ë¡œ ê·¸ë£¹í™”"""
        if not sentences:
            return []
        
        chunks = []
        current_chunk = ""
        sentence_count = 0
        
        for sentence in sentences:
            # ì²­í¬ì— ë¬¸ì¥ ì¶”ê°€ ê°€ëŠ¥í•œì§€ í™•ì¸
            test_chunk = current_chunk + (" " + sentence if current_chunk else sentence)
            
            if (len(test_chunk) <= self.max_chunk_length and 
                sentence_count < self.sentences_per_chunk):
                current_chunk = test_chunk
                sentence_count += 1
            else:
                # í˜„ì¬ ì²­í¬ ì™„ì„±í•˜ê³  ìƒˆ ì²­í¬ ì‹œì‘
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence
                sentence_count = 1
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ê°œì„ ëœ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í´ë˜ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class ImprovedPDFExtractor:
    def __init__(self):
        self.available_methods = []
        if PDFPLUMBER_AVAILABLE:
            self.available_methods.append("pdfplumber")
    
        
        # st.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ PDF ì¶”ì¶œ ë°©ë²•: {', '.join(self.available_methods)}")

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)"""
        
        # ë°©ë²• 1: pdfplumber (ê°€ì¥ ì •í™•í•œ í•œê¸€ ì²˜ë¦¬)
        if PDFPLUMBER_AVAILABLE:
            try:
                text = self._extract_with_pdfplumber(pdf_path)
                if text.strip():
                    # st.success("âœ… pdfplumberë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ")
                    return text
            except Exception as e:
                st.warning(f"pdfplumber ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    

    def _extract_with_pdfplumber(self, pdf_path: str) -> str:
        """pdfplumberë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        text = ""
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        return text

    
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ë””ë°”ì´ìŠ¤ ì„¤ì • í´ë˜ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class DeviceConfig:
    def __init__(self):
        self.device, self.info = self._detect()
        self.config = self._get_config()

    def _detect(self):
        if torch.cuda.is_available():
            return "cuda", {
                "type": "GPU",
                "name": torch.cuda.get_device_name(0),
                "memory": torch.cuda.get_device_properties(0).total_memory // (1024**3),
                "count": torch.cuda.device_count()
            }
        try:
            import psutil
            return "cpu", {
                "type": "CPU",
                "name": "CPU",
                "cores": psutil.cpu_count(),
                "memory": psutil.virtual_memory().total // (1024**3)
            }
        except ImportError:
            return "cpu", {"type": "CPU", "name": "CPU", "cores": "Unknown", "memory": "Unknown"}

    def _get_config(self):
        if self.device == "cuda":
            mem = self.info["memory"]
            if mem >= 24:
                return {"torch_dtype": torch.bfloat16, "max_new_tokens": 8000, "top_k": 12,
                        "embedding_batch_size": 64, "do_sample": True,
                        "temperature": 0.1, "top_p": 0.9, "expected_time": "5-15ì´ˆ",
                        "sim_threshold": 0.3}
            if mem >= 12:
                return {"torch_dtype": torch.bfloat16, "max_new_tokens": 6000, "top_k": 12,
                        "embedding_batch_size": 32, "do_sample": True,
                        "temperature": 0.1, "top_p": 0.9, "expected_time": "5-15ì´ˆ",
                        "sim_threshold": 0.3}
            if mem >= 8:
                return {"torch_dtype": torch.bfloat16, "max_new_tokens": 4000, "top_k": 10,
                        "embedding_batch_size": 16, "do_sample": True,
                        "temperature": 0.1, "top_p": 0.9, "expected_time": "8-20ì´ˆ",
                        "sim_threshold": 0.3}
            return {"torch_dtype": torch.float16, "max_new_tokens": 3000, "temperature": 0.1, "top_k": 10,
                    "embedding_batch_size": 8, "do_sample": False, "expected_time": "10-25ì´ˆ",
                    "sim_threshold": 0.3}
        return {"torch_dtype": torch.float32, "max_new_tokens": 2000, "temperature": 0.1, "top_k": 10,
                "embedding_batch_size": 4, "do_sample": False, "expected_time": "30-90ì´ˆ",
                "sim_threshold": 0.3}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ê°œì„ ëœ ë©”ì¸ ì±—ë´‡ í´ë˜ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class ImprovedPDFNotebookLM:
    _FORBIDDEN_PHRASES = {
        "ì•Œë ¤ì§€ì§€ ì•Šì€", "í™•ì‹¤í•˜ì§€ ì•Šì€", "ì•„ë§ˆë„",
        "ì¶”ì¸¡í•˜ê±´ëŒ€", "ì¼ë°˜ì ìœ¼ë¡œ", "ë³´í†µ", "ëŒ€ë¶€ë¶„ì˜ ê²½ìš°"
    }

    def __init__(
        self,
        model_name: str = "LGAI-EXAONE/EXAONE-4.0-1.2B",
        embed_model: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        reranker_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2",
        min_chunk_length: int = 50,
        max_chunk_length: int = 300,
        sentences_per_chunk: int = 2,
    ):
        # st.info("ğŸš€ PDF ë¶„ì„ ì¤‘...")
        
        # Device & Components
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device_config = DeviceConfig()
        self.pdf_extractor = ImprovedPDFExtractor()
        self.chunker = ImprovedKoreanSentenceChunker(
            min_chunk_length=min_chunk_length,
            max_chunk_length=max_chunk_length,
            sentences_per_chunk=sentences_per_chunk
        )


        # Embedding Model
        st.info("â–¶ ì„ë² ë”© ëª¨ë¸ ë¡œë”©...")
        self.embed_tokenizer = AutoTokenizer.from_pretrained(embed_model)
        self.embed_model = AutoModel.from_pretrained(embed_model).to(self.device_config.device)

        # Cross-Encoder
        # st.info("â–¶ Cross-Encoder ë¡œë”©...")
        self.reranker = CrossEncoder(reranker_name, device=self.device_config.device)

        # Containers
        self.documents: List[Dict[str, Any]] = []
        self.embeddings: np.ndarray | None = None
        self.chunk_hashes: Dict[str, Dict] = {}
        self.loaded_pdfs: List[str] = []
        torch.cuda.empty_cache()
        gc.collect()
        # st.success("âœ… PDF ë¶„ì„ ì™„ë£Œ!")

                # LLM
        st.info("â–¶ LLM ë¡œë”©...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
      #  self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=self.device_config.config["torch_dtype"], max_memory={0: "14GB"}, trust_remote_code=True).to(self.device)

        map_dev = "auto" if self.device_config.device == "cuda" else "cpu"
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype=self.device_config.config["torch_dtype"],
            device_map=map_dev, max_memory={0: "14GB"}, trust_remote_code=True
        ).to(self.device)

    def load_pdf_documents(self, pdf_paths: List[str]) -> None:
        """ê°œì„ ëœ PDF ë¬¸ì„œ ë¡œë”©"""
        st.info(f"ğŸ“š PDF ë¬¸ì„œ ì²˜ë¦¬ ì¤‘... ({len(pdf_paths)}ê°œ)")
        
        self.documents, self.embeddings, self.chunk_hashes, self.loaded_pdfs = [], None, {}, []
        all_docs: List[Dict[str, Any]] = []
        
        progress_bar = st.progress(0)
        
        for i, pdf_path in enumerate(pdf_paths):
            if not os.path.exists(pdf_path):
                st.warning(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {pdf_path}")
                continue
                
            # st.info(f"ì²˜ë¦¬ ì¤‘: {os.path.basename(pdf_path)}")
            
            # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
            full_text = self.pdf_extractor.extract_text_from_pdf(pdf_path)
            if not full_text.strip():
                st.warning(f"âš ï¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ: {pdf_path}")
                continue
            
            # í…ìŠ¤íŠ¸ ì²­í‚¹
            chunks = self.chunker.chunk_text(full_text)
            
            # ë¬¸ì„œ ê°ì²´ ìƒì„±
            for chunk_idx, chunk in enumerate(chunks, 1):
                if chunk.strip():
                    all_docs.append({
                        "text": chunk.strip(),
                        "page": 1,  # ì‹¤ì œ í˜ì´ì§€ ë²ˆí˜¸ëŠ” ë³µì¡í•˜ë¯€ë¡œ ì„ì‹œë¡œ 1 ì‚¬ìš©
                        "paragraph": chunk_idx,
                        "source": os.path.basename(pdf_path),
                        "full_path": pdf_path
                    })
            
            self.loaded_pdfs.append(pdf_path)
            progress_bar.progress((i + 1) / len(pdf_paths))
            
            # st.success(f"âœ… {os.path.basename(pdf_path)}: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

        if not all_docs:
            st.error("âŒ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        self.documents = all_docs

        # ì„ë² ë”© ìƒì„±
        st.info("ğŸ§® ì„ë² ë”© ìƒì„± ì¤‘...")
        self._generate_embeddings()
        
        # ì²­í¬ í•´ì‹œ ìƒì„±
        for d in self.documents:
            cid = self._hash(d["text"], d["page"], d["paragraph"], d["source"])
            d["chunk_id"] = cid
            self.chunk_hashes[cid] = d

        st.success(f"ğŸ‰ ì²˜ë¦¬ ì™„ë£Œ! ì´ {len(self.documents)}ê°œ ì²­í¬ ìƒì„±")

    def _generate_embeddings(self):
        """ì„ë² ë”© ìƒì„±"""
        texts = [d["text"] for d in self.documents]
        bs = self.device_config.config["embedding_batch_size"]
        embs = []
        
        progress_bar = st.progress(0)
        
        for i in range(0, len(texts), bs):
            batch = texts[i : i + bs]
            enc = self.embed_tokenizer(batch, padding=True, truncation=True, return_tensors="pt").to(self.device_config.device)
            with torch.no_grad():
                out = self.embed_model(**enc)
            pooled = mean_pooling(out, enc["attention_mask"])
            embs.append(pooled.cpu().numpy())
            
            progress_bar.progress(min(1.0, (i + bs) / len(texts)))
        
        self.embeddings = np.vstack(embs)

    @staticmethod
    def _hash(text: str, page: int, para: int, source: str) -> str:
        return hashlib.md5(f"{source}_{page}_{para}_{text}".encode()).hexdigest()[:8]

    def generate_answer(self, query: str) -> Dict[str, Any]:
        top_k = self.device_config.config["top_k"]
        chunks = self._retrieve(query, top_k)
        
        if not chunks:
            return {"answer": "ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "sources": [], "confidence": 0.0, "warnings": []}

        prompt = self._build_prompt(query, chunks)
        answer = self._llm_infer(prompt, chunks)
        # answer = self._llm_infer(prompt)
        confidence, warns = self._validate(answer, chunks)
        
        if confidence < 0.5:
            answer = self._conservative_reply(query, chunks)
            warns.append("ìë™ ë³´ìˆ˜ì  ë‹µë³€ ëª¨ë“œ ì ìš©")

        sources = [
            {
                "page": c["page"],
                "paragraph": c["paragraph"],
                "chunk_id": c["chunk_id"],
                "source_file": c["source"],
                "preview": c["text"][:400] + "..." if len(c["text"]) > 400 else c["text"],
                "similarity": float(c["similarity"]),
                "chunk_size": len(c["text"])
            }
            for c in chunks
        ]

        return {"answer": answer, "confidence": confidence, "warnings": warns, "sources": sources}

    def _retrieve(self, query: str, top_k: int) -> List[Dict]:
        enc = self.embed_tokenizer([query], padding=True, truncation=True, return_tensors="pt").to(self.device_config.device)
        with torch.no_grad():
            out = self.embed_model(**enc)
        q_emb = mean_pooling(out, enc["attention_mask"]).cpu().numpy()
        
        sims = cosine_similarity(q_emb, self.embeddings)[0]
        idxs = np.argsort(sims)[::-1][: top_k * 6]
        threshold = self.device_config.config["sim_threshold"]
        
        candidates = []
        for idx in idxs:
            if sims[idx] < threshold:
                continue
            doc = self.documents[idx].copy()
            doc["similarity"] = sims[idx]
            candidates.append(doc)

        if not candidates:
            return []

        texts = [c["text"] for c in candidates]
        scores = self.reranker.predict([(query, t) for t in texts])
        for c, s in zip(candidates, scores):
            c["similarity"] = float(s)
        
        candidates.sort(key=lambda x: x["similarity"], reverse=True)
        return candidates[:top_k]
    
    def _build_prompt(self, query: str, chunks: List[Dict]) -> List[Dict]:
        context = ""
        for i, c in enumerate(chunks, 1):
            context += f"[ë¬¸ì„œ {i}] ì¶œì²˜: {c['source']}, {c['page']}í˜ì´ì§€\n{c['text']}\n\n"
    
        sys = """ë‹¹ì‹ ì€ ì˜¤ì§ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ì „ë¬¸ ë¶„ì„ AIì…ë‹ˆë‹¤.

í•µì‹¬ ê·œì¹™:
1. ì œê³µëœ ë¬¸ì„œì— ëª…ì‹œëœ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
2. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
3. ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.
4. ë‹µë³€í•  ë•Œ ë°˜ë“œì‹œ ì°¸ì¡°í•œ ë¬¸ì„œì˜ ì¶œì²˜(íŒŒì¼ëª…, í˜ì´ì§€)ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
5. ë¬¸ì„œ ë‚´ìš©ê³¼ ëª¨ìˆœë˜ëŠ” ë‹µë³€ì„ í•˜ì§€ ë§ˆì„¸ìš”.
6. ë‹µë³€í•  ë•Œ ê²°ë¡ ì— ë„ë‹¬í•˜ê²Œ ëœ ì´ìœ ë¥¼ ë°˜ë“œì‹œ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.

ë‹µë³€ í˜•ì‹:
- ë¬¸ì„œì—ì„œ ì°¾ì€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€
- ê° ì •ë³´ì˜ ì¶œì²˜ë¥¼ (íŒŒì¼ëª…, í˜ì´ì§€ë²ˆí˜¸)ë¡œ ëª…ì‹œ
- ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ì•ŠìŒ"""

        messages = [
            {"role": "system", "content": sys},
        {"role": "user", "content": f"ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ì¡°í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.\n\n=== ì°¸ì¡° ë¬¸ì„œ ===\n{context}\n=== ì§ˆë¬¸ ===\n{query}"}
    ]
    
        return messages


    # def _build_prompt(self, query: str, chunks: List[Dict]) -> str:
    #     context = ""
    #     for i, c in enumerate(chunks, 1):
    #         context += f"[ì²­í¬ {i}] {c['source']}, {c['page']}í˜ì´ì§€:\n{c['text']}\n\n"
        
    #     sys = ("""ë‹¹ì‹ ì€ í”„ë¡¬í”„íŠ¸ë¡œ ì£¼ì–´ì§„ contextë§Œ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ì•„ ë‹µë³€í•˜ëŠ” íŒì‚¬ AIì…ë‹ˆë‹¤. 
    #            í”„ë¡¬í”„íŠ¸ë¡œ ì£¼ì–´ì§„ contextì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ê³  ì •í™•ë„ê°€ ë†’ì€ contextë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ 'í•´ë‹¹ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë¨¼ì € ë‹µí•œ í›„ì— ë‚˜ë¨¸ì§€ ë‹µë³€ì„ í•˜ì„¸ìš”. 
    #            í”„ë¡¬í”„íŠ¸ë¡œ ì£¼ì–´ì§„ contextì— ìˆëŠ” ì •ë³´ì— ê¸°ì´ˆí•˜ì—¬ ì™„ì„±ëœ ë¬¸ì¥ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. ë‹µë³€ì€ ê¸¸ê²Œ í•´ì•¼ í•©ë‹ˆë‹¤. 
    #            ì°¸ì¡°í•œ contextì˜ (íŒŒì¼ëª…, í˜ì´ì§€)ë¥¼ ë°˜ë“œì‹œ ëª…ì‹œí•˜ì—¬ êµ¬ì²´ì ì´ê³ , ëª…í™•í•˜ê²Œ ìƒê°ì˜ ê³¼ì •ê³¼ ì´ìœ ë¥¼ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤."""
    #     )
    #     return f"<system>\n{sys}\n</system>\n<context>\n{context}</context>\n<user>\n{query}\n</user>\n<assistant>"
    
    def _llm_infer(self, query: str, chunks: List[Dict]) -> str:
        messages = self._build_prompt(query, chunks)
    
    # EXAONE 4.0 chat template ì‚¬ìš©
        input_ids = self.tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt",
            # return_dict=True # ì´ ë¶€ë¶„ ì¶”ê°€ ì•ˆ í•˜ë©´ ì–´í…ì…˜ ì—†ë‹¤ëŠ” ê²½ê³  ë‚˜ì˜´. ì´ ë¶€ë¶„ ì¶”ê°€í•˜ë©´ ë¦¬ìŠ¤íŠ¸ > ë”•ì…”ë„ˆë¦¬ê°€ ë˜ì–´ì„œ ë‹¤ë¥¸ ë¶€ë¶„ë„ ë°”ê¿”ì•¼ í•¨
    )
    
        if self.device_config.device == "cuda":
            input_ids = input_ids.to("cuda")
    
    # ë” ë³´ìˆ˜ì ì¸ ìƒì„± íŒŒë¼ë¯¸í„°
        gen_kwargs = {
            "max_new_tokens": self.device_config.config["max_new_tokens"],
            "do_sample": False,  # deterministic ìƒì„±
            # "temperature": 0.1,  # ë‚®ì€ temperature
            "repetition_penalty": 1.1,
            "pad_token_id": self.tokenizer.eos_token_id,
            "eos_token_id": self.tokenizer.eos_token_id
    }
    
        out = self.model.generate(input_ids, **gen_kwargs) # return_dict=Trueë©´ **input_ids
        return self.tokenizer.decode(out[0][input_ids.shape[1]:], skip_special_tokens=True).strip() # return_dict=Trueë¥¼ ë„£ì§€ ì•Šì„ ë•Œ - ì–´í…ì…˜ ì •ë³´ ì—†ë‹¤ëŠ” ê²½ê³  ë‚˜ì˜´
        # input_len = input_ids["input_ids"].shape[1]
        # return self.tokenizer.decode(out[0][input_len:], skip_special_tokens=True).strip()


    # def _llm_infer(self, prompt: str) -> str:
    #     cfg = self.device_config.config
    #     ids = self.tokenizer(prompt, return_tensors="pt")
    #     if self.device_config.device == "cuda":
    #         ids = {k: v.to("cuda") for k, v in ids.items()}
        
    #     gen_kwargs = {
    #         "max_new_tokens": cfg["max_new_tokens"],
    #         "do_sample": cfg["do_sample"],
    #         "repetition_penalty": 1.05,
    #         "pad_token_id": self.tokenizer.eos_token_id,
    #         "eos_token_id": self.tokenizer.eos_token_id
    #     }
        
    #     if cfg["do_sample"]:
    #         gen_kwargs.update({"temperature": cfg["temperature"], "top_p": cfg["top_p"]})
        
    #     out = self.model.generate(**ids, **gen_kwargs)
    #     return self.tokenizer.decode(out[0][ids["input_ids"].shape[1]:], skip_special_tokens=True).strip()

    def _validate(self, answer: str, chunks: List[Dict]) -> tuple[float, List[str]]:
        """Context ì¤€ìˆ˜ìœ¨ì„ ë” ì—„ê²©í•˜ê²Œ ê²€ì¦"""
        confidence = 0.0
        warnings = []
    
        if not chunks:
            return 0.0, ["ì°¸ì¡°í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤"]
    
    # 1. ë¬¸ì„œ ì¶œì²˜ ëª…ì‹œ í™•ì¸
        source_mentioned = False
        for chunk in chunks:
            if chunk['source'] in answer:
                source_mentioned = True
                break
    
        if not source_mentioned:
            warnings.append("ì£¼ì˜! ë¬¸ì„œ ì¶œì²˜ê°€ ë³´ë‹¤ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            confidence *= 0.7
    
    # 2. Context ì™¸ë¶€ ì •ë³´ ì‚¬ìš© íƒì§€
        context_text = " ".join([c['text'] for c in chunks])
        answer_words = set(answer.split())
        context_words = set(context_text.split())
    
        overlap_ratio = len(answer_words & context_words) / max(len(answer_words), 1)
        if overlap_ratio < 0.3:
            warnings.append("ì£¼ì˜! ë¬¸ì„œ ë‚´ìš©ê³¼ ì—°ê´€ì„±ì´ ë‚®ì•„ ë‹µë³€ ë‚´ìš©ì„ ë¯¿ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            confidence *= 0.6
    
    # 3. ì¼ë°˜ì  ì§€ì‹ ì‚¬ìš© íƒì§€
        general_knowledge_indicators = [
        "ì¼ë°˜ì ìœ¼ë¡œ", "ë³´í†µ", "ëŒ€ë¶€ë¶„", "ì•Œë ¤ì§„ ë°”ì— ë”°ë¥´ë©´",
        "ì „ë¬¸ê°€ë“¤ì€", "ì—°êµ¬ì— ë”°ë¥´ë©´", "í†µìƒì ìœ¼ë¡œ"
    ]
    
        for indicator in general_knowledge_indicators:
            if indicator in answer:
                warnings.append(f"ì¼ë°˜ì  ì§€ì‹ ì‚¬ìš© íƒì§€: '{indicator}'")
                confidence *= 0.5
    
    # 4. ê¸°ë³¸ ì‹ ë¢°ë„ ê³„ì‚°
        base_confidence = np.mean([c['similarity'] for c in chunks])
        confidence += base_confidence
    
        return min(confidence, 1.0), warnings


    # def _validate(self, answer: str, chunks: List[Dict]) -> tuple[float, List[str]]:
    #     confidence = float(np.mean([c["similarity"] for c in chunks])) if chunks else 0.0
    #     warns: List[str] = []
        
    #     for p in self._FORBIDDEN_PHRASES:
    #         if p in answer:
    #             warns.append(f"ë¶ˆí™•ì‹¤ í‘œí˜„ ë°œê²¬: '{p}'")
    #             confidence *= 0.8
        
    #     if chunks:
    #         src = " ".join(c["text"] for c in chunks)
    #         overlap = len(set(answer.split()) & set(src.split()))
    #         ratio = overlap / max(1, len(answer.split()))
    #         if ratio < 0.3:
    #             warns.append("ì¶œì²˜-ë‹µë³€ ë‹¨ì–´ ì¤‘ë³µë¥  ë‚®ìŒ")
    #             confidence *= 0.6
        
    #     return round(confidence, 3), warns

    @staticmethod
    def _conservative_reply(query: str, chunks: List[Dict]) -> str:
        lines = ["ë¬¸ì„œì—ì„œ ë‹¤ìŒ ê´€ë ¨ ì²­í¬ë“¤ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n"]
        for i, c in enumerate(chunks, 1):
            lines.append(f"{i}. ({c['source']}, {c['page']}í˜ì´ì§€)\n   \"{c['text'][:150]}...\"")
        lines.append("\në” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return "\n".join(lines)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. Streamlit UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.write("CUDA available:", torch.cuda.is_available(),
         " | GPU count:", torch.cuda.device_count(),
         " | Current device:", torch.cuda.current_device())
st.set_page_config(page_title="PDF Chatbot", layout="wide")

st.title("ğŸš€ Chatbot ìš°ì£¼ by C.H.PARK")
st.markdown("### ì¸í„°ë„· ì—°ê²° ì—†ì´ LG EXAONEìœ¼ë¡œ NotebookLM ê¸°ëŠ¥ êµ¬í˜„í•´ë³´ê¸°")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.header("âš™ï¸ ì„¤ì •")

# ì²­í‚¹ íŒŒë¼ë¯¸í„°
st.sidebar.subheader("ì²­í‚¹ ì„¤ì •")
min_chunk_length = st.sidebar.slider("ìµœì†Œ ì²­í¬ ê¸¸ì´", 30, 500, 50, help="ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ë“¤ì„ ë³‘í•©í•˜ê¸° ìœ„í•´")
max_chunk_length = st.sidebar.slider("ìµœëŒ€ ì²­í¬ ê¸¸ì´", 200, 3000, 300, help="ëª¨ë¸ ì„±ëŠ¥ì´ ë‚®ìœ¼ë©´ ìµœëŒ€ ì²­í¬ ê¸¸ì´ë¥¼ ê¸¸ê²Œ í•  ìˆ˜ ì—†ê³ , ì²­í¬ ê¸¸ì´ì™€ ê²°ê³¼ì˜ ì •í™•ë„ê°€ ë¹„ë¡€í•˜ì§€ ì•Šì•„ ì¡°ì • í•„ìš”")
sentences_per_chunk = st.sidebar.slider("ì²­í¬ë‹¹ ìµœëŒ€ ë¬¸ì¥ ìˆ˜", 1, 10, 2, help="PDFë§ˆë‹¤ ìµœëŒ€ ì„±ëŠ¥ì„ ëŒì–´ë‚´ëŠ” ì²­í¬ë‹¹ ìµœëŒ€ ë¬¸ì¥ ìˆ˜ê°€ ë‹¤ë¥´ë¯€ë¡œ")

# PDF ì—…ë¡œë“œ
st.sidebar.subheader("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ")
uploaded_files = st.sidebar.file_uploader(
    "PDF íŒŒì¼ ì—…ë¡œë“œ", 
    type="pdf", 
    accept_multiple_files=True,
    help="ì—¬ëŸ¬ PDF íŒŒì¼ì„ ë™ì‹œì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
)

# ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë²„íŠ¼
if st.sidebar.button("ğŸ”„ PDF ë¶„ì„ ì‹œì‘", type="primary"):
    if not uploaded_files:
        st.sidebar.error("âŒ ìµœì†Œ í•˜ë‚˜ì˜ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    else:
        # ì—…ë¡œë“œëœ íŒŒì¼ ì €ì¥
        pdf_paths = []
        temp_dir = "temp_uploads"
        os.makedirs(temp_dir, exist_ok=True)
        
        for uploaded_file in uploaded_files:
            file_path = os.path.join(temp_dir, uploaded_file.name)
            with open(file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            pdf_paths.append(file_path)
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        with st.spinner("ğŸ”„ PDF ë¶„ì„ ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
            try:
                st.session_state.bot = ImprovedPDFNotebookLM(
                    min_chunk_length=min_chunk_length,
                    max_chunk_length=max_chunk_length,
                    sentences_per_chunk=sentences_per_chunk
                )
                
                # PDF ë¬¸ì„œ ë¡œë”©
                st.session_state.bot.load_pdf_documents(pdf_paths)
                
                st.sidebar.success("âœ… PDF ë¶„ì„ ì™„ë£Œ!")
                
                # ì‹œìŠ¤í…œ ì •ë³´ í‘œì‹œ
                st.sidebar.info(f"ğŸ“Š ì´ {len(st.session_state.bot.documents)}ê°œ ì²­í¬ ìƒì„±ë¨")
                st.sidebar.info(f"ğŸ“ {len(st.session_state.bot.loaded_pdfs)}ê°œ íŒŒì¼ ë¡œë”©ë¨")
                
            except Exception as e:
                st.sidebar.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# ë©”ì¸ ì˜ì—­
if "bot" in st.session_state:
    # ë¬¸ì„œ ì •ë³´ í‘œì‹œ
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ğŸ“ ë¡œë”©ëœ íŒŒì¼", len(st.session_state.bot.loaded_pdfs))
    with col2:
        st.metric("ğŸ“„ ìƒì„±ëœ ì²­í¬", len(st.session_state.bot.documents))
    with col3:
        avg_chunk_size = np.mean([len(doc["text"]) for doc in st.session_state.bot.documents]) if st.session_state.bot.documents else 0
        st.metric("ğŸ“ í‰ê·  ì²­í¬ í¬ê¸°", f"{avg_chunk_size:.0f}ì")
    
    st.divider()
    
    # ì§ˆë¬¸ ì…ë ¥
    st.subheader("ğŸ’¬ ì§ˆë¬¸í•˜ê¸°")
    query = st.text_input(
        "ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì§ˆë¬¸í•´ì£¼ì„¸ìš”:",
        placeholder="ì˜ˆ: ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        key="query_input"
    )
    
    col1, col2 = st.columns([1, 4])
    with col1:
        ask_button = st.button("ğŸ” ë‹µë³€ ìƒì„±", type="primary")
    with col2:
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
            if "conversation_history" in st.session_state:
                del st.session_state.conversation_history
            st.rerun()
    
    # ë‹µë³€ ìƒì„±
    if ask_button and query:
        with st.spinner("ğŸ¤” ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            start_time = time.time()
            result = st.session_state.bot.generate_answer(query)
            elapsed_time = time.time() - start_time
            torch.cuda.empty_cache()
            gc.collect()
        
        # ëŒ€í™” ê¸°ë¡ ì €ì¥
        if "conversation_history" not in st.session_state:
            st.session_state.conversation_history = []
        
        st.session_state.conversation_history.append({
            "query": query,
            "result": result,
            "timestamp": datetime.now(),
            "elapsed_time": elapsed_time
        })
        
        # ê²°ê³¼ í‘œì‹œ
        st.subheader("ğŸ’¡ ë‹µë³€")
        st.write(result["answer"])
        
        # ë©”íŠ¸ë¦­ ì •ë³´
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ì‹ ë¢°ë„", f"{result['confidence']:.3f}")
        with col2:
            st.metric("ì‘ë‹µ ì‹œê°„", f"{elapsed_time:.2f}ì´ˆ")
        with col3:
            st.metric("ì°¸ì¡° ì†ŒìŠ¤", len(result["sources"]))
        
        # ê²½ê³  í‘œì‹œ
        if result["warnings"]:
            st.warning("âš ï¸ " + " | ".join(result["warnings"]))
        
        # ì°¸ì¡° ì†ŒìŠ¤ í‘œì‹œ
        if result["sources"]:
            st.subheader("ğŸ“š ì°¸ì¡°ëœ ë¬¸ì„œ ì²­í¬")
            for i, source in enumerate(result["sources"], 1):
                with st.expander(f"ì²­í¬ {i}: {source['source_file']} (ìœ ì‚¬ë„: {source['similarity']:.3f})"):
                    st.write(f"**íŒŒì¼:** {source['source_file']}")
                    st.write(f"**í˜ì´ì§€:** {source['page']}")
                    st.write(f"**ì²­í¬ í¬ê¸°:** {source['chunk_size']}ì")
                    st.write(f"**ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:**")
                    st.write(source["preview"])

    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    if "conversation_history" in st.session_state and st.session_state.conversation_history:
        st.divider()
        st.subheader("ğŸ“œ ëŒ€í™” ê¸°ë¡")
        
        for i, conversation in enumerate(reversed(st.session_state.conversation_history[-5:]), 1):
            with st.expander(f"ì§ˆë¬¸ {len(st.session_state.conversation_history) - i + 1}: {conversation['query'][:50]}..."):
                st.write(f"**ì§ˆë¬¸:** {conversation['query']}")
                st.write(f"**ë‹µë³€:** {conversation['result']['answer']}")
                st.write(f"**ì‹œê°„:** {conversation['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}")
                st.write(f"**ì‹ ë¢°ë„:** {conversation['result']['confidence']:.3f}")

else:
    # ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°
    st.info("ğŸ‘† ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'PDF ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•´ì£¼ì„¸ìš”.")
    
# ì„¤ì¹˜ ê°€ì´ë“œ
with st.expander("ğŸ“¦ ì§ˆë¬¸ ê°€ì´ë“œ"):
    st.code("""
# ë¬¸ì¥ í˜•ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.
# ëª…í™•í•˜ê³  ê°„ë‹¨í•œ ë¬¸ì¥ìœ¼ë¡œ ì§ˆë¬¸í•´ì•¼ ì •í™•í•œ ë‹µë³€ì´ ë‚˜ì˜µë‹ˆë‹¤.
# ìƒí˜¸ ê´€ë ¨ì„±ì´ ë†’ì€ PDFë¥¼ í•¨ê»˜ ë¡œë“œí•´ì•¼ ë‹µë³€ì´ ì •í™•í•´ì§‘ë‹ˆë‹¤. 
# ëª¨ìˆœëœ ë‚´ìš©ì˜ PDFë¥¼ í•¨ê»˜ ë¡œë“œí•˜ë©´ ì°¸ì¡°í•œ ë¶€ë¶„ì— ë”°ë¼ ë‹¤ë¥¸ ë‹µë³€ì„ í•  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.  
    """)
    

    st.info("ğŸ’¡ê´€ë ¨ì„±ì´ ë‚®ë”ë¼ë„ ì¼ë‹¨ ë‹µë³€í•˜ë„ë¡ ì„¤ì •í•˜ì˜€ìŠµë‹ˆë‹¤. ë‹µë³€ ì‹ ë¢°ë„ ì ìˆ˜ê°€ ë‚®ì€ ë•Œì—ëŠ” í‹€ë¦° ë‹µë³€ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì•„ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")


