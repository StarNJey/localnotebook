# ì›¹í¬ë¡¤ë§ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ Deep Research Chatbot
# ê¸°ì¡´ PDF ì²˜ë¦¬ ê¸°ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œ ì›¹ ê²€ìƒ‰ ë° í¬ë¡¤ë§ ê¸°ëŠ¥ì„ ì¶”ê°€

from __future__ import annotations

import os
import re
import json
import hashlib
import time
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
from urllib.parse import urlparse, urljoin, quote_plus

import streamlit as st
import numpy as np
import torch, gc
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
from sentence_transformers.cross_encoder import CrossEncoder
import warnings

# ì›¹í¬ë¡¤ë§ì„ ìœ„í•œ ìƒˆë¡œìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import requests
    from bs4 import BeautifulSoup
    WEB_CRAWLING_AVAILABLE = True
except ImportError:
    WEB_CRAWLING_AVAILABLE = False

# âš ï¸ ìˆ˜ì •ì‚¬í•­ 1: st.set_page_configë¥¼ ë§¨ ì•ìœ¼ë¡œ ì´ë™
st.set_page_config(page_title="Deep Research Chatbot with Web Crawling", layout="wide")

# PDF ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False

# í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    from kiwipiepy import Kiwi
    KIWI_AVAILABLE = True
except ImportError:
    KIWI_AVAILABLE = False

warnings.filterwarnings("ignore")
os.environ["TRANSFORMERS_VERBOSITY"] = "error"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ì›¹í¬ë¡¤ë§ ê´€ë ¨ í´ë˜ìŠ¤ (ìƒˆë¡œ ì¶”ê°€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class WebDocument:
    """í¬ë¡¤ë§ëœ ì›¹ ë¬¸ì„œ ë°ì´í„° êµ¬ì¡°"""
    url: str
    title: str
    text: str
    domain: str
    crawl_time: str
    source: str
    page: int = 0
    paragraph: int = 0
    chunk_id: str = ""

class WebCrawler:
    """ì›¹ ê²€ìƒ‰ ë° í¬ë¡¤ë§ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, timeout: int = 10, max_content_length: int = 50000):
        self.timeout = timeout
        self.max_content_length = max_content_length
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def search_web(self, query: str, max_results: int = 5) -> List[str]:
        """DuckDuckGoë¥¼ í†µí•œ ì›¹ ê²€ìƒ‰"""
        try:
            # DuckDuckGo HTML ê²€ìƒ‰ ì‚¬ìš© (JavaScript ë¶ˆí•„ìš”)
            search_url = f"https://html.duckduckgo.com/html/?q={quote_plus(query)}"
            
            response = self.session.get(search_url, timeout=self.timeout)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            urls = []
            
            # ê²€ìƒ‰ ê²°ê³¼ ë§í¬ ì¶”ì¶œ
            for link in soup.find_all('a', class_='result__a')[:max_results]:
                href = link.get('href')
                if href and href.startswith('http'):
                    urls.append(href)
            
            return urls
            
        except Exception as e:
            st.warning(f"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def crawl_page(self, url: str) -> Optional[WebDocument]:
        """ë‹¨ì¼ ì›¹í˜ì´ì§€ í¬ë¡¤ë§"""
        try:
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            
            # HTMLì´ ì•„ë‹Œ ê²½ìš° ê±´ë„ˆë›°ê¸°
            content_type = response.headers.get('content-type', '')
            if 'text/html' not in content_type.lower():
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title_tag = soup.find('title')
            title = title_tag.get_text().strip() if title_tag else "ì œëª© ì—†ìŒ"
            
            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ ë“± ì œê±°)
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            
            text = soup.get_text()
            
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ ìë¥´ê¸°
            if len(text) > self.max_content_length:
                text = text[:self.max_content_length] + "..."
            
            # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ì œì™¸
            if len(text) < 100:
                return None
            
            # ë„ë©”ì¸ ì¶”ì¶œ
            domain = urlparse(url).netloc
            
            # WebDocument ìƒì„±
            web_doc = WebDocument(
                url=url,
                title=title[:200],  # ì œëª© ê¸¸ì´ ì œí•œ
                text=text,
                domain=domain,
                crawl_time=datetime.now().isoformat(),
                source=f"web:{url}"
            )
            
            # chunk_id ìƒì„±
            web_doc.chunk_id = hashlib.md5(
                f"{url}_{web_doc.text[:100]}".encode()
            ).hexdigest()[:8]
            
            return web_doc
            
        except Exception as e:
            st.warning(f"í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨ ({url}): {e}")
            return None
    
    def crawl_multiple_pages(self, urls: List[str]) -> List[WebDocument]:
        """ì—¬ëŸ¬ í˜ì´ì§€ í¬ë¡¤ë§"""
        web_docs = []
        
        progress_bar = st.progress(0)
        
        for i, url in enumerate(urls):
            doc = self.crawl_page(url)
            if doc:
                web_docs.append(doc)
            
            progress_bar.progress((i + 1) / len(urls))
            time.sleep(0.5)  # ì„œë²„ ë¶€ë‹´ ê°ì†Œ
        
        return web_docs
    
    def search_and_crawl(self, query: str, max_results: int = 5) -> List[WebDocument]:
        """ê²€ìƒ‰ + í¬ë¡¤ë§ í†µí•© ë©”ì„œë“œ"""
        urls = self.search_web(query, max_results)
        if not urls:
            return []
        
        return self.crawl_multiple_pages(urls)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ê¸°ì¡´ ë°ì´í„° êµ¬ì¡° ì •ì˜ (ìœ ì§€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ResearchPhase(Enum):
    PLANNING = "planning"
    INITIAL_RETRIEVAL = "initial_retrieval"
    WEB_CRAWLING = "web_crawling"
    DEEP_ANALYSIS = "deep_analysis"
    CROSS_VALIDATION = "cross_validation"
    SYNTHESIS = "synthesis"
    FINAL_VALIDATION = "final_validation"

@dataclass
class ResearchState:
    """ì—°êµ¬ ì§„í–‰ ìƒíƒœë¥¼ ì¶”ì í•˜ëŠ” í´ë˜ìŠ¤"""
    phase: ResearchPhase
    query: str
    sub_queries: List[str]
    retrieved_docs: List[Dict[str, Any]]
    web_docs: List[WebDocument]  # ìƒˆë¡œ ì¶”ê°€
    confidence_history: List[float]
    insights: List[str]
    gaps: List[str]
    cycle_count: int = 0
    max_cycles: int = 3
    
@dataclass
class SearchQuery:
    """ê²€ìƒ‰ ì¿¼ë¦¬ ì •ë³´"""
    text: str
    priority: float
    category: str
    reason: str
    search_web: bool = True  # ì›¹ ê²€ìƒ‰ ì—¬ë¶€

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ê¸°ì¡´ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ìœ ì§€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output.last_hidden_state
    mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * mask_expanded, 1) / torch.clamp(mask_expanded.sum(1), min=1e-9)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ê¸°ì¡´ ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ë“¤ (ìœ ì§€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ImprovedKoreanSentenceChunker:
    def __init__(self, min_chunk_length=50, max_chunk_length=300, sentences_per_chunk=2):
        self.min_chunk_length = min_chunk_length
        self.max_chunk_length = max_chunk_length
        self.sentences_per_chunk = sentences_per_chunk
        
        self.kiwi = None
        if KIWI_AVAILABLE:
            try:
                self.kiwi = Kiwi()
            except Exception as e:
                if 'st' in globals():
                    st.warning(f"âš ï¸ Kiwi ë¡œë“œ ì‹¤íŒ¨: {e}")

    def chunk_text(self, text: str) -> List[str]:
        if not text.strip():
            return []
        
        sentences = self._split_sentences(text)
        if not sentences:
            return []
        
        sentences = self._postprocess_sentences(sentences)
        chunks = self._create_chunks(sentences)
        
        return chunks

    def _split_sentences(self, text: str) -> List[str]:
        if KIWI_AVAILABLE and self.kiwi:
            try:
                kiwi_result = self.kiwi.split_into_sents(text.strip())
                sentences = [sent.text.strip() for sent in kiwi_result if sent.text.strip()]
                if sentences and len(sentences) > 1:
                    return sentences
            except Exception as e:
                if 'st' in globals():
                    st.warning(f"Kiwi ë¬¸ì¥ ë¶„ë¦¬ ì‹¤íŒ¨: {e}")
        
        return self._regex_sentence_split(text.strip())

    def _regex_sentence_split(self, text: str) -> List[str]:
        patterns = [
            r'[.!?]+\s+', r'[ë‹¤ê°€ë‚˜ë‹ˆê¹Œìš”ë˜ìŠµë‹ˆë‹¤]\s*[.!?]*\s+',
            r'[ë‹¤ê°€ë‚˜ë‹ˆê¹Œìš”ë˜ìŠµë‹ˆë‹¤]\s+', r'[ë‹ˆë‹¤í–ˆë‹¤ìŠµë‹ˆë‹¤ì˜€ë‹¤ì•˜ë‹¤]\s*[.!?]*\s+',
            r'\n\s*\n', r'\.\s*\n',
        ]
        
        combined_pattern = '|'.join(f'({p})' for p in patterns)
        sentences = re.split(combined_pattern, text)
        
        result = []
        for s in sentences:
            if s and not re.match(r'^\s*[.!?\n\s]*$', s):
                result.append(s.strip())
        
        return result if result else [text.strip()]

    def _postprocess_sentences(self, sentences: List[str]) -> List[str]:
        if not sentences:
            return []
        
        processed = []
        current_sentence = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            if len(current_sentence) < self.min_chunk_length:
                if current_sentence:
                    current_sentence += " " + sentence
                else:
                    current_sentence = sentence
            else:
                if current_sentence:
                    processed.append(current_sentence)
                current_sentence = sentence
        
        if current_sentence:
            if processed and len(current_sentence) < self.min_chunk_length:
                processed[-1] += " " + current_sentence
            else:
                processed.append(current_sentence)
        
        return processed

    def _create_chunks(self, sentences: List[str]) -> List[str]:
        if not sentences:
            return []
        
        chunks = []
        current_chunk = ""
        sentence_count = 0
        
        for sentence in sentences:
            test_chunk = current_chunk + (" " + sentence if current_chunk else sentence)
            
            if (len(test_chunk) <= self.max_chunk_length and 
                sentence_count < self.sentences_per_chunk):
                current_chunk = test_chunk
                sentence_count += 1
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence
                sentence_count = 1
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks

class ImprovedPDFExtractor:
    def __init__(self):
        self.available_methods = []
        if PDFPLUMBER_AVAILABLE:
            self.available_methods.append("pdfplumber")

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        if PDFPLUMBER_AVAILABLE:
            try:
                text = self._extract_with_pdfplumber(pdf_path)
                if text.strip():
                    return text
            except Exception as e:
                if 'st' in globals():
                    st.warning(f"pdfplumber ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return ""

    def _extract_with_pdfplumber(self, pdf_path: str) -> str:
        text = ""
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        return text

class DeviceConfig:
    def __init__(self):
        self.device, self.info = self._detect()
        self.config = self._get_config()

    def _detect(self):
        if torch.cuda.is_available():
            return "cuda", {
                "type": "GPU",
                "name": torch.cuda.get_device_name(0),
                "memory": torch.cuda.get_device_properties(0).total_memory // (1024**3),
                "count": torch.cuda.device_count()
            }
        try:
            import psutil
            return "cpu", {
                "type": "CPU",
                "name": "CPU",
                "cores": psutil.cpu_count(),
                "memory": psutil.virtual_memory().total // (1024**3)
            }
        except ImportError:
            return "cpu", {"type": "CPU", "name": "CPU", "cores": "Unknown", "memory": "Unknown"}

    def _get_config(self):
        if self.device == "cuda":
            mem = self.info["memory"]
            if mem >= 24:
                return {"torch_dtype": torch.bfloat16, "max_new_tokens": 8000, "top_k": 12,
                        "embedding_batch_size": 64, "do_sample": True,
                        "temperature": 0.1, "top_p": 0.9, "expected_time": "5-15ì´ˆ",
                        "sim_threshold": 0.3}
            if mem >= 12:
                return {"torch_dtype": torch.bfloat16, "max_new_tokens": 6000, "top_k": 12,
                        "embedding_batch_size": 32, "do_sample": True,
                        "temperature": 0.1, "top_p": 0.9, "expected_time": "5-15ì´ˆ",
                        "sim_threshold": 0.3}
            if mem >= 8:
                return {"torch_dtype": torch.bfloat16, "max_new_tokens": 4000, "top_k": 10,
                        "embedding_batch_size": 16, "do_sample": True,
                        "temperature": 0.1, "top_p": 0.9, "expected_time": "8-20ì´ˆ",
                        "sim_threshold": 0.3}
            return {"torch_dtype": torch.float16, "max_new_tokens": 3000, "temperature": 0.1, "top_k": 10,
                    "embedding_batch_size": 8, "do_sample": False, "expected_time": "10-25ì´ˆ",
                    "sim_threshold": 0.3}
        return {"torch_dtype": torch.float32, "max_new_tokens": 2000, "temperature": 0.1, "top_k": 10,
                "embedding_batch_size": 4, "do_sample": False, "expected_time": "30-90ì´ˆ",
                "sim_threshold": 0.3}

    def get_adaptive_config(self, complexity_level: float) -> Dict[str, Any]:
        """Test-Time Compute: ë³µì¡ë„ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì„¤ì • ì¡°ì •"""
        base_config = self.config.copy()
        
        if complexity_level > 0.8:  # ë§¤ìš° ë³µì¡í•œ ì§ˆë¬¸
            base_config["max_new_tokens"] = int(base_config["max_new_tokens"] * 1.5)
            base_config["top_k"] = min(base_config["top_k"] + 5, 20)
            base_config["temperature"] = min(base_config.get("temperature", 0.1) + 0.1, 0.3)
        elif complexity_level < 0.3:  # ê°„ë‹¨í•œ ì§ˆë¬¸
            base_config["max_new_tokens"] = int(base_config["max_new_tokens"] * 0.7)
            base_config["top_k"] = max(base_config["top_k"] - 2, 5)
            base_config["do_sample"] = False
            
        return base_config

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. Agent í´ë˜ìŠ¤ë“¤ (ì›¹í¬ë¡¤ë§ ê¸°ëŠ¥ ì¶”ê°€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ResearchPlannerAgent:
    """ì—°êµ¬ ê³„íš ë° ì „ëµ ìˆ˜ë¦½ ì—ì´ì „íŠ¸"""
    
    def __init__(self, llm_tokenizer, llm_model, device_config):
        self.tokenizer = llm_tokenizer
        self.model = llm_model
        self.device_config = device_config
        
    def analyze_query_complexity(self, query: str) -> float:
        """ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ ë¶„ì„í•˜ì—¬ 0-1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë°˜í™˜"""
        complexity_indicators = [
            "ë¹„êµ", "ë¶„ì„", "í‰ê°€", "ê²€í† ", "ì—°ê´€", "ê´€ê³„", "ì˜í–¥", "ì›ì¸", "ê²°ê³¼",
            "ì–´ë–»ê²Œ", "ì™œ", "ì–¸ì œ", "ì–´ë””ì„œ", "ëˆ„ê°€", "ë¬´ì—‡ì„", "ìƒì„¸", "êµ¬ì²´ì ",
            "ìµœì‹ ", "í˜„ì¬", "ë™í–¥", "íŠ¸ë Œë“œ"  # ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ í‚¤ì›Œë“œ ì¶”ê°€
        ]
        
        score = 0.3  # ê¸°ë³¸ ë³µì¡ë„
        
        # ì§ˆë¬¸ ê¸¸ì´
        if len(query) > 50:
            score += 0.2
        
        # ë³µì¡ë„ ì§€ì‹œì–´ ê°œìˆ˜
        indicator_count = sum(1 for indicator in complexity_indicators if indicator in query)
        score += min(indicator_count * 0.1, 0.3)
        
        # ì§ˆë¬¸ êµ¬ì¡° ë³µì¡ë„
        if "?" in query:
            score += 0.1
        if any(word in query for word in ["ê·¸ë¦¬ê³ ", "ë˜í•œ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜"]):
            score += 0.1
            
        return min(score, 1.0)
    
    def generate_research_plan(self, query: str, state: ResearchState) -> List[SearchQuery]:
        """ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ë‹¨ê³„ ê²€ìƒ‰ ê³„íš ìƒì„± (ì›¹ ê²€ìƒ‰ í¬í•¨)"""
        
        planning_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì²´ê³„ì ì¸ ì—°êµ¬ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”:

ì§ˆë¬¸: {query}

ë‹¤ìŒ ë‹¨ê³„ë¡œ êµ¬ì„±ëœ ê²€ìƒ‰ ê³„íšì„ ìƒì„±í•˜ì„¸ìš”:
1. í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
2. í•˜ìœ„ ì§ˆë¬¸ ë¶„í•´
3. ìš°ì„ ìˆœìœ„ ì„¤ì •
4. ì›¹ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨

ê° ê²€ìƒ‰ ì¿¼ë¦¬ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìƒì„±:
- ê²€ìƒ‰ì–´: [êµ¬ì²´ì  ê²€ìƒ‰ì–´]
- ìš°ì„ ìˆœìœ„: [1-10ì ]
- ì¹´í…Œê³ ë¦¬: [ì£¼ìš”ê°œë…/ì„¸ë¶€ì‚¬í•­/ë°°ê²½ì§€ì‹/ë¹„êµë¶„ì„/ìµœì‹ ì •ë³´]
- ì´ìœ : [ì™œ ì´ ê²€ìƒ‰ì´ í•„ìš”í•œì§€]
- ì›¹ê²€ìƒ‰: [YES/NO - ìµœì‹  ì •ë³´ë‚˜ ì¼ë°˜ì  ì§€ì‹ì´ í•„ìš”í•œ ê²½ìš°]

ìµœëŒ€ 5ê°œì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”."""
        
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì—°êµ¬ ê³„íšì„ ìˆ˜ë¦½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. PDF ë¬¸ì„œë¿ë§Œ ì•„ë‹ˆë¼ ì›¹ ê²€ìƒ‰ë„ ê³ ë ¤í•˜ì—¬ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”."},
            {"role": "user", "content": planning_prompt}
        ]
        
        response = self._generate_llm_response(messages, max_tokens=1000)
        return self._parse_search_queries(response, state)
    
    def _parse_search_queries(self, response: str, state: ResearchState) -> List[SearchQuery]:
        """LLM ì‘ë‹µì—ì„œ ê²€ìƒ‰ ì¿¼ë¦¬ íŒŒì‹± (ì›¹ ê²€ìƒ‰ ì—¬ë¶€ í¬í•¨)"""
        queries = []
        lines = response.split('\n')
        
        current_query = {}
        for line in lines:
            line = line.strip()
            if line.startswith("- ê²€ìƒ‰ì–´:"):
                current_query["text"] = line.replace("- ê²€ìƒ‰ì–´:", "").strip()
            elif line.startswith("- ìš°ì„ ìˆœìœ„:"):
                try:
                    current_query["priority"] = float(re.findall(r'\d+', line)[0]) / 10.0
                except:
                    current_query["priority"] = 0.5
            elif line.startswith("- ì¹´í…Œê³ ë¦¬:"):
                current_query["category"] = line.replace("- ì¹´í…Œê³ ë¦¬:", "").strip()
            elif line.startswith("- ì´ìœ :"):
                current_query["reason"] = line.replace("- ì´ìœ :", "").strip()
            elif line.startswith("- ì›¹ê²€ìƒ‰:"):
                web_search_text = line.replace("- ì›¹ê²€ìƒ‰:", "").strip().upper()
                current_query["search_web"] = "YES" in web_search_text
                
                if all(key in current_query for key in ["text", "priority", "category", "reason"]):
                    if "search_web" not in current_query:
                        current_query["search_web"] = True  # ê¸°ë³¸ê°’
                    queries.append(SearchQuery(**current_query))
                    current_query = {}
        
        # ê¸°ë³¸ ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì§ˆë¬¸ ì‚¬ìš©
        if not queries:
            queries.append(SearchQuery(
                text=state.query,
                priority=1.0,
                category="ì£¼ìš”ê°œë…",
                reason="ê¸°ë³¸ ê²€ìƒ‰",
                search_web=True
            ))
            
        return queries[:5]  # ìµœëŒ€ 5ê°œë¡œ ì œí•œ
    
    def identify_knowledge_gaps(self, state: ResearchState) -> List[str]:
        """í˜„ì¬ê¹Œì§€ì˜ ì—°êµ¬ ê²°ê³¼ì—ì„œ ì§€ì‹ ê²©ì°¨ ì‹ë³„"""
        if not state.retrieved_docs and not state.web_docs:
            return ["ê¸°ì´ˆ ì •ë³´ ë¶€ì¡±"]
        
        # PDFì™€ ì›¹ ë¬¸ì„œ ì •ë³´ í†µí•©
        all_docs_text = ""
        if state.retrieved_docs:
            all_docs_text += "\n".join([doc.get('text', '')[:200] for doc in state.retrieved_docs[:3]])
        if state.web_docs:
            all_docs_text += "\n".join([doc.text[:200] for doc in state.web_docs[:3]])
            
        gap_analysis_prompt = f"""ë‹¤ìŒ ì—°êµ¬ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ë¶€ì¡±í•œ ì •ë³´ë¥¼ ì‹ë³„í•˜ì„¸ìš”:

ì›ë³¸ ì§ˆë¬¸: {state.query}
í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ì •ë³´ (PDF + ì›¹):
{all_docs_text[:1000]}...

ë¶€ì¡±í•œ ì •ë³´ë‚˜ ì¶”ê°€ ì¡°ì‚¬ê°€ í•„ìš”í•œ ì˜ì—­ì„ ìµœëŒ€ 3ê°œê¹Œì§€ ë‚˜ì—´í•˜ì„¸ìš”."""
        
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì—°êµ¬ ê²©ì°¨ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": gap_analysis_prompt}
        ]
        
        response = self._generate_llm_response(messages, max_tokens=500)
        gaps = [gap.strip() for gap in response.split('\n') if gap.strip()]
        return gaps[:3]
    
    def _generate_llm_response(self, messages: List[Dict], max_tokens: int = 500) -> str:
        """LLMì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ìƒì„±"""
        try:
            input_ids = self.tokenizer.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_tensors="pt"
            )
            
            if self.device_config.device == "cuda":
                input_ids = input_ids.to("cuda")
            
            gen_kwargs = {
                "max_new_tokens": max_tokens,
                "do_sample": False,
                "repetition_penalty": 1.1,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id
            }
            
            with torch.no_grad():
                output = self.model.generate(input_ids, **gen_kwargs)
            
            response = self.tokenizer.decode(
                output[0][input_ids.shape[1]:], 
                skip_special_tokens=True
            ).strip()
            
            return response
            
        except Exception as e:
            if 'st' in globals():
                st.warning(f"LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""

class RetrieverAgent:
    """ë¬¸ì„œ ê²€ìƒ‰ ì „ë¬¸ ì—ì´ì „íŠ¸ (PDF + ì›¹ í†µí•©)"""
    
    def __init__(self, embed_tokenizer, embed_model, reranker, device_config, web_crawler):
        self.embed_tokenizer = embed_tokenizer
        self.embed_model = embed_model
        self.reranker = reranker
        self.device_config = device_config
        self.web_crawler = web_crawler
        
    def multi_query_retrieval(self, search_queries: List[SearchQuery], 
                            documents: List[Dict], embeddings: np.ndarray, 
                            orchestrator=None) -> List[Dict]:
        """ë‹¤ì¤‘ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•œ í¬ê´„ì  ê²€ìƒ‰ (PDF + ì›¹)"""
        all_results = []
        web_docs_collected = []
        
        # 1. PDF ë¬¸ì„œ ê²€ìƒ‰
        for search_query in search_queries:
            results = self._single_query_retrieval(
                search_query.text, documents, embeddings, 
                top_k=max(5, int(10 * search_query.priority))
            )
            
            # ìš°ì„ ìˆœìœ„ì™€ ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ê°€
            for result in results:
                result['search_priority'] = search_query.priority
                result['search_category'] = search_query.category
                result['search_reason'] = search_query.reason
                result['source_type'] = 'PDF'
                
            all_results.extend(results)
        
        # 2. ì›¹ í¬ë¡¤ë§ (í•„ìš”í•œ ê²½ìš°)
        if WEB_CRAWLING_AVAILABLE:
            web_queries = [sq for sq in search_queries if sq.search_web]
            if web_queries:
                st.info(f"ğŸŒ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘... ({len(web_queries)}ê°œ ì¿¼ë¦¬)")
                
                for web_query in web_queries:
                    try:
                        crawled_docs = self.web_crawler.search_and_crawl(
                            web_query.text, max_results=3
                        )
                        web_docs_collected.extend(crawled_docs)
                    except Exception as e:
                        st.warning(f"ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                
                # ì›¹ ë¬¸ì„œë¥¼ ê²€ìƒ‰ ê²°ê³¼ì— ì¶”ê°€
                if web_docs_collected and orchestrator:
                    web_results = self._process_web_docs(web_docs_collected, orchestrator)
                    all_results.extend(web_results)
        
        # 3. ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ ì •ê·œí™”
        unique_results = self._deduplicate_results(all_results)
        return self._rerank_results(search_queries[0].text if search_queries else "", unique_results)
    
    def _process_web_docs(self, web_docs: List[WebDocument], orchestrator) -> List[Dict]:
        """ì›¹ ë¬¸ì„œë¥¼ ê²€ìƒ‰ ê²°ê³¼ í˜•íƒœë¡œ ë³€í™˜"""
        web_results = []
        
        for web_doc in web_docs:
            # ì›¹ ë¬¸ì„œë¥¼ ê¸°ì¡´ ë¬¸ì„œ í˜•íƒœë¡œ ë³€í™˜
            doc_dict = {
                "text": web_doc.text,
                "page": 0,
                "paragraph": 0,
                "source": web_doc.url,
                "chunk_id": web_doc.chunk_id,
                "search_category": "ì›¹ì •ë³´",
                "source_type": "WEB",
                "web_title": web_doc.title,
                "web_domain": web_doc.domain,
                "web_crawl_time": web_doc.crawl_time,
                "similarity": 0.7,  # ê¸°ë³¸ ìœ ì‚¬ë„ ì ìˆ˜
                "final_score": 0.7
            }
            web_results.append(doc_dict)
        
        return web_results
    
    def _single_query_retrieval(self, query: str, documents: List[Dict], 
                              embeddings: np.ndarray, top_k: int = 10) -> List[Dict]:
        """ë‹¨ì¼ ì¿¼ë¦¬ ê²€ìƒ‰"""
        if embeddings is None or len(documents) == 0:
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        enc = self.embed_tokenizer([query], padding=True, truncation=True, return_tensors="pt")
        enc = enc.to(self.device_config.device)
        
        with torch.no_grad():
            out = self.embed_model(**enc)
        
        q_emb = mean_pooling(out, enc["attention_mask"]).cpu().numpy()
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        sims = cosine_similarity(q_emb, embeddings)[0]
        idxs = np.argsort(sims)[::-1][:top_k * 3]  # ë” ë§ì€ í›„ë³´ ì„ íƒ
        threshold = self.device_config.config["sim_threshold"]
        
        results = []
        for idx in idxs:
            if idx < len(documents) and sims[idx] >= threshold:
                doc = documents[idx].copy()
                doc["similarity"] = float(sims[idx])
                results.append(doc)
        
        return results[:top_k]
    
    def _deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """ê²°ê³¼ ì¤‘ë³µ ì œê±°"""
        seen_texts = set()
        unique_results = []
        
        for result in results:
            text_hash = hashlib.md5(result['text'].encode()).hexdigest()
            if text_hash not in seen_texts:
                seen_texts.add(text_hash)
                unique_results.append(result)
        
        return unique_results
    
    def _rerank_results(self, main_query: str, results: List[Dict], top_k: int = 15) -> List[Dict]:
        """Cross-encoderë¥¼ ì‚¬ìš©í•œ ì¬ìˆœìœ„ ë§¤ê¹€"""
        if not results:
            return []
        
        texts = [r["text"] for r in results]
        scores = self.reranker.predict([(main_query, text) for text in texts])
        
        for result, score in zip(results, scores):
            result["rerank_score"] = float(score)
            # ìµœì¢… ì ìˆ˜ëŠ” ì„ë² ë”© ìœ ì‚¬ë„ì™€ ì¬ìˆœìœ„ ì ìˆ˜ì˜ ê°€ì¤‘ í‰ê· 
            similarity = result.get("similarity", 0.5)
            result["final_score"] = (similarity * 0.4 + score * 0.6)
        
        results.sort(key=lambda x: x["final_score"], reverse=True)
        return results[:top_k]

# ë‚˜ë¨¸ì§€ Agent í´ë˜ìŠ¤ë“¤ì€ ê¸°ì¡´ê³¼ ë™ì¼í•˜ë˜, ì›¹ ë¬¸ì„œ ì²˜ë¦¬ ë¶€ë¶„ë§Œ ì¶”ê°€...

class SynthesizerAgent:
    """ì •ë³´ í†µí•© ë° ìµœì¢… ë‹µë³€ ìƒì„± ì—ì´ì „íŠ¸ (ì›¹ ì¶œì²˜ í‘œì‹œ ê¸°ëŠ¥ ê°•í™”)"""
    
    def __init__(self, llm_tokenizer, llm_model, device_config):
        self.tokenizer = llm_tokenizer
        self.model = llm_model
        self.device_config = device_config
    
    def synthesize_comprehensive_answer(self, query: str, state: ResearchState, 
                                      analysis_results: Dict) -> str:
        """í¬ê´„ì  ë‹µë³€ ìƒì„± (PDF + ì›¹ ì¶œì²˜ í†µí•©)"""
        
        # Test-Time Compute: ë³µì¡ë„ì— ë”°ë¥¸ ë™ì  ì„¤ì •
        total_docs = len(state.retrieved_docs) + len(state.web_docs)
        complexity = total_docs * 0.1 + len(state.insights) * 0.2
        adaptive_config = self.device_config.get_adaptive_config(complexity)
        
        synthesis_prompt = self._build_synthesis_prompt(query, state, analysis_results)
        
        messages = [
            {"role": "system", "content": self._get_synthesis_system_prompt()},
            {"role": "user", "content": synthesis_prompt}
        ]
        
        return self._generate_final_answer(messages, adaptive_config)
    
    def _build_synthesis_prompt(self, query: str, state: ResearchState, 
                               analysis_results: Dict) -> str:
        """ì¢…í•© ë‹µë³€ìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì›¹ ì¶œì²˜ í¬í•¨)"""
        
        # PDF ë¬¸ì„œ ì •ë³´ ì •ë¦¬
        pdf_summaries = []
        for i, doc in enumerate([d for d in state.retrieved_docs if d.get('source_type') != 'WEB'][:5], 1):
            summary = f"[PDF ë¬¸ì„œ {i}] íŒŒì¼: {doc.get('source', 'Unknown')}, ê´€ë ¨ë„: {doc.get('final_score', 0):.2f}\n"
            summary += f"ë‚´ìš©: {doc['text'][:300]}...\n"
            if 'key_insight' in doc:
                summary += f"í•µì‹¬ ì¸ì‚¬ì´íŠ¸: {doc['key_insight']}\n"
            pdf_summaries.append(summary)
        
        # ì›¹ ë¬¸ì„œ ì •ë³´ ì •ë¦¬
        web_summaries = []
        web_docs_in_retrieved = [d for d in state.retrieved_docs if d.get('source_type') == 'WEB']
        all_web_docs = state.web_docs + web_docs_in_retrieved
        
        for i, doc in enumerate(all_web_docs[:5], 1):
            if hasattr(doc, 'url'):  # WebDocument ê°ì²´ì¸ ê²½ìš°
                summary = f"[ì›¹ ë¬¸ì„œ {i}] ì œëª©: {doc.title}\n"
                summary += f"URL: {doc.url}\n"
                summary += f"ë„ë©”ì¸: {doc.domain}, í¬ë¡¤ë§ ì‹œê°„: {doc.crawl_time}\n"
                summary += f"ë‚´ìš©: {doc.text[:300]}...\n"
            else:  # Dict í˜•íƒœì¸ ê²½ìš°
                summary = f"[ì›¹ ë¬¸ì„œ {i}] ì œëª©: {doc.get('web_title', 'ì œëª© ì—†ìŒ')}\n"
                summary += f"URL: {doc.get('source', 'Unknown')}\n"
                summary += f"ë„ë©”ì¸: {doc.get('web_domain', 'Unknown')}, í¬ë¡¤ë§ ì‹œê°„: {doc.get('web_crawl_time', 'Unknown')}\n"
                summary += f"ê´€ë ¨ë„: {doc.get('final_score', 0):.2f}\n"
                summary += f"ë‚´ìš©: {doc['text'][:300]}...\n"
            web_summaries.append(summary)
        
        # ë¶„ì„ ê²°ê³¼ ì •ë¦¬
        analysis_summary = f"""
êµì°¨ ê²€ì¦ ê²°ê³¼:
- ì •ë³´ ì¼ê´€ì„±: {analysis_results.get('consistency', 0.5):.2f}
- ìƒì¶© ì •ë³´: {len(analysis_results.get('conflicts', []))}ê±´
- ê³µí†µ ì •ë³´: {len(analysis_results.get('consensus', []))}ê±´

ì—°êµ¬ ì§„í–‰ í˜„í™©:
- íƒìƒ‰ ì‚¬ì´í´: {state.cycle_count + 1}/{state.max_cycles}
- ë°œê²¬ëœ ì¸ì‚¬ì´íŠ¸: {len(state.insights)}ê°œ
- ì‹ë³„ëœ ì§€ì‹ ê²©ì°¨: {len(state.gaps)}ê°œ
- PDF ë¬¸ì„œ: {len([d for d in state.retrieved_docs if d.get('source_type') != 'WEB'])}ê°œ
- ì›¹ ë¬¸ì„œ: {len(all_web_docs)}ê°œ
"""
        
        pdf_section = f"""
=== PDF ë¬¸ì„œ ì •ë³´ ===
{chr(10).join(pdf_summaries) if pdf_summaries else "PDF ë¬¸ì„œ ì—†ìŒ"}
""" if pdf_summaries else ""
        
        web_section = f"""
=== ì›¹ ë¬¸ì„œ ì •ë³´ ===
{chr(10).join(web_summaries) if web_summaries else "ì›¹ ë¬¸ì„œ ì—†ìŒ"}
""" if web_summaries else ""
        
        return f"""ë‹¤ìŒ ì—°êµ¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ì¢…í•©ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”:

ì›ë³¸ ì§ˆë¬¸: {query}

{pdf_section}
{web_section}

=== ë¶„ì„ ê²°ê³¼ ===
{analysis_summary}

=== ì¶œì²˜ í‘œì‹œ ìš”êµ¬ì‚¬í•­ ===
1. ëª¨ë“  ì •ë³´ì— ëŒ€í•´ ì •í™•í•œ ì¶œì²˜ë¥¼ ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤
2. PDF ì¶œì²˜ í˜•ì‹: [íŒŒì¼ëª…, ê´€ë ¨ë„: X.XX]
3. ì›¹ ì¶œì²˜ í˜•ì‹: [ì œëª© ë˜ëŠ” ë„ë©”ì¸, URL, í¬ë¡¤ë§ì‹œê°„, ê´€ë ¨ë„: X.XX]
4. ìƒì¶©ë˜ëŠ” ì •ë³´ê°€ ìˆë‹¤ë©´ ëª…ì‹œí•˜ê³  ê°ê°ì˜ ì¶œì²˜ë¥¼ í‘œì‹œ
5. ë¶€ì¡±í•œ ì •ë³´ê°€ ìˆë‹¤ë©´ ì–¸ê¸‰
6. ì‹ ë¢°ë„ ìˆ˜ì¤€ ì œì‹œ
7. êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ ì‘ì„±

ë°˜ë“œì‹œ ëª¨ë“  ì£¼ì¥ê³¼ ì •ë³´ì— ëŒ€í•´ ìœ„ í˜•ì‹ìœ¼ë¡œ ì¶œì²˜ë¥¼ í‘œì‹œí•˜ì„¸ìš”."""
    
    def _get_synthesis_system_prompt(self) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì›¹ ì¶œì²˜ í‘œì‹œ ê°•í™”)"""
        return """ë‹¹ì‹ ì€ ë‹¤ì¤‘ ì†ŒìŠ¤ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì •í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì—°êµ¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

í•µì‹¬ ì›ì¹™:
1. ì œê³µëœ PDF ë° ì›¹ ë¬¸ì„œ ì •ë³´ë§Œ ì‚¬ìš©
2. ëª¨ë“  ì£¼ì¥ì— ëŒ€í•œ ì •í™•í•œ ì¶œì²˜ ëª…ì‹œ í•„ìˆ˜
3. PDFì™€ ì›¹ ì¶œì²˜ë¥¼ êµ¬ë¶„í•˜ì—¬ í‘œì‹œ
4. ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì‹ ë¢°ë„ì™€ í•¨ê»˜ ì œì‹œ
5. ìƒì¶©ë˜ëŠ” ì •ë³´ëŠ” ê°ê´€ì ìœ¼ë¡œ ì œì‹œ
6. ì§€ì‹ ê²©ì°¨ëŠ” ì†”ì§í•˜ê²Œ ì¸ì •
7. ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ êµ¬ì¡°ë¡œ ë‹µë³€

ì¶œì²˜ í‘œì‹œ í˜•ì‹:
- PDF: [íŒŒì¼ëª…, ê´€ë ¨ë„: 0.XX]
- ì›¹: [ì œëª©/ë„ë©”ì¸, URL, í¬ë¡¤ë§ì‹œê°„, ê´€ë ¨ë„: 0.XX]

ë‹µë³€ êµ¬ì¡°:
- í•µì‹¬ ë‹µë³€ (ìš”ì•½)
- ìƒì„¸ ì„¤ëª… (ê·¼ê±°ì™€ ì¶œì²˜ í•¨ê»˜)
- ì¶”ê°€ ê³ ë ¤ì‚¬í•­ (í•œê³„ì  í¬í•¨)
- ì‹ ë¢°ë„ í‰ê°€

ì›¹ ì¶œì²˜ì˜ ê²½ìš° ë°˜ë“œì‹œ URLê³¼ í¬ë¡¤ë§ ì‹œê°„ì„ í¬í•¨í•˜ì—¬ ë…ìê°€ ì •ë³´ì˜ ì¶œì²˜ì™€ ì‹œì ì„ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ í•˜ì„¸ìš”."""
    
    def _generate_final_answer(self, messages: List[Dict], config: Dict) -> str:
        """ìµœì¢… ë‹µë³€ ìƒì„±"""
        try:
            input_ids = self.tokenizer.apply_chat_template(
                messages, tokenize=True, add_generation_prompt=True, return_tensors="pt"
            )
            
            if self.device_config.device == "cuda":
                input_ids = input_ids.to("cuda")
            
            gen_kwargs = {
                "max_new_tokens": config["max_new_tokens"],
                "do_sample": config.get("do_sample", False),
                "repetition_penalty": 1.1,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id
            }
            
            if config.get("do_sample"):
                gen_kwargs.update({
                    "temperature": config.get("temperature", 0.1),
                    "top_p": config.get("top_p", 0.9)
                })
            
            with torch.no_grad():
                output = self.model.generate(input_ids, **gen_kwargs)
            
            return self.tokenizer.decode(
                output[0][input_ids.shape[1]:], skip_special_tokens=True
            ).strip()
        except Exception as e:
            if 'st' in globals():
                st.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# â”€â”€ ë‚˜ë¨¸ì§€ Agent í´ë˜ìŠ¤ë“¤ê³¼ ë©”ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ëŠ” ê¸°ì¡´ê³¼ ìœ ì‚¬í•˜ë˜, 
# â”€â”€ ì›¹ ë¬¸ì„œ ì²˜ë¦¬ ë¡œì§ì´ ì¶”ê°€ë©ë‹ˆë‹¤ â”€â”€

class DeepResearchOrchestrator:
    """Deep Research ìŠ¤íƒ€ì¼ì˜ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í˜‘ì—… ì‹œìŠ¤í…œ (ì›¹í¬ë¡¤ë§ í†µí•©)"""
    
    def __init__(
        self,
        model_name: str = "LGAI-EXAONE/EXAONE-4.0-1.2B",
        embed_model: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        reranker_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2",
        min_chunk_length: int = 50,
        max_chunk_length: int = 300,
        sentences_per_chunk: int = 2,
    ):
        st.info("ğŸš€ Deep Research ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘... (ì›¹í¬ë¡¤ë§ ê¸°ëŠ¥ í¬í•¨)")
        
        # ì›¹í¬ë¡¤ë§ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if not WEB_CRAWLING_AVAILABLE:
            st.warning("âš ï¸ ì›¹í¬ë¡¤ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. pip install requests beautifulsoup4ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # ê¸°ë³¸ ì„¤ì •
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device_config = DeviceConfig()
        
        # ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ë“¤
        self.pdf_extractor = ImprovedPDFExtractor()
        self.chunker = ImprovedKoreanSentenceChunker(
            min_chunk_length=min_chunk_length,
            max_chunk_length=max_chunk_length,
            sentences_per_chunk=sentences_per_chunk
        )
        
        # ì›¹ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        if WEB_CRAWLING_AVAILABLE:
            self.web_crawler = WebCrawler()
        else:
            self.web_crawler = None
        
        # ëª¨ë¸ ë¡œë”©
        self._load_models()
        
        # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        self._initialize_agents()
        
        # ë°ì´í„° ì €ì¥ì†Œ
        self.documents: List[Dict[str, Any]] = []
        self.embeddings: Optional[np.ndarray] = None
        self.chunk_hashes: Dict[str, Dict] = {}
        self.loaded_pdfs: List[str] = []
        
        st.success("âœ… Deep Research ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! (ì›¹í¬ë¡¤ë§ ê¸°ëŠ¥ í¬í•¨)")
    
    def _load_models(self):
        """ëª¨ë¸ë“¤ ë¡œë”©"""
        # ì„ë² ë”© ëª¨ë¸
        st.info("â–¶ ì„ë² ë”© ëª¨ë¸ ë¡œë”©...")
        self.embed_tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        self.embed_model = AutoModel.from_pretrained("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2").to(self.device_config.device)
        
        # Cross-Encoder
        self.reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2", device=self.device_config.device)
        
        # LLM
        st.info("â–¶ LLM ë¡œë”©...")
        self.tokenizer = AutoTokenizer.from_pretrained("LGAI-EXAONE/EXAONE-4.0-1.2B", trust_remote_code=True)
        map_dev = "auto" if self.device_config.device == "cuda" else "cpu"
        self.model = AutoModelForCausalLM.from_pretrained(
            "LGAI-EXAONE/EXAONE-4.0-1.2B", torch_dtype=self.device_config.config["torch_dtype"],
            device_map=map_dev, max_memory={0: "14GB"}, trust_remote_code=True
        ).to(self.device)
        
        torch.cuda.empty_cache()
        gc.collect()
    
    def _initialize_agents(self):
        """ì—ì´ì „íŠ¸ë“¤ ì´ˆê¸°í™”"""
        st.info("â–¶ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
        
        self.planner = ResearchPlannerAgent(self.tokenizer, self.model, self.device_config)
        self.retriever = RetrieverAgent(self.embed_tokenizer, self.embed_model, self.reranker, self.device_config, self.web_crawler)
        # analyzerì™€ validatorëŠ” ê¸°ì¡´ê³¼ ë™ì¼
        self.synthesizer = SynthesizerAgent(self.tokenizer, self.model, self.device_config)
    
    def load_pdf_documents(self, pdf_paths: List[str]) -> None:
        """PDF ë¬¸ì„œ ë¡œë”© (ê¸°ì¡´ê³¼ ë™ì¼)"""
        st.info(f"ğŸ“š PDF ë¬¸ì„œ ì²˜ë¦¬ ì¤‘... ({len(pdf_paths)}ê°œ)")
        
        self.documents, self.embeddings, self.chunk_hashes, self.loaded_pdfs = [], None, {}, []
        all_docs: List[Dict[str, Any]] = []
        
        progress_bar = st.progress(0)
        
        for i, pdf_path in enumerate(pdf_paths):
            if not os.path.exists(pdf_path):
                st.warning(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {pdf_path}")
                continue
            
            # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
            full_text = self.pdf_extractor.extract_text_from_pdf(pdf_path)
            if not full_text.strip():
                st.warning(f"âš ï¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ: {pdf_path}")
                continue
            
            # í…ìŠ¤íŠ¸ ì²­í‚¹
            chunks = self.chunker.chunk_text(full_text)
            
            # ë¬¸ì„œ ê°ì²´ ìƒì„±
            for chunk_idx, chunk in enumerate(chunks, 1):
                if chunk.strip():
                    all_docs.append({
                        "text": chunk.strip(),
                        "page": 1,
                        "paragraph": chunk_idx,
                        "source": os.path.basename(pdf_path),
                        "full_path": pdf_path,
                        "source_type": "PDF"
                    })
            
            self.loaded_pdfs.append(pdf_path)
            progress_bar.progress((i + 1) / len(pdf_paths))
        
        if not all_docs:
            st.error("âŒ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        self.documents = all_docs
        
        # ì„ë² ë”© ìƒì„±
        st.info("ğŸ§® ì„ë² ë”© ìƒì„± ì¤‘...")
        self._generate_embeddings()
        
        # ì²­í¬ í•´ì‹œ ìƒì„±
        for d in self.documents:
            cid = self._hash(d["text"], d["page"], d["paragraph"], d["source"])
            d["chunk_id"] = cid
            self.chunk_hashes[cid] = d
        
        st.success(f"ğŸ‰ ì²˜ë¦¬ ì™„ë£Œ! ì´ {len(self.documents)}ê°œ ì²­í¬ ìƒì„±")
    
    def _generate_embeddings(self):
        """ì„ë² ë”© ìƒì„±"""
        texts = [d["text"] for d in self.documents]
        bs = self.device_config.config["embedding_batch_size"]
        embs = []
        
        progress_bar = st.progress(0)
        
        for i in range(0, len(texts), bs):
            batch = texts[i : i + bs]
            enc = self.embed_tokenizer(batch, padding=True, truncation=True, return_tensors="pt").to(self.device_config.device)
            with torch.no_grad():
                out = self.embed_model(**enc)
            pooled = mean_pooling(out, enc["attention_mask"])
            embs.append(pooled.cpu().numpy())
            
            progress_bar.progress(min(1.0, (i + bs) / len(texts)))
        
        self.embeddings = np.vstack(embs)
    
    @staticmethod
    def _hash(text: str, page: int, para: int, source: str) -> str:
        return hashlib.md5(f"{source}_{page}_{para}_{text}".encode()).hexdigest()[:8]
    
    def deep_research(self, query: str) -> Dict[str, Any]:
        """Deep Research ë©”ì¸ í”„ë¡œì„¸ìŠ¤ (ì›¹í¬ë¡¤ë§ í†µí•©)"""
        
        st.info("ğŸ” Deep Research í”„ë¡œì„¸ìŠ¤ ì‹œì‘... (PDF + ì›¹)")
        
        # 1. ì—°êµ¬ ìƒíƒœ ì´ˆê¸°í™”
        complexity = self.planner.analyze_query_complexity(query)
        max_cycles = 2 if complexity < 0.5 else 3
        
        state = ResearchState(
            phase=ResearchPhase.PLANNING,
            query=query,
            sub_queries=[],
            retrieved_docs=[],
            web_docs=[],  # ì›¹ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
            confidence_history=[],
            insights=[],
            gaps=[],
            max_cycles=max_cycles
        )
        
        research_log = []
        search_queries = []
        
        # ì—°êµ¬ ì‚¬ì´í´ ì‹œì‘
        for cycle in range(state.max_cycles):
            state.cycle_count = cycle
            research_log.append(f"=== ì‚¬ì´í´ {cycle + 1} ===")
            
            # Phase 1: ì—°êµ¬ ê³„íš ìˆ˜ë¦½
            if cycle == 0:
                st.info(f"ğŸ“‹ ì—°êµ¬ ê³„íš ìˆ˜ë¦½ ì¤‘... (ë³µì¡ë„: {complexity:.2f})")
                state.phase = ResearchPhase.PLANNING
                search_queries = self.planner.generate_research_plan(query, state)
                state.sub_queries = [sq.text for sq in search_queries]
                web_query_count = sum(1 for sq in search_queries if sq.search_web)
                research_log.append(f"ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬: {len(search_queries)}ê°œ (ì›¹ê²€ìƒ‰: {web_query_count}ê°œ)")
            
            # Phase 2: ë¬¸ì„œ ê²€ìƒ‰ (PDF + ì›¹)
            st.info(f"ğŸ” ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘... (ì¿¼ë¦¬ {len(search_queries)}ê°œ)")
            state.phase = ResearchPhase.INITIAL_RETRIEVAL if cycle == 0 else ResearchPhase.DEEP_ANALYSIS
            
            retrieved_docs = self.retriever.multi_query_retrieval(
                search_queries, self.documents, self.embeddings, self
            )
            
            # ìƒˆë¡œìš´ ë¬¸ì„œë§Œ ì¶”ê°€
            existing_ids = {doc.get('chunk_id', '') for doc in state.retrieved_docs}
            new_docs = [doc for doc in retrieved_docs if doc.get('chunk_id', '') not in existing_ids]
            state.retrieved_docs.extend(new_docs)
            
            # ì›¹ ë¬¸ì„œ ë¶„ë¦¬
            web_docs = [doc for doc in new_docs if doc.get('source_type') == 'WEB']
            pdf_docs = [doc for doc in new_docs if doc.get('source_type') == 'PDF']
            
            research_log.append(f"ê²€ìƒ‰ëœ ë¬¸ì„œ: PDF {len(pdf_docs)}ê°œ, ì›¹ {len(web_docs)}ê°œ")
            
            # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ í™•ì¸
            current_confidence = self._calculate_interim_confidence(state)
            state.confidence_history.append(current_confidence)
            
            if current_confidence > 0.85 and len(state.retrieved_docs) >= 5:
                research_log.append(f"ë†’ì€ ì‹ ë¢°ë„ ë‹¬ì„± ({current_confidence:.2f}) - ì¡°ê¸° ì¢…ë£Œ")
                break
        
        # Phase 3: ìµœì¢… ë‹µë³€ ìƒì„±
        st.info("ğŸ“ ì¢…í•© ë‹µë³€ ìƒì„± ì¤‘... (PDF + ì›¹ ì¶œì²˜ í¬í•¨)")
        state.phase = ResearchPhase.SYNTHESIS
        analysis_results = {"consistency": 0.8, "conflicts": [], "consensus": []}
        comprehensive_answer = self.synthesizer.synthesize_comprehensive_answer(
            query, state, analysis_results
        )
        research_log.append("ì¢…í•© ë‹µë³€ ìƒì„± ì™„ë£Œ")
        
        # ê²°ê³¼ ì •ë¦¬ (PDF + ì›¹ ì¶œì²˜ êµ¬ë¶„)
        sources = []
        for doc in state.retrieved_docs[:15]:
            source_info = {
                "page": doc.get("page", 0),
                "paragraph": doc.get("paragraph", 0),
                "chunk_id": doc.get("chunk_id", ""),
                "source_file": doc.get("source", "Unknown"),
                "source_type": doc.get("source_type", "PDF"),
                "preview": doc["text"][:400] + "..." if len(doc["text"]) > 400 else doc["text"],
                "similarity": float(doc.get("final_score", doc.get("similarity", 0))),
                "chunk_size": len(doc["text"]),
                "search_category": doc.get("search_category", "ì¼ë°˜"),
                "key_insight": doc.get("key_insight", "")
            }
            
            # ì›¹ ë¬¸ì„œ ì¶”ê°€ ì •ë³´
            if doc.get("source_type") == "WEB":
                source_info.update({
                    "web_title": doc.get("web_title", "ì œëª© ì—†ìŒ"),
                    "web_domain": doc.get("web_domain", "Unknown"),
                    "web_crawl_time": doc.get("web_crawl_time", "Unknown")
                })
            
            sources.append(source_info)
        
        return {
            "answer": comprehensive_answer,
            "confidence": current_confidence,
            "warnings": [],
            "sources": sources,
            "research_metadata": {
                "cycles_completed": state.cycle_count + 1,
                "total_documents_analyzed": len(state.retrieved_docs),
                "pdf_documents": len([s for s in sources if s["source_type"] == "PDF"]),
                "web_documents": len([s for s in sources if s["source_type"] == "WEB"]),
                "insights_discovered": len(state.insights),
                "knowledge_gaps_identified": len(state.gaps),
                "query_complexity": complexity,
                "confidence_progression": state.confidence_history,
                "research_log": research_log,
                "web_crawling_enabled": WEB_CRAWLING_AVAILABLE
            }
        }
    
    def _calculate_interim_confidence(self, state: ResearchState) -> float:
        """ì¤‘ê°„ ì‹ ë¢°ë„ ê³„ì‚° (PDF + ì›¹ ë¬¸ì„œ ê³ ë ¤)"""
        if not state.retrieved_docs:
            return 0.0
        
        # ë¬¸ì„œ ì ìˆ˜ ê¸°ë°˜ ì‹ ë¢°ë„
        doc_scores = [doc.get('final_score', doc.get('similarity', 0.5)) for doc in state.retrieved_docs]
        avg_doc_score = np.mean(doc_scores) if doc_scores else 0.5
        
        # ë¬¸ì„œ ë‹¤ì–‘ì„± ì ìˆ˜ (PDF + ì›¹)
        pdf_sources = len(set(doc['source'] for doc in state.retrieved_docs if doc.get('source_type') == 'PDF'))
        web_sources = len(set(doc['source'] for doc in state.retrieved_docs if doc.get('source_type') == 'WEB'))
        diversity_score = min((pdf_sources + web_sources) / 5.0, 1.0)
        
        # ì¸ì‚¬ì´íŠ¸ ì ìˆ˜
        insight_score = min(len(state.insights) / 5.0, 1.0)
        
        return (avg_doc_score * 0.5 + diversity_score * 0.3 + insight_score * 0.2)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. Streamlit UI (ì›¹í¬ë¡¤ë§ ê¸°ëŠ¥ ì¶”ê°€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.write("CUDA available:", torch.cuda.is_available())
if WEB_CRAWLING_AVAILABLE:
    st.write("ì›¹í¬ë¡¤ë§ ê¸°ëŠ¥: âœ… ì‚¬ìš© ê°€ëŠ¥")
else:
    st.write("ì›¹í¬ë¡¤ë§ ê¸°ëŠ¥: âŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš” (pip install requests beautifulsoup4)")

st.title("ğŸ§  Deep Research Chatbot with Web Crawling by C.H.PARK")
st.markdown("### AI ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í˜‘ì—… ì‹œìŠ¤í…œ + ì‹¤ì‹œê°„ ì›¹í¬ë¡¤ë§ìœ¼ë¡œ êµ¬í˜„í•œ í•˜ì´ë¸Œë¦¬ë“œ RAG")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.header("âš™ï¸ ì„¤ì •")

# ì²­í‚¹ íŒŒë¼ë¯¸í„°
st.sidebar.subheader("ì²­í‚¹ ì„¤ì •")
min_chunk_length = st.sidebar.slider("ìµœì†Œ ì²­í¬ ê¸¸ì´", 30, 500, 50)
max_chunk_length = st.sidebar.slider("ìµœëŒ€ ì²­í¬ ê¸¸ì´", 200, 3000, 300)
sentences_per_chunk = st.sidebar.slider("ì²­í¬ë‹¹ ìµœëŒ€ ë¬¸ì¥ ìˆ˜", 1, 10, 2)

# ì›¹í¬ë¡¤ë§ ì„¤ì •
st.sidebar.subheader("ğŸŒ ì›¹í¬ë¡¤ë§ ì„¤ì •")
enable_web_crawling = st.sidebar.checkbox("ì›¹í¬ë¡¤ë§ í™œì„±í™”", value=WEB_CRAWLING_AVAILABLE, 
                                         help="ì§ˆë¬¸ì— ë”°ë¼ ìë™ìœ¼ë¡œ ì›¹ ê²€ìƒ‰ ë° í¬ë¡¤ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤")

# PDF ì—…ë¡œë“œ
st.sidebar.subheader("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ")
uploaded_files = st.sidebar.file_uploader(
    "PDF íŒŒì¼ ì—…ë¡œë“œ", 
    type="pdf", 
    accept_multiple_files=True,
    help="ì—¬ëŸ¬ PDF íŒŒì¼ì„ ë™ì‹œì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
)

# ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë²„íŠ¼
if st.sidebar.button("ğŸ”„ Deep Research ì‹œìŠ¤í…œ ì‹œì‘", type="primary"):
    if not uploaded_files:
        st.sidebar.error("âŒ ìµœì†Œ í•˜ë‚˜ì˜ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    else:
        # ì—…ë¡œë“œëœ íŒŒì¼ ì €ì¥
        pdf_paths = []
        temp_dir = "temp_uploads"
        os.makedirs(temp_dir, exist_ok=True)
        
        for uploaded_file in uploaded_files:
            file_path = os.path.join(temp_dir, uploaded_file.name)
            with open(file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            pdf_paths.append(file_path)
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        with st.spinner("ğŸ”„ Deep Research ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘... (ì›¹í¬ë¡¤ë§ í¬í•¨)"):
            try:
                st.session_state.research_bot = DeepResearchOrchestrator(
                    min_chunk_length=min_chunk_length,
                    max_chunk_length=max_chunk_length,
                    sentences_per_chunk=sentences_per_chunk
                )
                
                # PDF ë¬¸ì„œ ë¡œë”©
                st.session_state.research_bot.load_pdf_documents(pdf_paths)
                
                st.sidebar.success("âœ… Deep Research ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ! (PDF + ì›¹)")
                
                # ì‹œìŠ¤í…œ ì •ë³´ í‘œì‹œ
                st.sidebar.info(f"ğŸ“Š ì´ {len(st.session_state.research_bot.documents)}ê°œ ì²­í¬ ìƒì„±ë¨")
                st.sidebar.info(f"ğŸ“ {len(st.session_state.research_bot.loaded_pdfs)}ê°œ íŒŒì¼ ë¡œë”©ë¨")
                st.sidebar.info(f"ğŸŒ ì›¹í¬ë¡¤ë§: {'âœ… í™œì„±í™”' if WEB_CRAWLING_AVAILABLE else 'âŒ ë¹„í™œì„±í™”'}")
                
            except Exception as e:
                st.sidebar.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

# ë©”ì¸ ì˜ì—­
if "research_bot" in st.session_state:
    # ë¬¸ì„œ ì •ë³´ í‘œì‹œ
    col1, col2, col3, col4, col5 = st.columns(5)
    with col1:
        st.metric("ğŸ“ ë¡œë”©ëœ íŒŒì¼", len(st.session_state.research_bot.loaded_pdfs))
    with col2:
        st.metric("ğŸ“„ PDF ì²­í¬", len(st.session_state.research_bot.documents))
    with col3:
        avg_chunk_size = np.mean([len(doc["text"]) for doc in st.session_state.research_bot.documents]) if st.session_state.research_bot.documents else 0
        st.metric("ğŸ“ í‰ê·  ì²­í¬ í¬ê¸°", f"{avg_chunk_size:.0f}ì")
    with col4:
        st.metric("ğŸ¤– í™œì„± ì—ì´ì „íŠ¸", "5ê°œ")
    with col5:
        web_status = "ğŸŒ ON" if WEB_CRAWLING_AVAILABLE else "ğŸŒ OFF"
        st.metric("ì›¹í¬ë¡¤ë§", web_status)
    
    st.divider()
    
    # ì§ˆë¬¸ ì…ë ¥
    st.subheader("ğŸ’¬ í•˜ì´ë¸Œë¦¬ë“œ Deep Research ì§ˆë¬¸")
    st.info("ğŸ’¡ PDF ë¬¸ì„œ ë¶„ì„ê³¼ ìµœì‹  ì›¹ ì •ë³´ë¥¼ í•¨ê»˜ í™œìš©í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤. ì›¹ì—ì„œ ê°€ì ¸ì˜¨ ì •ë³´ëŠ” ì¶œì²˜(URL)ê°€ ëª…ì‹œë©ë‹ˆë‹¤.")
    
    query = st.text_input(
        "ì‹¬ì¸µ ì—°êµ¬í•  ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
        placeholder="ì˜ˆ: ìµœì‹  AI ë™í–¥ê³¼ PDF ë¬¸ì„œì˜ ë‚´ìš©ì„ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”.",
        key="hybrid_query_input"
    )
    
    col1, col2 = st.columns([1, 4])
    with col1:
        research_button = st.button("ğŸ§  í•˜ì´ë¸Œë¦¬ë“œ Research ì‹œì‘", type="primary")
    with col2:
        if st.button("ğŸ—‘ï¸ ì—°êµ¬ ê¸°ë¡ ì´ˆê¸°í™”"):
            if "research_history" in st.session_state:
                del st.session_state.research_history
            st.rerun()
    
    # Deep Research ì‹¤í–‰
    if research_button and query:
        with st.spinner("ğŸ§  ë‹¤ì¤‘ ì—ì´ì „íŠ¸ê°€ PDF + ì›¹ í•˜ì´ë¸Œë¦¬ë“œ ì—°êµ¬ë¥¼ ìˆ˜í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            start_time = time.time()
            result = st.session_state.research_bot.deep_research(query)
            elapsed_time = time.time() - start_time
        
        torch.cuda.empty_cache()
        gc.collect()
        
        # ì—°êµ¬ ê¸°ë¡ ì €ì¥
        if "research_history" not in st.session_state:
            st.session_state.research_history = []
        
        st.session_state.research_history.append({
            "query": query,
            "result": result,
            "timestamp": datetime.now(),
            "elapsed_time": elapsed_time
        })
        
        # ê²°ê³¼ í‘œì‹œ
        st.subheader("ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ Deep Research ê²°ê³¼")
        st.write(result["answer"])
        
        # ìƒì„¸ ë©”íŠ¸ë¦­ ì •ë³´
        col1, col2, col3, col4, col5 = st.columns(5)
        metadata = result["research_metadata"]
        
        with col1:
            st.metric("ì‹ ë¢°ë„", f"{result['confidence']:.3f}")
        with col2:
            st.metric("ì—°êµ¬ ì‚¬ì´í´", f"{metadata['cycles_completed']}/{metadata.get('max_cycles', 3)}")
        with col3:
            st.metric("PDF ë¬¸ì„œ", metadata.get('pdf_documents', 0))
        with col4:
            st.metric("ì›¹ ë¬¸ì„œ", metadata.get('web_documents', 0))
        with col5:
            st.metric("ì†Œìš” ì‹œê°„", f"{elapsed_time:.1f}ì´ˆ")
        
        # ì°¸ì¡° ì†ŒìŠ¤ í‘œì‹œ (PDFì™€ ì›¹ ë¶„ë¦¬)
        if result["sources"]:
            st.subheader("ğŸ“š ë¶„ì„ëœ ì†ŒìŠ¤")
            
            # PDFì™€ ì›¹ ì†ŒìŠ¤ ë¶„ë¦¬
            pdf_sources = [s for s in result["sources"] if s["source_type"] == "PDF"]
            web_sources = [s for s in result["sources"] if s["source_type"] == "WEB"]
            
            col1, col2 = st.columns(2)
            
            with col1:
                if pdf_sources:
                    st.subheader("ğŸ“„ PDF ë¬¸ì„œ ì†ŒìŠ¤")
                    for i, source in enumerate(pdf_sources[:5], 1):
                        with st.expander(f"PDF {i}: {source['source_file']} (ì ìˆ˜: {source['similarity']:.3f})"):
                            st.write(f"**íŒŒì¼**: {source['source_file']}")
                            st.write(f"**í˜ì´ì§€**: {source['page']}, **ë‹¨ë½**: {source['paragraph']}")
                            st.write(f"**ë¯¸ë¦¬ë³´ê¸°**: {source['preview'][:200]}...")
                            if source.get('key_insight'):
                                st.write(f"ğŸ’¡ **í•µì‹¬ ì¸ì‚¬ì´íŠ¸**: {source['key_insight']}")
            
            with col2:
                if web_sources:
                    st.subheader("ğŸŒ ì›¹ ë¬¸ì„œ ì†ŒìŠ¤")
                    for i, source in enumerate(web_sources[:5], 1):
                        with st.expander(f"ì›¹ {i}: {source.get('web_title', 'ì œëª© ì—†ìŒ')} (ì ìˆ˜: {source['similarity']:.3f})"):
                            st.write(f"**ì œëª©**: {source.get('web_title', 'ì œëª© ì—†ìŒ')}")
                            st.write(f"**URL**: {source['source_file']}")
                            st.write(f"**ë„ë©”ì¸**: {source.get('web_domain', 'Unknown')}")
                            st.write(f"**í¬ë¡¤ë§ ì‹œê°„**: {source.get('web_crawl_time', 'Unknown')}")
                            st.write(f"**ë¯¸ë¦¬ë³´ê¸°**: {source['preview'][:200]}...")
        
        # ì—°êµ¬ ë¡œê·¸ í‘œì‹œ
        with st.expander("ğŸ” ìƒì„¸ ì—°êµ¬ ë¡œê·¸"):
            for log_entry in metadata.get('research_log', []):
                st.write(log_entry)

    # ì—°êµ¬ ê¸°ë¡ í‘œì‹œ
    if "research_history" in st.session_state and st.session_state.research_history:
        st.divider()
        st.subheader("ğŸ“œ í•˜ì´ë¸Œë¦¬ë“œ Deep Research ê¸°ë¡")
        
        for i, research in enumerate(reversed(st.session_state.research_history[-3:]), 1):
            with st.expander(f"ì—°êµ¬ {len(st.session_state.research_history) - i + 1}: {research['query'][:50]}..."):
                st.write(f"**ì§ˆë¬¸**: {research['query']}")
                st.write(f"**ì£¼ìš” ê²°ê³¼**: {research['result']['answer'][:300]}...")
                st.write(f"**ì—°êµ¬ ì‹œê°„**: {research['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}")
                st.write(f"**ìµœì¢… ì‹ ë¢°ë„**: {research['result']['confidence']:.3f}")
                
                metadata = research['result']['research_metadata']
                pdf_count = metadata.get('pdf_documents', 0)
                web_count = metadata.get('web_documents', 0)
                st.write(f"**ë¶„ì„ ì†ŒìŠ¤**: PDF {pdf_count}ê°œ, ì›¹ {web_count}ê°œ")

else:
    # ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°
    st.info("ğŸ‘† ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'Deep Research ì‹œìŠ¤í…œ ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•´ì£¼ì„¸ìš”.")
    
    # ê¸°ëŠ¥ ì†Œê°œ
    st.subheader("ğŸ†• í•˜ì´ë¸Œë¦¬ë“œ Deep Researchì˜ ìƒˆë¡œìš´ ê¸°ëŠ¥")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        **ğŸ“„ PDF ë¬¸ì„œ ë¶„ì„**
        - ì—…ë¡œë“œëœ ë¬¸ì„œì˜ ì‹¬ì¸µ ë¶„ì„
        - ì •í™•í•œ ì²­í‚¹ ë° ì„ë² ë”©
        - ì¶œì²˜ ëª…ì‹œ: [íŒŒì¼ëª…, ê´€ë ¨ë„]
        """)
    
    with col2:
        st.markdown("""
        **ğŸŒ ì‹¤ì‹œê°„ ì›¹í¬ë¡¤ë§**
        - ìµœì‹  ì •ë³´ ìë™ ê²€ìƒ‰
        - ê´€ë ¨ ì›¹í˜ì´ì§€ í¬ë¡¤ë§
        - ì¶œì²˜ ëª…ì‹œ: [ì œëª©, URL, í¬ë¡¤ë§ì‹œê°„, ê´€ë ¨ë„]
        """)

# ì‚¬ìš© ê°€ì´ë“œ
with st.expander("ğŸ“ í•˜ì´ë¸Œë¦¬ë“œ Deep Research ì‚¬ìš© ê°€ì´ë“œ"):
    st.markdown("""
    ### ğŸ§  í•˜ì´ë¸Œë¦¬ë“œ Deep Research ì‹œìŠ¤í…œì˜ íŠ¹ì§•
    
    **ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í˜‘ì—… + ì›¹í¬ë¡¤ë§**:
    - ğŸ“‹ **Research Planner**: ì§ˆë¬¸ ë¶„ì„ ë° PDF/ì›¹ ê²€ìƒ‰ ê³„íš ìˆ˜ë¦½
    - ğŸ” **Retriever Agent**: PDF ë¬¸ì„œ + ì›¹ í¬ë¡¤ë§ í†µí•© ê²€ìƒ‰
    - ğŸ”¬ **Analyzer Agent**: PDF + ì›¹ ì •ë³´ êµì°¨ ê²€ì¦
    - ğŸ“ **Synthesizer Agent**: ì¶œì²˜ë³„ ì¸ìš©ì„ í¬í•¨í•œ ì¢…í•© ë‹µë³€ ìƒì„±
    - âœ… **Validator Agent**: ìµœì¢… í’ˆì§ˆ ê²€ì¦
    
    **í•˜ì´ë¸Œë¦¬ë“œ ì •ë³´ ì†ŒìŠ¤**:
    - PDF ë¬¸ì„œ: ìƒì„¸í•˜ê³  ì •í™•í•œ ê¸°ì¡´ ì •ë³´
    - ì›¹ í¬ë¡¤ë§: ìµœì‹  ë™í–¥ ë° ì¼ë°˜ ì§€ì‹
    - ì¶œì²˜ êµ¬ë¶„: PDFì™€ ì›¹ ì†ŒìŠ¤ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ í‘œì‹œ
    
    **ì¶œì²˜ í‘œì‹œ í˜•ì‹**:
    - PDF: [íŒŒì¼ëª…, ê´€ë ¨ë„: 0.XX]
    - ì›¹: [ì œëª©/ë„ë©”ì¸, URL, í¬ë¡¤ë§ì‹œê°„, ê´€ë ¨ë„: 0.XX]
    
    **ìµœì í™”ëœ ì§ˆë¬¸ ìœ í˜•**:
    - ğŸ” **ìµœì‹  ë™í–¥**: "ìµœê·¼ AI ë°œì „ê³¼ PDFì˜ ê¸°ìˆ  ë™í–¥ì„ ë¹„êµí•´ì£¼ì„¸ìš”"
    - ğŸ¯ **ì‹¬ì¸µ ë¶„ì„**: "PDF ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í˜„ì¬ ì‹œì¥ ìƒí™©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”"  
    - ğŸ”— **í†µí•© ë¶„ì„**: "ë¬¸ì„œ ë‚´ìš©ê³¼ ìµœì‹  ë‰´ìŠ¤ë¥¼ ì¢…í•©í•˜ì—¬ í‰ê°€í•´ì£¼ì„¸ìš”"
    - ğŸ“Š **íŠ¸ë Œë“œ ë¶„ì„**: "PDF ë°ì´í„°ì™€ í˜„ì¬ ì›¹ ì •ë³´ë¥¼ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”"
    """)
    
    st.info("ğŸ’¡ PDF ë¬¸ì„œì˜ ì •í™•ì„±ê³¼ ì›¹ ì •ë³´ì˜ ìµœì‹ ì„±ì„ ê²°í•©í•˜ì—¬ ë”ìš± í¬ê´„ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.")
