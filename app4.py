from __future__ import annotations

import os
import re
import hashlib
import time
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from enum import Enum

import streamlit as st
import numpy as np
import torch, gc
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
from sentence_transformers.cross_encoder import CrossEncoder
import warnings

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Deep Research Chatbot", layout="wide")
warnings.filterwarnings("ignore")
os.environ["TRANSFORMERS_VERBOSITY"] = "error"

# PDF ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import pdfplumber
    PDFPLUMBER_AVAILABLE = True
except ImportError:
    PDFPLUMBER_AVAILABLE = False

# í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from kiwipiepy import Kiwi
    KIWI_AVAILABLE = True
except ImportError:
    KIWI_AVAILABLE = False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ë°ì´í„° êµ¬ì¡° ì •ì˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ResearchPhase(Enum):
    PLANNING = "planning"
    INITIAL_RETRIEVAL = "initial_retrieval"
    DEEP_ANALYSIS = "deep_analysis"
    CROSS_VALIDATION = "cross_validation"
    SYNTHESIS = "synthesis"
    FINAL_VALIDATION = "final_validation"

@dataclass
class ResearchState:
    phase: ResearchPhase
    query: str
    sub_queries: List[str]
    retrieved_docs: List[Dict[str, Any]]
    confidence_history: List[float]
    insights: List[str]
    gaps: List[str]
    cycle_count: int = 0
    max_cycles: int = 3

@dataclass
class SearchQuery:
    text: str
    priority: float
    category: str
    reason: str

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output.last_hidden_state
    mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * mask_expanded, 1) / torch.clamp(mask_expanded.sum(1), min=1e-9)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ì²­í‚¹ ë° ì¶”ì¶œê¸°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ImprovedKoreanSentenceChunker:
    def __init__(self, min_chunk_length=50, max_chunk_length=300, sentences_per_chunk=2):
        self.min_chunk_length = min_chunk_length
        self.max_chunk_length = max_chunk_length
        self.sentences_per_chunk = sentences_per_chunk
        self.kiwi = Kiwi() if KIWI_AVAILABLE else None

    def chunk_text(self, text: str) -> List[str]:
        if not text.strip():
            return []
        sentences = self._split_sentences(text)
        sentences = self._postprocess_sentences(sentences)
        return self._create_chunks(sentences)

    def _split_sentences(self, text: str) -> List[str]:
        if self.kiwi:
            try:
                kiwi_result = self.kiwi.split_into_sents(text)
                sents = [sent.text.strip() for sent in kiwi_result if sent.text.strip()]
                if len(sents) > 1:
                    return sents
            except:
                pass
        return self._regex_sentence_split(text)

    def _regex_sentence_split(self, text: str) -> List[str]:
        patterns = [
            r'[.!?]+\s+', r'\n\s*\n', r'\.\s*\n',
            r'[ë‹¤ê°€ë‚˜ë‹ˆê¹Œìš”ë˜ìŠµë‹ˆë‹¤]\s*[.!?]*\s+',
            r'[ë‹ˆë‹¤í–ˆë‹¤ìŠµë‹ˆë‹¤ì˜€ë‹¤ì•˜ë‹¤]\s*[.!?]*\s+',
        ]
        combined = '|'.join(f'({p})' for p in patterns)
        parts = re.split(combined, text)
        return [p.strip() for p in parts if p and not re.match(r'^\s*$', p)]

    def _postprocess_sentences(self, sentences: List[str]) -> List[str]:
        processed, buf = [], ""
        for s in sentences:
            if len(buf) < self.min_chunk_length:
                buf = f"{buf} {s}".strip()
            else:
                processed.append(buf)
                buf = s
        if buf:
            processed.append(buf)
        return processed

    def _create_chunks(self, sentences: List[str]) -> List[str]:
        chunks, buf, count = [], "", 0
        for s in sentences:
            candidate = f"{buf} {s}".strip() if buf else s
            if len(candidate) <= self.max_chunk_length and count < self.sentences_per_chunk:
                buf, count = candidate, count + 1
            else:
                if buf:
                    chunks.append(buf)
                buf, count = s, 1
        if buf:
            chunks.append(buf)
        return chunks

class ImprovedPDFExtractor:
    def extract_text_from_pdf(self, path: str) -> str:
        if not PDFPLUMBER_AVAILABLE:
            return ""
        try:
            text = ""
            with pdfplumber.open(path) as pdf:
                for page in pdf.pages:
                    ptxt = page.extract_text() or ""
                    text += ptxt + "\n"
            return text
        except:
            return ""

class CrawlerAgent:
    """ì›¹ í¬ë¡¤ë§ ë° ì²­í¬ ìƒì„± ì—ì´ì „íŠ¸"""
    def __init__(self, chunker: ImprovedKoreanSentenceChunker):
        self.chunker = chunker

    def crawl_and_chunk(self, url: str) -> List[Dict[str, Any]]:
        try:
            resp = requests.get(url, timeout=10, headers={"User-Agent": "DeepResearchBot/1.0"})
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, "html.parser")
            paras = [p.get_text(strip=True) for p in soup.find_all("p") if p.get_text(strip=True)]
            text = "\n".join(paras)
        except:
            return []
        chunks = self.chunker.chunk_text(text)
        return [
            {"text": c, "page": 0, "paragraph": i+1, "source": url, "full_path": url}
            for i, c in enumerate(chunks)
        ]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ë””ë°”ì´ìŠ¤ ë° ì—ì´ì „íŠ¸ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class DeviceConfig:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.config = self._get_config()

    def _get_config(self) -> Dict[str, Any]:
        if self.device == "cuda":
            return {"torch_dtype": torch.bfloat16, "embedding_batch_size": 32, "sim_threshold": 0.3}
        else:
            return {"torch_dtype": torch.float32, "embedding_batch_size": 4, "sim_threshold": 0.3}

# Placeholder agent classes for completeness; implement as needed
class ResearchPlannerAgent: ...
class RetrieverAgent: ...
class AnalyzerAgent: ...
class SynthesizerAgent: ...
class ValidatorAgent: ...

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ë©”ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í´ë˜ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class DeepResearchOrchestrator:
    def __init__(
        self,
        model_name: str,
        embed_model: str,
        reranker_name: str,
        min_chunk_length: int,
        max_chunk_length: int,
        sentences_per_chunk: int,
    ):
        st.info("ğŸš€ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        self.device_config = DeviceConfig()

        # ìœ í‹¸ë¦¬í‹°
        self.chunker = ImprovedKoreanSentenceChunker(min_chunk_length, max_chunk_length, sentences_per_chunk)
        self.pdf_extractor = ImprovedPDFExtractor()
        self.crawler = CrawlerAgent(self.chunker)

        # ëª¨ë¸ ë¡œë”©
        self.embed_tokenizer = AutoTokenizer.from_pretrained(embed_model)
        self.embed_model = AutoModel.from_pretrained(embed_model).to(self.device_config.device)
        self.reranker = CrossEncoder(reranker_name, device=self.device_config.device)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype=self.device_config.config["torch_dtype"],
            device_map="auto", trust_remote_code=True
        ).to(self.device_config.device)
        torch.cuda.empty_cache(); gc.collect()

        # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        self.planner = ResearchPlannerAgent(self.tokenizer, self.model, self.device_config)
        self.retriever = RetrieverAgent(self.embed_tokenizer, self.embed_model, self.reranker, self.device_config)
        self.analyzer = AnalyzerAgent(self.tokenizer, self.model, self.device_config)
        self.synthesizer = SynthesizerAgent(self.tokenizer, self.model, self.device_config)
        self.validator = ValidatorAgent(self.tokenizer, self.model, self.device_config)

        # ë°ì´í„° ì €ì¥ì†Œ
        self.documents: List[Dict[str, Any]] = []
        self.embeddings: Optional[np.ndarray] = None
        self.chunk_hashes: Dict[str, Dict] = {}
        self.loaded_pdfs: List[str] = []
        self.loaded_urls: List[str] = []
        st.success("âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")

    def _hash(self, text: str, page: int, para: int, source: str) -> str:
        return hashlib.md5(f"{source}_{page}_{para}_{text}".encode()).hexdigest()[:8]

    def _generate_embeddings(self):
        texts = [d["text"] for d in self.documents]
        bs = self.device_config.config["embedding_batch_size"]
        embs = []
        for i in range(0, len(texts), bs):
            batch = texts[i : i + bs]
            enc = self.embed_tokenizer(batch, padding=True, truncation=True, return_tensors="pt").to(self.device_config.device)
            with torch.no_grad():
                out = self.embed_model(**enc)
            embs.append(mean_pooling(out, enc["attention_mask"]).cpu().numpy())
        self.embeddings = np.vstack(embs) if embs else np.array([])

    def load_documents(self, pdf_paths: List[str], crawl_urls: List[str]) -> None:
        st.info(f"ğŸ“š PDF ì²˜ë¦¬ ì¤‘ ({len(pdf_paths)}ê°œ)")
        all_docs: List[Dict[str, Any]] = []

        # PDF ë¡œë”©
        for path in pdf_paths:
            if not os.path.exists(path):
                st.warning(f"âŒ íŒŒì¼ ëª» ì°¾ìŒ: {path}")
                continue
            text = self.pdf_extractor.extract_text_from_pdf(path)
            if not text.strip():
                st.warning(f"âš ï¸ PDF í…ìŠ¤íŠ¸ ì—†ìŒ: {path}")
                continue
            self.loaded_pdfs.append(path)
            for idx, chunk in enumerate(self.chunker.chunk_text(text), start=1):
                all_docs.append({"text": chunk, "page": 1, "paragraph": idx,
                                 "source": os.path.basename(path), "full_path": path})

        # ì›¹ í¬ë¡¤ë§
        urls = [u.strip() for u in crawl_urls if u.strip()]
        if urls:
            st.info(f"ğŸŒ í¬ë¡¤ë§ ì¤‘ ({len(urls)}ê°œ)")
            for url in urls:
                crawled = self.crawler.crawl_and_chunk(url)
                if crawled:
                    all_docs.extend(crawled)
                    self.loaded_urls.append(url)
                else:
                    st.warning(f"âš ï¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {url}")

        if not all_docs:
            st.error("âŒ ì²˜ë¦¬í•  ë¬¸ì„œ ì—†ìŒ")
            return

        # ì²­í¬ ID ìƒì„±
        self.documents = all_docs
        self.chunk_hashes = {}
        for d in self.documents:
            cid = self._hash(d["text"], d["page"], d["paragraph"], d["source"])
            d["chunk_id"] = cid
            self.chunk_hashes[cid] = d

        # ì„ë² ë”©
        st.info("ğŸ§® ì„ë² ë”© ìƒì„± ì¤‘...")
        self._generate_embeddings()
        st.success(f"ğŸ‰ ì´ {len(self.documents)}ê°œ ì²­í¬ ìƒì„±ë¨")

    def deep_research(self, query: str) -> Dict[str, Any]:
        # ì‹¤ì œ êµ¬í˜„ í•„ìš”
        return {
            "answer": "ë‹µë³€ ìƒì„± ë¡œì§ì´ êµ¬í˜„ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.",
            "confidence": 0.0,
            "warnings": [],
            "sources": [],
            "research_metadata": {
                "cycles_completed": 0,
                "max_cycles": 0,
                "total_documents_analyzed": 0,
                "confidence_progression": [],
                "research_log": []
            }
        }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. Streamlit UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.title("ğŸ§  Deep Research Chatbot")
st.sidebar.header("âš™ï¸ ì„¤ì •")

# ì²­í‚¹ íŒŒë¼ë¯¸í„°
min_chunk_length = st.sidebar.slider("ìµœì†Œ ì²­í¬ ê¸¸ì´", 30, 500, 50)
max_chunk_length = st.sidebar.slider("ìµœëŒ€ ì²­í¬ ê¸¸ì´", 200, 3000, 300)
sentences_per_chunk = st.sidebar.slider("ë¬¸ì¥ ìˆ˜/ì²­í¬", 1, 10, 2)

# íŒŒì¼ ì—…ë¡œë“œ & URL ì…ë ¥
uploaded_files = st.sidebar.file_uploader("PDF ì—…ë¡œë“œ", type="pdf", accept_multiple_files=True)
crawl_input = st.sidebar.text_area("í¬ë¡¤ë§í•  URL (ì¤„ë°”ê¿ˆ)", "").splitlines()

if st.sidebar.button("ğŸ”„ ì‹œìŠ¤í…œ ì‹œì‘"):
    if not uploaded_files:
        st.sidebar.error("PDF íŒŒì¼ ìµœì†Œ 1ê°œ í•„ìš”")
    else:
        pdf_paths = []
        tmp = "temp_uploads"
        os.makedirs(tmp, exist_ok=True)
        for f in uploaded_files:
            p = os.path.join(tmp, f.name)
            with open(p, "wb") as fp:
                fp.write(f.getbuffer())
            pdf_paths.append(p)
        bot = DeepResearchOrchestrator(
            model_name="LGAI-EXAONE/EXAONE-4.0-1.2B",
            embed_model="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            reranker_name="cross-encoder/ms-marco-MiniLM-L-6-v2",
            min_chunk_length=min_chunk_length,
            max_chunk_length=max_chunk_length,
            sentences_per_chunk=sentences_per_chunk
        )
        bot.load_documents(pdf_paths, crawl_input)
        st.session_state.research_bot = bot
        st.sidebar.success("âœ… ì´ˆê¸°í™” ì™„ë£Œ")

if "research_bot" in st.session_state:
    bot = st.session_state.research_bot

    # ì§ˆë¬¸ ì…ë ¥ ë° ì‹¤í–‰
    st.subheader("ğŸ’¬ Deep Research ì§ˆë¬¸")
    query = st.text_input("ì‹¬ì¸µ ì—°êµ¬í•  ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”:", key="deep_query_input")

    if st.button("ğŸ§  Deep Research ì‹œì‘") and query:
        with st.spinner("ğŸ§  ë‹¤ì¤‘ ì—ì´ì „íŠ¸ê°€ ì‹¬ì¸µ ì—°êµ¬ ì¤‘..."):
            start_time = time.time()
            result = bot.deep_research(query)
            elapsed = time.time() - start_time

        st.subheader("ğŸ¯ ê²°ê³¼")
        st.write(result["answer"])

        meta = result["research_metadata"]
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("ì‹ ë¢°ë„", f"{result['confidence']:.3f}")
        col2.metric("ì‚¬ì´í´", f"{meta['cycles_completed']}/{meta['max_cycles']}")
        col3.metric("ë¬¸ì„œ ë¶„ì„", meta["total_documents_analyzed"])
        col4.metric("ì†Œìš” ì‹œê°„", f"{elapsed:.1f}ì´ˆ")

        if meta.get("confidence_progression"):
            st.subheader("ğŸ“ˆ ì‹ ë¢°ë„ ë³€í™”")
            st.line_chart({"ì‹ ë¢°ë„": meta["confidence_progression"]})

        if result.get("sources"):
            st.subheader("ğŸ“š ì°¸ì¡° ì†ŒìŠ¤")
            for src in result["sources"]:
                st.markdown(f"- **{src['source_file']}** ({src['search_category']}, ì ìˆ˜ {src['similarity']:.3f})")
                if src.get("key_insight"):
                    st.markdown(f"  - ì¸ì‚¬ì´íŠ¸: {src['key_insight']}")

        if result.get("warnings"):
            st.warning("âš ï¸ " + " | ".join(result["warnings"]))

        if meta.get("research_log"):
            with st.expander("ğŸ” ì—°êµ¬ ë¡œê·¸"):
                for entry in meta["research_log"]:
                    st.write(entry)

        # ì—°êµ¬ ê¸°ë¡ ì €ì¥
        if "research_history" not in st.session_state:
            st.session_state.research_history = []
        st.session_state.research_history.append({
            "query": query,
            "result": result,
            "timestamp": datetime.now(),
            "elapsed_time": elapsed
        })

    # ì´ì „ ì—°êµ¬ ê¸°ë¡ í‘œì‹œ
    if "research_history" in st.session_state and st.session_state.research_history:
        st.divider()
        st.subheader("ğŸ“œ ì´ì „ Deep Research ê¸°ë¡")
        for idx, record in enumerate(reversed(st.session_state.research_history[-3:]), 1):
            with st.expander(f"{len(st.session_state.research_history)-idx+1}. {record['query'][:30]}..."):
                st.write(f"**ì§ˆë¬¸:** {record['query']}")
                st.write(f"**ê²°ê³¼ ìš”ì•½:** {record['result']['answer'][:200]}...")
                st.write(f"**ì™„ë£Œ ì‹œê°:** {record['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}")
                st.write(f"**ì†Œìš” ì‹œê°„:** {record['elapsed_time']:.1f}ì´ˆ")
                st.write(f"**ìµœì¢… ì‹ ë¢°ë„:** {record['result']['confidence']:.3f}")

    # ì‚¬ìš© ê°€ì´ë“œ
    st.divider()
    with st.expander("ğŸ“ Deep Research ì‚¬ìš© ê°€ì´ë“œ"):
        st.markdown("""
        ### ğŸ§  Deep Research ì‹œìŠ¤í…œì˜ íŠ¹ì§•
        
        **ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í˜‘ì—…**:
        - Research Planner, Retriever Agent, Analyzer Agent, Synthesizer Agent, Validator Agent
        **ì§€ëŠ¥í˜• ì—°êµ¬ ê³¼ì •**:
        - ì ì‘í˜• ì—°êµ¬ ì‚¬ì´í´, ì§€ì‹ ê²©ì°¨ ë³´ì™„, ì‹ ë¢°ë„ í‰ê°€ ë° ì¡°ê¸° ì¢…ë£Œ
        """)
        st.info("ğŸ’¡ ë”ìš± ì •í™•í•œ ë‹µë³€ì„ ìœ„í•´ ë‹¤ìˆ˜ ì—ì´ì „íŠ¸ë¥¼ í™œìš©í•©ë‹ˆë‹¤.")
